{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f4680e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimizers and Learning loops\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92723d",
   "metadata": {},
   "source": [
    "## Introduction to Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36dc38",
   "metadata": {},
   "source": [
    "In previous chapters, we saw how to load data and trained a linear regression model using mini-batch gradient decent. In practice, we don't need to write our own implementation of gradient decent as Pytorch provides various inbuilt optimizers algorithm. There are many different optimizers available in PyTorch, and each one has its own set of hyperparameters that can be tuned. Some of the most popular optimizers include:\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: This is a simple optimizer that updates the model's parameters using the gradient of the loss with respect to the parameters\n",
    "- **Adam (Adaptive Moment Estimation)**: This optimizer is based on the concept of momentum, which can help the optimizer to converge more quickly to a good solution. Adam also includes adaptive learning rates, which means that the optimizer can automatically adjust the learning rates of different parameters based on the historical gradient information\n",
    "- **RMSprop (Root Mean Square Propagation)**: This optimizer is similar to Adam, but it uses a different weighting for the gradient history\n",
    "- **Adagrad (Adaptive Gradient Algorithm)**: This optimizer is designed to handle sparse data, and it adjusts the learning rate for each parameter based on the historical gradient information\n",
    "- **Adadelta**: This optimizer is an extension of Adagrad that seeks to reduce its aggressive, monotonically declining learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff5916",
   "metadata": {},
   "source": [
    "## Exercise: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcfd9c",
   "metadata": {},
   "source": [
    "Let's look at how we can start using Pytorch's optimizer by continuing the previous linear regression example. Notice, this time we will use four input features instead of one in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72078913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.509632Z",
     "start_time": "2023-01-07T17:19:19.504642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature size: (1500, 4)\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "## Importing required functions\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style(\"dark\")\n",
    "%matplotlib inline\n",
    "\n",
    "## Generate dataset with linear property\n",
    "X, y, coef = make_regression(\n",
    "    n_samples=1500,\n",
    "    n_features=4, ## Using four features\n",
    "    n_informative=4,\n",
    "    noise=0.3,\n",
    "    coef=True,\n",
    "    random_state=0,\n",
    "    bias=2\n",
    ")\n",
    "\n",
    "print(f'Input feature size: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e18370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T16:47:47.237871Z",
     "start_time": "2023-01-07T16:47:47.234643Z"
    }
   },
   "source": [
    "Now we will create a custom `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "855aeba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.519108Z",
     "start_time": "2023-01-07T17:19:19.511387Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating our custom TabularDataset \n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_sample = self.data[idx]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\n",
    "            \"X\": torch.tensor(current_sample, dtype=torch.float), \n",
    "            \"y\": torch.tensor(current_target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761d596",
   "metadata": {},
   "source": [
    "We have modified the `TabularDataset` class to handle additional features. Now, the class takes two inputs: `data` which includes our four features, and `targets` which is our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4835f817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.529244Z",
     "start_time": "2023-01-07T17:19:19.521859Z"
    }
   },
   "outputs": [],
   "source": [
    "## Making a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "\n",
    "## Creating Tabular Dataset\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "test_dataset = TabularDataset(X_test, y_test)\n",
    "\n",
    "## Creating Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627354df",
   "metadata": {},
   "source": [
    "We have divided our sample into a training set and a test set, and used the `TabularDataset` class to create train and test objects. Finally, we created data loaders for the training set and test set using these objects. \n",
    "\n",
    ":::{.callout-note}\n",
    "In the code, the training data is shuffled using the Dataloader while the testing data is not. This is a common practice when training a machine learning model.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1869165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.541953Z",
     "start_time": "2023-01-07T17:19:19.530677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights: torch.Size([4, 1])\n",
      "Shape of bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n",
    "        self.b = torch.randn(n_out).requires_grad_(True)\n",
    "        self.params = [self.w, self.b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b\n",
    "    \n",
    "## Initializing model\n",
    "torch.manual_seed(4)\n",
    "model = Linear(X.shape[1], 1)\n",
    "\n",
    "print(f\"Shape of weights: {model.w.shape}\")\n",
    "print(f\"Shape of bias: {model.b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d6894",
   "metadata": {},
   "source": [
    "We are using the same linear model as last time but this time it will take four inputs instead of one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed69afb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.550404Z",
     "start_time": "2023-01-07T17:19:19.543138Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a311776",
   "metadata": {},
   "source": [
    "Next, we will define our optimizer. We will use PyTorch's implementation of stochastic gradient descent (SGD) by initializing `torch.optim.SGD`. Here we are passing the model parameters which needs to get modified during training process and a hyperparameter learning rate (`lr`) of `1e-3`.\n",
    "\n",
    "\n",
    "For more information about other available optimizers and their hyperparameters, you can refer to the PyTorch optimizer documentation at [this link](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b1baa27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.559775Z",
     "start_time": "2023-01-07T17:19:19.552417Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    for batch in iter(data_loader):\n",
    "        ## Taking one mini-batch\n",
    "        y_pred = model.forward(batch['X']).squeeze()\n",
    "        y_true = batch['y']\n",
    "        \n",
    "        ## Calculation mean square error per min-batch\n",
    "        loss = torch.square(y_pred - y_true).sum()\n",
    "    \n",
    "        ## Computing gradients per mini-batch\n",
    "        loss.backward()\n",
    "        \n",
    "        ## Update model parameters and zero grad\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(model, data_loader, optimizer):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iter(data_loader):\n",
    "            y_pred = model.forward(batch['X']).squeeze()\n",
    "            y_true = batch['y']\n",
    "            loss += torch.square(y_pred- y_true).sum()\n",
    "    return loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b6bfd",
   "metadata": {},
   "source": [
    "For the training loop (defined in `train_one_epoch`), we will go through each mini-batch and do the following:\n",
    "\n",
    "- Use the model to make a prediction\n",
    "- Calculate the Mean Squared Error (MSE) and the gradients\n",
    "- Update the model parameters using the optimizer's step() function\n",
    "- Reset the gradients to zero for the next mini-batch using the optimizer's zero_grad() function\"\n",
    "\n",
    "In the validation loop (defined in `validate_one_epoch`), we will process each mini-batch as follows:\n",
    "\n",
    "- Use the trained model to make a prediction\n",
    "- Calculate the Mean Squared Error (MSE) loss and return the overall MSE at the end\n",
    "\n",
    "Now let's run through some epochs and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3589799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-07T17:19:19.748844Z",
     "start_time": "2023-01-07T17:19:19.560983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,Train MSE: 13529.4424 Test MSE: 15622.040\n",
      "Epoch 1,Train MSE: 278.9650 Test MSE: 329.534\n",
      "Epoch 2,Train MSE: 12.0025 Test MSE: 12.883\n",
      "Epoch 3,Train MSE: 5.8382 Test MSE: 5.576\n",
      "Epoch 4,Train MSE: 5.6956 Test MSE: 5.428\n",
      "Epoch 5,Train MSE: 5.6985 Test MSE: 5.439\n",
      "Epoch 6,Train MSE: 5.6968 Test MSE: 5.439\n",
      "Epoch 7,Train MSE: 5.7135 Test MSE: 5.459\n",
      "Epoch 8,Train MSE: 5.7058 Test MSE: 5.408\n",
      "Epoch 9,Train MSE: 5.6976 Test MSE: 5.414\n",
      "Actual coefficients are: \n",
      "[63.0061 44.1452 84.3648  9.3378] \n",
      "Trained model weights are: \n",
      "[63.0042 44.1552 84.3705  9.3369]\n",
      "Actual Bias term is 2 \n",
      "Trained model bias term is \n",
      "2.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    \n",
    "    # run one training loop\n",
    "    train_one_epoch(model, train_dataloader, optimizer)\n",
    "    # run validation loop on training to compute training loss\n",
    "    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n",
    "    # run validation loop on testing to compute test loss\n",
    "    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n",
    "    \n",
    "    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n",
    "    \n",
    "print(f\"Actual coefficients are: \\n{np.round(coef,4)} \\nTrained model weights are: \\n{np.round(model.w.squeeze().detach().numpy(),4)}\")\n",
    "print(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{model.b.squeeze().detach().numpy().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5d793",
   "metadata": {},
   "source": [
    "As shown above, our model has fit the data well. The actual coefficients and bias used to generate the random data roughly match the weights and bias term of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ef61a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb8d24",
   "metadata": {},
   "source": [
    "In PyTorch, optimizers are used to update the parameters of a model during training. Optimizers adjust the parameters of the model based on the gradients of the loss function with respect to the parameters, in order to minimize the loss. \n",
    "\n",
    "There are many different optimizers available in PyTorch, including SGD, Adam, RMSprop, and more. You can choose the optimizer that works best for your specific problem and model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed97cb",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371965ae",
   "metadata": {},
   "source": [
    "- [Pytorch Optim Documentation](https://pytorch.org/docs/stable/optim.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
