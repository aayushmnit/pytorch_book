{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f4680e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimizers and Learning loops\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36dc38",
   "metadata": {},
   "source": [
    "In previous chapters, we saw how to load data and trained a linear regression model using mini-batch gradient decent. In practice, we don't need to write our own implementation of gradient decent as Pytorch provides various inbuilt optimizers algorithm. There are many different optimizers available in PyTorch, and each one has its own set of hyperparameters that can be tuned. Some of the most popular optimizers include:\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent)**: This is a simple optimizer that updates the model's parameters using the gradient of the loss with respect to the parameters\n",
    "- **Adam (Adaptive Moment Estimation)**: This optimizer is based on the concept of momentum, which can help the optimizer to converge more quickly to a good solution. Adam also includes adaptive learning rates, which means that the optimizer can automatically adjust the learning rates of different parameters based on the historical gradient information\n",
    "- **RMSprop (Root Mean Square Propagation)**: This optimizer is similar to Adam, but it uses a different weighting for the gradient history\n",
    "- **Adagrad (Adaptive Gradient Algorithm)**: This optimizer is designed to handle sparse data, and it adjusts the learning rate for each parameter based on the historical gradient information\n",
    "- **Adadelta**: This optimizer is an extension of Adagrad that seeks to reduce its aggressive, monotonically declining learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcfd9c",
   "metadata": {},
   "source": [
    "Let's look at how we can start using Pytorch's optimizer by continuing the previous linear regression example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "72078913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.613654Z",
     "start_time": "2023-01-04T06:47:24.607496Z"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "## Importing required functions\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style(\"dark\")\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_data(x, y, y_pred=None, label=None):\n",
    "    clear_output(wait=True)\n",
    "    sns.scatterplot(x = X.squeeze(), y=y)\n",
    "    if y_pred is not None:\n",
    "        sns.lineplot(x = X.squeeze(), y=y_pred.squeeze(), color='red')\n",
    "    plt.xlabel(\"Input\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    if label: \n",
    "        plt.title(label)\n",
    "    plt.show()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "\n",
    "## Generate dataset with linear property\n",
    "X, y, coef = make_regression(\n",
    "    n_samples=1500,\n",
    "    n_features=4, ## Using four features\n",
    "    n_informative=4,\n",
    "    noise=0.3,\n",
    "    coef=True,\n",
    "    random_state=0,\n",
    "    bias=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b102b9",
   "metadata": {},
   "source": [
    "Notice that this time we will use four features instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d30ed29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.635781Z",
     "start_time": "2023-01-04T06:47:24.615586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature size: (1500, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input feature size: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "855aeba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.645813Z",
     "start_time": "2023-01-04T06:47:24.637046Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating our custom TabularDataset \n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_sample = self.data[idx]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\n",
    "            \"X\": torch.tensor(current_sample, dtype=torch.float), \n",
    "            \"y\": torch.tensor(current_target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3761d596",
   "metadata": {},
   "source": [
    "We have modified the `TabularDataset` class to handle additional features. Now, the class takes two inputs: `data` which includes our four features, and `targets` which is our target variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4835f817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.655644Z",
     "start_time": "2023-01-04T06:47:24.647647Z"
    }
   },
   "outputs": [],
   "source": [
    "## Making a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "\n",
    "## Creating Tabular Dataset\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "test_dataset = TabularDataset(X_test, y_test)\n",
    "\n",
    "## Creating Dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627354df",
   "metadata": {},
   "source": [
    "We have divided our sample into a training set and a test set, and used the `TabularDataset` class to create train and test objects. Finally, we created data loaders for the training set and test set using these objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f1869165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.665362Z",
     "start_time": "2023-01-04T06:47:24.656880Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n",
    "        self.b = torch.randn(n_out).requires_grad_(True)\n",
    "        self.params = [self.w, self.b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b\n",
    "    \n",
    "## Initializing model\n",
    "torch.manual_seed(4)\n",
    "model = Linear(X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d6894",
   "metadata": {},
   "source": [
    "We are using the same linear model as last time but this time it will take four inputs instead of one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ed69afb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.676663Z",
     "start_time": "2023-01-04T06:47:24.666630Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a311776",
   "metadata": {},
   "source": [
    "Next, we will define our optimizer. We will use PyTorch's implementation of stochastic gradient descent (SGD) by initializing `torch.optim.SGD`. For more information about other available optimizers, you can refer to the PyTorch optimizer documentation at [this link](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2b1baa27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.686217Z",
     "start_time": "2023-01-04T06:47:24.677954Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    for batch in iter(data_loader):\n",
    "        ## Taking one mini-batch\n",
    "        y_pred = model.forward(batch['X']).squeeze()\n",
    "        y_true = batch['y']\n",
    "        \n",
    "        ## Calculation mean square error per min-batch\n",
    "        loss = torch.square(y_pred - y_true).sum()\n",
    "    \n",
    "        ## Computing gradients per mini-batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(model, data_loader, optimizer):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iter(data_loader):\n",
    "            y_pred = model.forward(batch['X']).squeeze()\n",
    "            y_true = batch['y']\n",
    "            loss += torch.square(y_pred- y_true).sum()\n",
    "    return loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b6bfd",
   "metadata": {},
   "source": [
    "For the training loop (defined in `train_one_epoch`), we will go through each mini-batch and do the following:\n",
    "\n",
    "- Use the model to make a prediction\n",
    "- Calculate the Mean Squared Error (MSE) and the gradients\n",
    "- Update the model parameters using the optimizer's step() function\n",
    "- Reset the gradients to zero for the next mini-batch using the optimizer's zero_grad() function\"\n",
    "\n",
    "In the validation loop (defined in `validate_one_epoch`), we will process each mini-batch as follows:\n",
    "\n",
    "- Use the trained model to make a prediction\n",
    "- Calculate the Mean Squared Error (MSE) loss and return the overall MSE at the end\n",
    "\n",
    "Now let's run through some epochs and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a3589799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T06:47:24.873273Z",
     "start_time": "2023-01-04T06:47:24.687351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,Train MSE: 10894.3438 Test MSE: 10579.818\n",
      "Epoch 1,Train MSE: 173.3263 Test MSE: 177.178\n",
      "Epoch 2,Train MSE: 8.5975 Test MSE: 8.723\n",
      "Epoch 3,Train MSE: 5.6845 Test MSE: 5.609\n",
      "Epoch 4,Train MSE: 5.6420 Test MSE: 5.557\n",
      "Epoch 5,Train MSE: 5.6445 Test MSE: 5.558\n",
      "Epoch 6,Train MSE: 5.6452 Test MSE: 5.559\n",
      "Epoch 7,Train MSE: 5.6453 Test MSE: 5.559\n",
      "Epoch 8,Train MSE: 5.6454 Test MSE: 5.559\n",
      "Epoch 9,Train MSE: 5.6454 Test MSE: 5.559\n",
      "Actual coefficients are: \n",
      "[63.00614902 44.14519607 84.36475203  9.33783172] \n",
      "Trained model weights are: \n",
      "[63.00543  44.15637  84.36545   9.322486]\n",
      "Actual Bias term is 2 \n",
      "Trained model bias term is \n",
      "1.9980753660202026\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    \n",
    "    # run one training loop\n",
    "    train_one_epoch(model, train_dataloader, optimizer)\n",
    "    # run validation loop to compute loss on validation set\n",
    "    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n",
    "    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n",
    "    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n",
    "    \n",
    "print(f\"Actual coefficients are: \\n{coef} \\nTrained model weights are: \\n{model.w.squeeze().detach().numpy()}\")\n",
    "print(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{model.b.squeeze().detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b182ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e8b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
