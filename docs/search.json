[
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "The PyTorch Book",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis is a collection of content compiled during my journey to learn PyTorch, machine learning, and deep learning. The book will primarily cover practical applications of PyTorch and popular frameworks that build upon PyTorch, such as FastAI and Huggingface. Please note that this book is a work in progress and additional chapters will be added over time.I hope that this book, with its collection of theory and practical projects, will be useful for others who are also learning PyTorch."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#what-is-pytorch",
    "href": "basics/0_pytorch_intro.html#what-is-pytorch",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.1 What is PyTorch",
    "text": "1.1 What is PyTorch\n\n\n\nPyTorch is an open-source machine learning library for Python, based on the Torch library, used for applications such as deep learning and GPU based acceleration. It is primarily developed by Facebook’s artificial intelligence research group. PyTorch provides a high-level interface for working with large-scale machine learning models. It allows developers to define and train neural networks in a fast, flexible, and dynamic way.\nSome key features of PyTorch include:\n\nTensor computations with GPU acceleration: PyTorch includes GPU acceleration out of the box, allowing you to train your models on a single or multiple GPUs with ease.\nDynamic computation graphs: PyTorch allows you to define your computations as a dynamic graph, which allows you to change the structure of the graph on the fly and enables you to use dynamic control flow.\nEasy model building: PyTorch provides a wide range of predefined layers and architectures that you can use to build your models, making it easy to get started with machine learning.\nEcosystem support: PyTorch has a large and active community of users, and there are many tools and libraries available that are built on top of PyTorch, such as visualization tools, data loaders, and more.\n\nOverall, PyTorch is a powerful and flexible tool for working with large-scale machine learning models, and is widely used in research and industry.\n\n\n\n\n\n\nNote\n\n\n\n\n\nOn a personal note, I have used many different Deep learning frameworks and I find Pytorch to be the most intuitive. It blends nicely with regular Python and have really good ecosystem. I am also big fan of FastAI library which is written on top of Pytorch and is one of the best framework to do deep learning based project."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#installation",
    "href": "basics/0_pytorch_intro.html#installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.2 Installation",
    "text": "1.2 Installation\nSince, Pytorch is under active development and new version keeps getting released. You want to visit this website for latest Pytorch installation instruction.\n\n\n\nFig 1: Pytorch Installation page.\n\n\nThe above interface is what you will typically see on the installation page, based on your OS and other selection, it will produce a command you can run on your command line interface and install Pytorch. I recommend installing through conda and it will take care of the dependencies automatically."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#checking-installation",
    "href": "basics/0_pytorch_intro.html#checking-installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.3 Checking Installation",
    "text": "1.3 Checking Installation\nLet’s run the import command and see if we have Pytorch setup correctly.\n\nimport torch\nprint(torch.__version__)\n\n1.12.1\n\n\nIt is recommended to use a machine with a GPU when working with PyTorch because it is typically used for GPU-accelerated computations. If you are running PyTorch on a machine with a GPU and want to verify that it has access to the GPU, you can run the following command-\n\ntorch.cuda.is_available()\n\nTrue\n\n\nIf PyTorch is installed on a machine with a GPU, the above command will return True otherwise it will return False."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#what-are-tensors",
    "href": "basics/1_pytorch_tensors.html#what-are-tensors",
    "title": "2  Tensors",
    "section": "2.1 What are Tensors?",
    "text": "2.1 What are Tensors?\nPyTorch provides tensors as its primary data structure. Tensors are similar to numpy arrays, but they can be used on a GPU to accelerate the computation. PyTorch tensors are similar to numpy arrays, but they have additional functionality (automatic differentiation) and are designed to take advantage of GPUs for acceleration. Similar to numpy, tensors in PyTorch supports a variety of operations, including indexing, slicing, math operations, linear algebra operations, and more. Let’s dive in by importing the library.\n\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#initializing-a-tensor",
    "href": "basics/1_pytorch_tensors.html#initializing-a-tensor",
    "title": "2  Tensors",
    "section": "2.2 Initializing a Tensor",
    "text": "2.2 Initializing a Tensor\nThere are several ways to initialize tensors in PyTorch. Here are some examples:\nInitializing from an iterator like a list\n\n# Initialize a tensor from a list\ntensor_from_list = torch.tensor([1, 2, 3, 4])\nprint(\"Tensor from list: \\n\", tensor_from_list)\n\n# Initialize a tensor from a nested list\ntensor_from_nested_list = torch.tensor([[1, 2], [3, 4]])\nprint(\"Tensor from nested list: \\n\", tensor_from_nested_list)\n\nTensor from list: \n tensor([1, 2, 3, 4])\nTensor from nested list: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from a numpy array\n\n# Create a NumPy array\nnumpy_array = np.array([[1, 2], [3, 4]])\n\n# Initialize a tensor from a NumPy array\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(\"Tensor from np array: \\n\", tensor_from_numpy)\n\nTensor from np array: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from another tensor\n\n# Create a tensor\noriginal_tensor = torch.tensor([1, 2, 3, 4])\n\n# Initialize a new tensor from the original tensor\nnew_tensor = original_tensor.clone()\nprint(\"Tensor from another tensor: \\n\", new_tensor)\n\nTensor from another tensor: \n tensor([1, 2, 3, 4])\n\n\nConstant or random initialization\n\n# Initialize a tensor with all elements set to zero\ntensor_zeros = torch.zeros(3, 4)\nprint(\"Tensor with all elements set to zero: \\n\", tensor_zeros)\n\n# Initialize a tensor with all elements set to one\ntensor_ones = torch.ones(3, 4)\nprint(\"\\n Tensor with all elements set to one: \\n\", tensor_ones)\n\n# Initialize a tensor with all elements set to a specific value\ntensor_full = torch.full((3, 4), fill_value=2.5)\nprint(\"\\n Tensor with all elements set to a specific value: \\n\", tensor_full)\n\n# Initialize a tensor with random values\ntensor_rand = torch.rand(3, 4)\nprint(\"\\n Tensor with random initialization: \\n\", tensor_rand)\n\n# Initialize a tensor with random values from a normal distribution\ntensor_randn = torch.randn(3, 4)\nprint(\"\\n Tensor with random values from a normal distribution: \\n\", tensor_randn)\n\nTensor with all elements set to zero: \n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n Tensor with all elements set to one: \n tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n Tensor with all elements set to a specific value: \n tensor([[2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000]])\n\n Tensor with random initialization: \n tensor([[0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n Tensor with random values from a normal distribution: \n tensor([[ 1.0550,  0.9214, -1.3023,  0.4119],\n        [-0.4691,  0.8733,  0.7910, -2.3932],\n        [-0.6304, -0.8792,  0.4188,  0.4221]])"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#tensor-attributes",
    "href": "basics/1_pytorch_tensors.html#tensor-attributes",
    "title": "2  Tensors",
    "section": "2.3 Tensor Attributes",
    "text": "2.3 Tensor Attributes\nIt has several attributes that you can access to get information about the tensor. Here are some common attributes of a PyTorch tensor:\n\nshape: returns the shape of the tensor as a tuple of integers. For example, if the tensor has dimensions (batch_size, num_channels, height, width), the shape would be (batch_size, num_channels, height, width).\ndtype: returns the data type of the tensor. For example, the data type could be torch.float32 or torch.int64.\ndevice: returns the device on which the tensor is stored. This can be the CPU or a GPU.\nrequires_grad: a boolean flag indicating whether the tensor requires gradient computation. If set to True, the tensor’s gradients will be computed during backpropagation.\ngrad: a tensor containing the gradient of the tensor with respect to some scalar value. This attribute is typically used during training with gradient descent.\n\nYou can access these attributes by calling them on a tensor object. For example:\n\ntensor_randn = torch.randn(3, 4)\nprint(f\"Shape of tensor : {tensor_randn.shape}\")\nprint(f\"Type of tensor : {tensor_randn.dtype}\")\nprint(f\"Device tensor is stored on : {tensor_randn.device}\")\nprint(f\"Autograd enabled : {tensor_randn.requires_grad}\")\nprint(f\"Any stored gradient : {tensor_randn.grad}\")\n\nShape of tensor : torch.Size([3, 4])\nType of tensor : torch.float32\nDevice tensor is stored on : cpu\nAutograd enabled : False\nAny stored gradient : None\n\n\nAs we can see above we initialized a random tensor of shape (3,4) with a torch.float32 data type and its currently on a cpu device. Currently, automatic gradient calculations are disabled and no gradient is stored in the tensor.\nThere are several other attributes that you can access, such as ndim, size, numel, storage, etc. You can find more information about these attributes in the PyTorch Tensor documentation."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#tensor-operations",
    "href": "basics/1_pytorch_tensors.html#tensor-operations",
    "title": "2  Tensors",
    "section": "2.4 Tensor Operations",
    "text": "2.4 Tensor Operations\nThere are several operations you can perform on tensors, lets look at the most commonly used operations.\n\n2.4.1 Moving tensor from CPU to GPU\nTo move a tensor from CPU to GPU is a simple command but probably the one which people will use the most.\n\ntensor_randn.to(\"cuda\")\n\ntensor([[-0.0984, -1.3804,  0.3343, -0.1623],\n        [ 0.9155, -0.8620, -0.3943, -0.2997],\n        [-0.1336, -0.7395, -0.7143, -0.0735]], device='cuda:0')\n\n\nAs we can see the tensor_randn is now moved to a cuda(GPU) device.\n\n\n2.4.2 Slicing and Indexing\nPyTorch tensors similar to numpy arrays support various slicing and indexing operations.\n\ntensor_randn = torch.randn(3, 4)\ntensor_randn\n\ntensor([[-1.3470,  0.2204,  0.2963, -0.9745],\n        [ 0.1867, -1.8338, -1.1872, -1.2987],\n        [ 0.0517, -0.3206,  0.3584, -0.4778]])\n\n\n\nprint(f\"First row:  \\n{tensor_randn[0]}\")\nprint(f\"\\n First column: \\n {tensor_randn[:, 0]}\")\nprint(f\"\\n Last column: {tensor_randn[..., -1]}\")\nprint(f\"\\n Selected columns: \\n {tensor_randn[:,2:4]}\")\n## Assignment of column to zero\ntensor_randn[:,1] = 0\nprint(\"\\n Assigning column to zero: \\n\", tensor_randn)\n\nFirst row:  \ntensor([-1.3470,  0.2204,  0.2963, -0.9745])\n\n First column: \n tensor([-1.3470,  0.1867,  0.0517])\n\n Last column: tensor([-0.9745, -1.2987, -0.4778])\n\n Selected columns: \n tensor([[ 0.2963, -0.9745],\n        [-1.1872, -1.2987],\n        [ 0.3584, -0.4778]])\n\n Assigning column to zero: \n tensor([[-1.3470,  0.0000,  0.2963, -0.9745],\n        [ 0.1867,  0.0000, -1.1872, -1.2987],\n        [ 0.0517,  0.0000,  0.3584, -0.4778]])\n\n\n\n\n2.4.3 Concatenation\nThe torch.cat function can be used to concatenate or join multiple tensors together, which is often useful when working with deep learning models.\nLet’s take our previous defined tensors and check their shape.\n\ntensor_ones.shape, tensor_zeros.shape, tensor_rand.shape\n\n(torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]))\n\n\nWe can concatenate these tensors column wise by using torch.cat with dim=1. We will get a resultant tensor with shape (3,12).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=1)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([3, 12])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8675,\n         0.0161, 0.5472, 0.7002],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6551,\n         0.3049, 0.4088, 0.6341],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2363,\n         0.8951, 0.0335, 0.5779]])\n\n\nWe can concatenate these tensors row wise by using torch.cat with dim=0. We will get a resultant tensor with shape (9,4).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=0)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([9, 4])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n\n\n\n2.4.4 Arithmetic operations\nIn PyTorch, you can perform arithmetic operations on tensors in a similar way to how you would perform them on numpy arrays. Lets look at some common arithmetic operations -\nElement wise addition\n\ntnsr1 = torch.randn((3,4))\nprint(f\"Tensor 1: \\n\", tnsr1)\ntnsr2 = torch.randn((3,4))\nprint(f\"\\n Tensor 2: \\n\", tnsr2)\n\n## Addition\ntensor_add = tnsr1 + tnsr2 \nprint(f\"\\n Tensor additions using + : \\n\", tensor_add)\n\ntensor_add = tnsr1.add(tnsr2)\nprint(f\"\\n Tensor additions using .add : \\n\", tensor_add)\n\nTensor 1: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n Tensor 2: \n tensor([[-0.3125,  1.0860,  0.7340,  0.2249],\n        [-0.9887,  0.2265, -0.5214, -1.5676],\n        [ 0.6817,  0.1099, -0.5298, -0.3109]])\n\n Tensor additions using + : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n Tensor additions using .add : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n\nElement wise subtraction\n\n## Subtraction\ntensor_sub = tnsr1 - tnsr2 \nprint(f\"\\n Tensor subtraction using - : \\n\", tensor_sub)\n\ntensor_sub = tnsr1.sub(tnsr2)\nprint(f\"\\n Tensor subtraction using .sub : \\n\", tensor_sub)\n\n\n Tensor subtraction using - : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n Tensor subtraction using .sub : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n\nElement wise multiplication\n\n## Multiplication\ntensor_mul = tnsr1 * tnsr2 \nprint(f\"\\n Tensor element-wise multiplication using * : \\n\", tensor_mul)\n\ntensor_mul = tnsr1.mul(tnsr2)\nprint(f\"\\n Tensor element-wise multiplication using .mul : \\n\", tensor_mul)\n\n\n Tensor element-wise multiplication using * : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n Tensor element-wise multiplication using .mul : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n\nElement wise division\n\n## Division\ntensor_div = tnsr1 / tnsr2 \nprint(f\"\\n Tensor element-wise division using / : \\n\", tensor_div)\n\ntensor_div = tnsr1.div(tnsr2)\nprint(f\"\\n Tensor element-wise division using .div : \\n\", tensor_div)\n\n\n Tensor element-wise division using + : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n Tensor element-wise division using .div : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n\nMatrix multiplication\n\ntensor_mm = tnsr1 @ tnsr2.T\nprint(f\"\\n Tensor matrix multiplication using @: \\n\", tensor_mm)\n\ntensor_mm = tnsr1.matmul(tnsr2.T)\nprint(f\"\\n Tensor matrix multiplication using .matmul: \\n\", tensor_mm)\n\n\n Tensor matrix multiplication using @: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n Tensor matrix multiplication using .matmul: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nObserve that tnsr1 and tnsr2 have shape (3,4). To perform a matrix multiplication, we used the .T function to transpose tnsr2, which changed its shape to (4,3). The resulting matrix multiplication has size (3x3).\n\n\nSumming it up\nSumming tensors along rows and columns is a common operation. Here is the syntax for this operation:\n\nprint(f\"Tensor: \\n {tnsr1}\" )\nprint(f\"\\n All up sum: \\n {tnsr1.sum()}\")\nprint(f\"\\n Column wise sum: \\n {tnsr1.sum(0)}\")\nprint(f\"\\n Row wise sum: \\n {tnsr1.sum(1)}\")\n\nTensor: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n All up sum: \n -2.48371958732605\n\n Column wise sum: \n tensor([-1.9103, -1.4094,  1.2977, -0.4617])\n\n Row wise sum: \n tensor([-1.5841, -0.9375,  0.0378])"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#why-gpus",
    "href": "basics/1_pytorch_tensors.html#why-gpus",
    "title": "2  Tensors",
    "section": "2.5 Why GPUs?",
    "text": "2.5 Why GPUs?\nDeep learning models often involve large amounts of matrix operations such as matrix multiplication. Let’s do a speed comparison b/w numpy CPU implementation, Pytorch CPU and GPU implementation.\n\n2.5.1 Matrix multiplication using numpy\nLet’s initialize one tensors of size (1000,64,64) and one tensor of size (64,32) and lets do a matrix multiplication speed comparison\n\narr1 = np.random.randn(1000, 64, 64)\narr2 = np.random.randn(64, 32)\n\n\n%timeit -n 50 res = np.matmul(arr1, arr2)\n\n9.7 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs we can see matrix multiplicaiton on numpy which uses a highly optmized matrix multiplication does the above operation in 9.7 milliseconds.\n\n\n2.5.2 Matrix multiplication using pytorch on cpu\nNow let’s do the same operation using PyTorch tensors on CPU.\n\ntnsr1 = torch.from_numpy(arr1)\ntnsr2 = torch.from_numpy(arr2)\n\n\n%timeit -n 50 res = tnsr1 @ tnsr2\n\n2.78 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nWe can see that PyTorch on CPU performed the same operation in 2.78 milliseconds which is roughly 3 times faster than the numpy version.\n\n\n2.5.3 Matrix multiplication using pytorch on GPU\nLet’s do the same operation on GPU using Pytorch.\n\ntnsr1 = tnsr1.to(\"cuda\")\ntnsr2 = tnsr2.to(\"cuda\")\n\n\n%timeit -n 50 res = (tnsr1 @ tnsr2)\n\n15.6 µs ± 4.32 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs demonstrated by the Matrix multiplication example, the GPU version was completed in 15.6 microseconds, a significant improvement over both the Pytorch CPU version (which took 2.8 milliseconds) and the numpy implementation (which took 9.7 milliseconds). This speedup is even more pronounced when working with larger matrices."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#references",
    "href": "basics/1_pytorch_tensors.html#references",
    "title": "2  Tensors",
    "section": "2.6 References",
    "text": "2.6 References\n\nPytorch tensors tutorial documentation."
  },
  {
    "objectID": "basics/2_broadcasting.html#what-is-broadcasting",
    "href": "basics/2_broadcasting.html#what-is-broadcasting",
    "title": "3  Broadcasting",
    "section": "3.1 What is Broadcasting?",
    "text": "3.1 What is Broadcasting?\nIn PyTorch, broadcasting refers to the automatic expansion of a tensor’s dimensions to match the dimensions of another tensor during an operation. This allows for element-wise operations between tensors of different shapes, as long as certain rules are followed.\nFor example, consider the following operation:\n\nimport torch\n\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nc = a + b\nc\n\ntensor([5, 7, 9])\n\n\nHere, a and b are both 1-dimensional tensors with shape (3,). When performing the addition operation, PyTorch will “broadcast” a to have the same shape as b, resulting in a tensor c with shape (3,) and values [5, 7, 9].\nBroadcasting can also occur when one tensor has fewer dimensions than the other. For example:\n\na = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\nb = torch.tensor([7, 8, 9])\nc = a + b\n\nHere, a has shape (2, 3) and b has shape (3,). To perform the addition, PyTorch will broadcast b to have shape (1, 3), resulting in a tensor c with shape (2, 3) and values.\n\nc\n\ntensor([[ 8, 10, 12],\n        [11, 13, 15]])\n\n\nBroadcasting is a powerful feature in PyTorch that allows for efficient operations between tensors of different shapes, and is an important concept to understand when working with deep learning models."
  },
  {
    "objectID": "basics/2_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "href": "basics/2_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "title": "3  Broadcasting",
    "section": "3.2 Exercise: K Means using Broadcasting principle",
    "text": "3.2 Exercise: K Means using Broadcasting principle\nIn this section we will be using broadcasting principles to showcase the power of broadcasting and implement our own version of K-means clustering which can run on PyTorch. K-Means is a clustering algorithm that is used to group a set of data points into a specified number of clusters. Here is the general pseudocode for the K-Means algorithm:\n\nInitialize the number of clusters, k, and the maximum number of iterations, max_iter.\nRandomly select k data points as the initial centroids for the clusters.\nIterate for a maximum of max_iter times:\n\nAssign each data point to the cluster with the nearest centroid.\nCalculate the new centroid for each cluster by taking the mean of all data points in the cluster.\n\nReturn the final clusters and their centroids.\n\n\n3.2.1 Create some random data\nLet’s try to create some random data using scikit-learn make_blobs function.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\nn_samples = 1500\nX, y = make_blobs(n_samples=n_samples, centers = 3, random_state=3)\n\n## Normalize\nX = (X - X.mean(axis=0))/X.std(axis=0)\nX = torch.from_numpy(X)\ndef plot_cluster(data, y, points=[]):\n    fig, ax = plt.subplots()\n    ax.scatter(data[:,0], data[:,1], c=y, cmap='plasma')\n    for i, point in enumerate(points):\n        ax.plot(*point, markersize=10, marker=\"x\", color='r', mew=5)\n        ax.plot(*point, markersize=5, marker=\"x\", color='b', mew=2)\n\nplot_cluster(X, y)\n\n\n\n\nFig 1. Visualizing randomly created clusters.\n\n\n\n\nAs we can see above, we have created 1500 samples with three clusters.\n\n\n3.2.2 Randomly initialize the centroids\nFor this exercise, we will use a random initialization for the centroids, although there are more sophisticated techniques such as the “kmeans++” method that can be used to improve the convergence of the algorithm. For the sake of simplicity, we will stick with a random initialization.\n\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nprint(f\"Shape: {centroids.shape} \\n {centroids}\")\n\nShape: torch.Size([3, 2]) \n tensor([[ 0.3923, -0.2236],\n        [-0.3195, -1.2050],\n        [ 1.0445, -0.6332]])\n\n\nLet’s visualize the randomly initialized centroids.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 2. Visualizing randomly created centroid and cluster.\n\n\n\n\nAs we can see in the visualization above, the centroids are random.\n\n\n3.2.3 Assign cluster to the nearest centroid\nWe have 1500 samples and three randomly initialized centroids. Now to compute distance between these centroids and samples we can make use of broadcasting which is vectorized and significantly improve our compute times as we don’t need to loop each sample and centroid to calculate distance.\nTo do broadcasting, we need to make sure that the two tensors are compatible for broadcasting, to achieve this we will add a additional dimension using unsqueeze method.\n\nprint(f\"Before unsqueeze: \\n Data shape: {X.shape}, Centroid shape: {centroids.shape}\")\nprint(f\"\\nAfter unsqueeae: \\n Data shape: {X.unsqueeze(1).shape}, Centroid shape: {centroids.unsqueeze(0).shape}\")\n\nBefore unsqueeze: \n Data shape: torch.Size([1500, 2]), Centroid shape: torch.Size([3, 2])\n\nAfter unsqueeae: \n Data shape: torch.Size([1500, 1, 2]), Centroid shape: torch.Size([1, 3, 2])\n\n\nWe can now compute the Euclidean distance between all 1500 samples and the three centroids in a vectorized format. To do this, we will subtract each centroid from the samples, square the differences, and sum them.\n\nsquare_dist = (X.unsqueeze(1) - centroids.unsqueeze(0)).square().sum(axis=-1)\nprint(f\"Distance of 1500 samples with three centroids : {square_dist.shape}\")\n\nDistance of 1500 samples with three centroids : torch.Size([1500, 3])\n\n\nFor assigning sample to the nearest cluster we can use the argmin function to find the cluster with smallest distance for each sample.\n\nprint(pd.DataFrame(square_dist.argmin(-1)).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nWe can see that 974 samples are close to centroid 0 and 515 samples are near to centroid 1 and 11 samples are close to sample 2. Now let’s pack all of the above in a simple function.\n\ndef nearest_centroid(data, points):\n    '''\n    Find nearest centroid for each sample \n    '''\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\n## Check if it works as before\nnc = nearest_centroid(X,centroids)\nprint(pd.DataFrame(nc).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nLet’s visualize the cluster assignment.\n\nplot_cluster(X, nc, centroids)\n\n\n\n\nFig 3. Visualizing newly created cluster based on centroids\n\n\n\n\n\n\n3.2.4 Update centroids based on new clusters\nTo obtain the new centroid coordinates, we need to compute the mean of all the samples that are assigned to the cluster and update the centroids accordingly.\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\nnew_centroids = update_centroids(X, nc)\n\nLets visualize the new updated centroids.\n\nplot_cluster(X, nc, new_centroids)\n\n\n\n\nFig 4. Visualizing newly created centroids based on nearest cluster assignment\n\n\n\n\nWe can see updated centroids moved to the middle of the cluster.\n\n\n3.2.5 Iterate for a maximum of max_iter times\nLet’s set max_iter to 20 and run the cluster assignment and updating centroid for max_iter times.\n\nmax_iter = 20\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nfor _ in range(max_iter):\n    nc = nearest_centroid(X,centroids)\n    centroids = update_centroids(X, nc)\n\nLet’s visualize the centroids after running it max_iter times.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 5. Visualizing final cluster centers with original labels\n\n\n\n\nWe can see now that the centroids have converged to the desired cluster center.\n\n\n3.2.6 Packaging all up\nLet’s package all the above functions to do K means.\n\ndef nearest_centroid(data, points):\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\n\ndef k_means(X, k, max_iter=20, device=\"cpu\"):\n    ## Random initialization\n    if device == \"cpu\": \n        X = X.detach().cpu()\n    else: \n        X = X.to(device)\n    \n    centroids = torch.randn((k,X.shape[1])).to(device)\n    \n    ## Updating centroids for max_iter\n    for iter in range(max_iter): \n        new_centroids = update_centroids(X, nearest_centroid(X,centroids)).to(centroids.dtype)\n        \n        ## Early stopping\n        if torch.equal(centroids,new_centroids): break\n        else: centroids = new_centroids\n            \n    return centroids\n\nLet’s check if the function runs correctly.\n\ncentroids = k_means(X,3)\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 6. Visualizing output cluster centers with original labels\n\n\n\n\nLet’s run the function on GPU.\n\ncentroids = k_means(X,3, device=\"cuda\").detach().cpu()\nplot_cluster(X.detach().cpu(), y, centroids)\n\n\n\n\nFig 7. Visualizing output cluster centers with original labels"
  },
  {
    "objectID": "basics/2_broadcasting.html#conclusion",
    "href": "basics/2_broadcasting.html#conclusion",
    "title": "3  Broadcasting",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nBroadcasting is a powerful feature in PyTorch that allows you to perform arithmetic operations on tensors of different shapes, as long as they are “broadcastable.”\nAs we have seen above, broadcasting allows you to perform operations on tensors of different shapes as if they were the same shape, by repeating or “broadcasting” the values of the smaller tensor along the missing dimensions. This can be a useful way to perform element-wise operations on tensors without having to explicitly pad or resize them."
  },
  {
    "objectID": "basics/3_autograd.html#what-is-autograd",
    "href": "basics/3_autograd.html#what-is-autograd",
    "title": "4  Automatic Differentiation",
    "section": "4.1 What is Autograd?",
    "text": "4.1 What is Autograd?\nIn PyTorch, autograd automatically computes gradients. It is a key part of PyTorch’s deep learning framework, and is used to optimize model parameters during training by computing gradients of the loss function with respect to the model’s parameters.\nAutograd can compute gradients for both scalar and vector-valued functions, and it can do so efficiently for a large variety of differentiable operations, including matrix and element-wise operations, as well as higher-order derivatives.\nLet’s take a simple example of looking at a function. \\[y = a^3 - b^2 + 3\\]\nDifferentiation of this function with respect to a and b is going to be:\n\\[\\frac{dy}{da} = 3a^2\\]\n\\[\\frac{dy}{db} = -2b\\]\nSo if: \\[a = 5, b = 6\\]\nGradient with respect to a and b will be: \\[\\frac{dy}{da} = 3a^2 => 3*5^2 => 75\\]\n\\[\\frac{dy}{db} = -2b => -2*6 => -12\\]\nNow let’s observe these in PyTorch. To make a tensor compute gradients automatically we can initialize them with requires_grad = True.\n\nimport torch\n\n## initializing a anb b with requires grad = True\na = torch.tensor([5.], requires_grad=True)\nb = torch.tensor([6.], requires_grad=True)\n\ny = a**3 - b**2\ny\n\ntensor([89.], grad_fn=<SubBackward0>)\n\n\nTo compute the derivatives we can call backward method and retrieve gradients from a and b calling a.grad and b.grad.\n\ny.backward()\nprint(f\"Gradient of a and b is {a.grad.item()} and {b.grad.item()} respectively.\")\n\nGradient of a and b is 75.0 and -12.0 respectively.\n\n\nAs computed above the gradient of a and b is 75 and -12 respectively."
  },
  {
    "objectID": "basics/3_autograd.html#exercise-linear-regression",
    "href": "basics/3_autograd.html#exercise-linear-regression",
    "title": "4  Automatic Differentiation",
    "section": "4.2 Exercise: Linear regression",
    "text": "4.2 Exercise: Linear regression\nIn this section, we will implement a linear regression model in PyTorch and use the Autograd package to optimize the model’s parameters through gradient descent.\n\n4.2.1 Creating dummy data\nLet’s begin by generating linear data that exhibits linear characteristics. We will use sklearn make_regression function to do the same.\n\n\nCode\n## Importing required functions\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport time\nfrom IPython.display import clear_output\nsns.set_style(\"dark\")\n%matplotlib inline\n\ndef plot_data(x, y, y_pred=None, label=None):\n    clear_output(wait=True)\n    sns.scatterplot(x = X.squeeze(), y=y)\n    if y_pred is not None:\n        sns.lineplot(x = X.squeeze(), y=y_pred.squeeze(), color='red')\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Target\")\n    if label: \n        plt.title(label)\n    plt.show()\n    time.sleep(0.5)\n\n\n\n## Generate some dataset\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=1,\n    n_informative=1,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2\n)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\nplot_data(X,y, label=f\"Coefficient: {coef:.2f}, Bias:{2}\")\n\n\n\n\nFig 1. Visualizing our linear data\n\n\n\n\n\n\n4.2.2 Define a linear regression function\nSince we are only building a simple linear regression with one feature and one bias term. It can be defined as following -\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n        \n    def forward(self, x):\n        return x @ self.w + self.b\n\nLet’s initialize the model and make a random prediction.\n\n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n## Making a random prediction\nwith torch.no_grad():\n    y_pred = model.forward(X).numpy()\n    \n## Plotting the prediction\nplot_data(X, y, y_pred)\n\n\n\n\nFig 2. Visualizing our data with random predictions\n\n\n\n\nAs shown above, our randomly generated predictions do not accurately fit the data. To improve the model’s performance, we can use the autograd function to create a simple gradient descent function called step, which runs one epoch of training. This will allow us to optimize the model’s parameters and improve the accuracy of our predictions.\n\n\n4.2.3 Stochastic Gradient Descent\n\ndef step(X, y, model, lr = 0.1):\n    y_pred = model.forward(X)\n    \n    ## Calculation mean square error\n    loss = torch.square(y - y_pred.squeeze()).mean()\n    \n    ## Computing gradients\n    loss.backward()\n    \n    ## Updating parameters\n    with torch.no_grad():\n        for param in model.params:\n            param -= lr*param.grad.data\n            param.grad.data.zero_()\n    return loss\n\nLets walk through the step function:\n\nThe model performs a forward pass to generate predictions.\nThe mean squared error loss is calculated between the predicted values and the true values.\nThe backward method is used to compute gradients for the model’s parameters.\nThe gradients are updated with the specified learning rate.\nThe gradients are reset to zero for the next iteration.\n\n\nfor i in range(30):\n    # run one gradient descent epoch\n    loss = step(X, y, model)\n    with torch.no_grad():\n        y_pred = model.forward(X).numpy()\n    # plot each step with delay\n    plot_data(X, y, y_pred, label=f\"Step: {i+1}, MSE = {loss:.2f}\")\n\n\n\n\nFig 3. Visualizing the predictions of our trained model over the data\n\n\n\n\nAs observed above, our model’s performance improved with each epoch and the mean squared error (MSE) decreased consistently.\n\nprint(f\"True coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 2.00."
  },
  {
    "objectID": "basics/3_autograd.html#define-a-linear-regression-function",
    "href": "basics/3_autograd.html#define-a-linear-regression-function",
    "title": "4  Automatic Differentiation",
    "section": "4.3 Define a linear regression function",
    "text": "4.3 Define a linear regression function\nSince we are only building a simple linear regression with one feature and one bias term. It can be defined as following -\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n        \n    def forward(self, x):\n        return x @ self.w + self.b\n\nLet’s initialize the model and make a random prediction.\n\n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n## Making a random prediction\nwith torch.no_grad():\n    y_pred = model.forward(X).numpy()\n    \n## Plotting the prediction\nplot_data(X, y, y_pred)\n\n\n\n\nFig 2. Visualizing our data with random prediction\n\n\n\n\nAs shown above, our randomly generated predictions do not accurately fit the data. To improve the model’s performance, we can use the autograd function to create a simple gradient descent function called step, which runs one epoch of training. This will allow us to optimize the model’s parameters and improve the accuracy of our predictions.\n\ndef step(X, y, model, lr = 0.1):\n    y_pred = model.forward(X)\n    \n    ## Calculation mean square error\n    loss = torch.square(y - y_pred.squeeze()).mean()\n    \n    ## Computing gradients\n    loss.backward()\n    \n    ## Updating parameters\n    with torch.no_grad():\n        for param in model.params:\n            param -= lr*param.grad.data\n            param.grad.data.zero_()\n    return loss\n\nLets walk through the step function:\n\nThe model performs a forward pass to generate predictions.\nThe mean squared error loss is calculated between the predicted values and the true values.\nThe backward method is used to compute gradients for the model’s parameters.\nThe gradients are updated with the specified learning rate.\nThe gradients are reset to zero for the next iteration.\n\n\nfor i in range(30):\n    # run one gradient descent epoch\n    loss = step(X, y, model)\n    with torch.no_grad():\n        y_pred = model.forward(X).numpy()\n    # plot each step with delay\n    plot_data(X, y, y_pred, label=f\"Step: {i+1}, MSE = {loss:.2f}\")\n\n\n\n\nAs observed above, our model’s performance improved with each epoch and the mean squared error (MSE) decreased consistently.\n\nprint(f\"True coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 2.00."
  },
  {
    "objectID": "basics/3_autograd.html#conclusion",
    "href": "basics/3_autograd.html#conclusion",
    "title": "4  Automatic Differentiation",
    "section": "4.3 Conclusion",
    "text": "4.3 Conclusion\nAutograd is a key part of PyTorch’s deep learning framework and is an essential tool for optimizing and training neural network models. It is designed to make it easy to implement and train complex models by automatically computing gradients for differentiable operations."
  },
  {
    "objectID": "basics/3_autograd.html#references",
    "href": "basics/3_autograd.html#references",
    "title": "4  Automatic Differentiation",
    "section": "4.4 References",
    "text": "4.4 References\n\nPytorch Auto grad tutorial"
  }
]