[
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "The PyTorch Book",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis is a collection of content compiled during my journey to learn PyTorch, machine learning, and deep learning. The book will primarily cover practical applications of PyTorch and popular frameworks that build upon PyTorch, such as FastAI and Huggingface. Please note that this book is a work in progress and additional chapters will be added over time.I hope that this book, with its collection of theory and practical projects, will be useful for others who are also learning PyTorch."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#what-is-pytorch",
    "href": "basics/0_pytorch_intro.html#what-is-pytorch",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.1 What is PyTorch",
    "text": "1.1 What is PyTorch\n\n\n\nPyTorch is an open-source machine learning library for Python, based on the Torch library, used for applications such as deep learning and GPU based acceleration. It is primarily developed by Facebook’s artificial intelligence research group. PyTorch provides a high-level interface for working with large-scale machine learning models. It allows developers to define and train neural networks in a fast, flexible, and dynamic way.\nSome key features of PyTorch include:\n\nTensor computations with GPU acceleration: PyTorch includes GPU acceleration out of the box, allowing you to train your models on a single or multiple GPUs with ease.\nDynamic computation graphs: PyTorch allows you to define your computations as a dynamic graph, which allows you to change the structure of the graph on the fly and enables you to use dynamic control flow.\nEasy model building: PyTorch provides a wide range of predefined layers and architectures that you can use to build your models, making it easy to get started with machine learning.\nEcosystem support: PyTorch has a large and active community of users, and there are many tools and libraries available that are built on top of PyTorch, such as visualization tools, data loaders, and more.\n\nOverall, PyTorch is a powerful and flexible tool for working with large-scale machine learning models, and is widely used in research and industry.\n\n\n\n\n\n\nNote\n\n\n\n\n\nOn a personal note, I have used many different Deep learning frameworks and I find Pytorch to be the most intuitive. It blends nicely with regular Python and have really good ecosystem. I am also big fan of FastAI library which is written on top of Pytorch and is one of the best framework to do deep learning based project."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#installation",
    "href": "basics/0_pytorch_intro.html#installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.2 Installation",
    "text": "1.2 Installation\nSince, Pytorch is under active development and new version keeps getting released. You want to visit this website for latest Pytorch installation instruction.\n\n\n\nFig 1: Pytorch Installation page.\n\n\nThe above interface is what you will typically see on the installation page, based on your OS and other selection, it will produce a command you can run on your command line interface and install Pytorch. I recommend installing through conda and it will take care of the dependencies automatically."
  },
  {
    "objectID": "basics/0_pytorch_intro.html#checking-installation",
    "href": "basics/0_pytorch_intro.html#checking-installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.3 Checking Installation",
    "text": "1.3 Checking Installation\nLet’s run the import command and see if we have Pytorch setup correctly.\n\nimport torch\nprint(torch.__version__)\n\n1.12.1\n\n\nIt is recommended to use a machine with a GPU when working with PyTorch because it is typically used for GPU-accelerated computations. If you are running PyTorch on a machine with a GPU and want to verify that it has access to the GPU, you can run the following command-\n\ntorch.cuda.is_available()\n\nTrue\n\n\nIf PyTorch is installed on a machine with a GPU, the above command will return True otherwise it will return False."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#what-are-tensors",
    "href": "basics/1_pytorch_tensors.html#what-are-tensors",
    "title": "2  Tensors",
    "section": "2.1 What are Tensors?",
    "text": "2.1 What are Tensors?\nPyTorch provides tensors as its primary data structure. Tensors are similar to numpy arrays, but they can be used on a GPU to accelerate the computation. PyTorch tensors are similar to numpy arrays, but they have additional functionality (automatic differentiation) and are designed to take advantage of GPUs for acceleration. Similar to numpy, tensors in PyTorch supports a variety of operations, including indexing, slicing, math operations, linear algebra operations, and more. Let’s dive in by importing the library.\n\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#initializing-a-tensor",
    "href": "basics/1_pytorch_tensors.html#initializing-a-tensor",
    "title": "2  Tensors",
    "section": "2.2 Initializing a Tensor",
    "text": "2.2 Initializing a Tensor\nThere are several ways to initialize tensors in PyTorch. Here are some examples:\nInitializing from an iterator like a list\n\nimport torch\n\n# Initialize a tensor from a list\ntensor_from_list = torch.tensor([1, 2, 3, 4])\nprint(\"Tensor from list: \\n\", tensor_from_list)\n\n# Initialize a tensor from a nested list\ntensor_from_nested_list = torch.tensor([[1, 2], [3, 4]])\nprint(\"Tensor from nested list: \\n\", tensor_from_nested_list)\n\nTensor from list: \n tensor([1, 2, 3, 4])\nTensor from nested list: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from a numpy array\n\n# Create a NumPy array\nnumpy_array = np.array([[1, 2], [3, 4]])\n\n# Initialize a tensor from a NumPy array\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(\"Tensor from np array: \\n\", tensor_from_numpy)\n\nTensor from np array: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from another tensor\n\n# Create a tensor\noriginal_tensor = torch.tensor([1, 2, 3, 4])\n\n# Initialize a new tensor from the original tensor\nnew_tensor = original_tensor.clone()\nprint(\"Tensor from another tensor: \\n\", new_tensor)\n\nTensor from another tensor: \n tensor([1, 2, 3, 4])\n\n\nConstant or random initialization\n\n# Initialize a tensor with all elements set to zero\ntensor_zeros = torch.zeros(3, 4)\nprint(\"Tensor with all elements set to zero: \\n\", tensor_zeros)\n\n# Initialize a tensor with all elements set to one\ntensor_ones = torch.ones(3, 4)\nprint(\"\\n Tensor with all elements set to one: \\n\", tensor_ones)\n\n# Initialize a tensor with all elements set to a specific value\ntensor_full = torch.full((3, 4), fill_value=2.5)\nprint(\"\\n Tensor with all elements set to a specific value: \\n\", tensor_full)\n\n# Initialize a tensor with random values\ntensor_rand = torch.rand(3, 4)\nprint(\"\\n Tensor with random initialization: \\n\", tensor_rand)\n\n# Initialize a tensor with random values from a normal distribution\ntensor_randn = torch.randn(3, 4)\nprint(\"\\n Tensor with random values from a normal distribution: \\n\", tensor_randn)\n\nTensor with all elements set to zero: \n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n Tensor with all elements set to one: \n tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n Tensor with all elements set to a specific value: \n tensor([[2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000]])\n\n Tensor with random initialization: \n tensor([[0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n Tensor with random values from a normal distribution: \n tensor([[ 1.0550,  0.9214, -1.3023,  0.4119],\n        [-0.4691,  0.8733,  0.7910, -2.3932],\n        [-0.6304, -0.8792,  0.4188,  0.4221]])"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#references",
    "href": "basics/1_pytorch_tensors.html#references",
    "title": "2  Tensors",
    "section": "2.5 References",
    "text": "2.5 References\n\nPytorch tensors tutorial documentation."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#tensor-attributes",
    "href": "basics/1_pytorch_tensors.html#tensor-attributes",
    "title": "2  Tensors",
    "section": "2.3 Tensor Attributes",
    "text": "2.3 Tensor Attributes\nIt has several attributes that you can access to get information about the tensor. Here are some common attributes of a PyTorch tensor:\n\nshape: returns the shape of the tensor as a tuple of integers. For example, if the tensor has dimensions (batch_size, num_channels, height, width), the shape would be (batch_size, num_channels, height, width).\ndtype: returns the data type of the tensor. For example, the data type could be torch.float32 or torch.int64.\ndevice: returns the device on which the tensor is stored. This can be the CPU or a GPU.\nrequires_grad: a boolean flag indicating whether the tensor requires gradient computation. If set to True, the tensor’s gradients will be computed during backpropagation.\ngrad: a tensor containing the gradient of the tensor with respect to some scalar value. This attribute is typically used during training with gradient descent.\n\nYou can access these attributes by calling them on a tensor object. For example:\n\ntensor_randn = torch.randn(3, 4)\nprint(f\"Shape of tensor : {tensor_randn.shape}\")\nprint(f\"Type of tensor : {tensor_randn.dtype}\")\nprint(f\"Device tensor is stored on : {tensor_randn.device}\")\nprint(f\"Autograd enabled : {tensor_randn.requires_grad}\")\nprint(f\"Any stored gradient : {tensor_randn.grad}\")\n\nShape of tensor : torch.Size([3, 4])\nType of tensor : torch.float32\nDevice tensor is stored on : cpu\nAutograd enabled : False\nAny stored gradient : None\n\n\nAs we can see above we initialized a random tensor of shape (3,4) with a torch.float32 data type and its currently on a cpu device. Currently, automatic gradient calculations are disabled and no gradient is stored in the tensor.\nThere are several other attributes that you can access, such as ndim, size, numel, storage, etc. You can find more information about these attributes in the PyTorch Tensor documentation."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#moving-tensor-from-cpu-to-gpu",
    "href": "basics/1_pytorch_tensors.html#moving-tensor-from-cpu-to-gpu",
    "title": "2  Tensors",
    "section": "2.4 Moving tensor from CPU to GPU",
    "text": "2.4 Moving tensor from CPU to GPU\nTo move a tensor from CPU to GPU is a simple command but probably the one which people will use the most.\n\ntensor_randn.to(\"cuda\")\n\ntensor([[-0.0984, -1.3804,  0.3343, -0.1623],\n        [ 0.9155, -0.8620, -0.3943, -0.2997],\n        [-0.1336, -0.7395, -0.7143, -0.0735]], device='cuda:0')\n\n\nAs we can see the tensor_randn is now moved to a cuda(GPU) device."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#tensor-operations",
    "href": "basics/1_pytorch_tensors.html#tensor-operations",
    "title": "2  Tensors",
    "section": "2.4 Tensor Operations",
    "text": "2.4 Tensor Operations\nThere are several operations you can perform on tensors, lets look at the most commonly used operations.\n\n2.4.1 Moving tensor from CPU to GPU\nTo move a tensor from CPU to GPU is a simple command but probably the one which people will use the most.\n\ntensor_randn.to(\"cuda\")\n\ntensor([[-0.0984, -1.3804,  0.3343, -0.1623],\n        [ 0.9155, -0.8620, -0.3943, -0.2997],\n        [-0.1336, -0.7395, -0.7143, -0.0735]], device='cuda:0')\n\n\nAs we can see the tensor_randn is now moved to a cuda(GPU) device.\n\n\n2.4.2 Slicing and Indexing\nPyTorch tensors similar to numpy arrays support various slicing and indexing operations.\n\ntensor_randn = torch.randn(3, 4)\ntensor_randn\n\ntensor([[-1.3470,  0.2204,  0.2963, -0.9745],\n        [ 0.1867, -1.8338, -1.1872, -1.2987],\n        [ 0.0517, -0.3206,  0.3584, -0.4778]])\n\n\n\nprint(f\"First row:  \\n{tensor_randn[0]}\")\nprint(f\"\\n First column: \\n {tensor_randn[:, 0]}\")\nprint(f\"\\n Last column: {tensor_randn[..., -1]}\")\nprint(f\"\\n Selected columns: \\n {tensor_randn[:,2:4]}\")\n## Assignment of column to zero\ntensor_randn[:,1] = 0\nprint(\"\\n Assigning column to zero: \\n\", tensor_randn)\n\nFirst row:  \ntensor([-1.3470,  0.2204,  0.2963, -0.9745])\n\n First column: \n tensor([-1.3470,  0.1867,  0.0517])\n\n Last column: tensor([-0.9745, -1.2987, -0.4778])\n\n Selected columns: \n tensor([[ 0.2963, -0.9745],\n        [-1.1872, -1.2987],\n        [ 0.3584, -0.4778]])\n\n Assigning column to zero: \n tensor([[-1.3470,  0.0000,  0.2963, -0.9745],\n        [ 0.1867,  0.0000, -1.1872, -1.2987],\n        [ 0.0517,  0.0000,  0.3584, -0.4778]])\n\n\n\n\n2.4.3 Concatenation\nThe torch.cat function can be used to concatenate or join multiple tensors together, which is often useful when working with deep learning models.\nLet’s take our previous defined tensors and check their shape.\n\ntensor_ones.shape, tensor_zeros.shape, tensor_rand.shape\n\n(torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]))\n\n\nWe can concatenate these tensors column wise by using torch.cat with dim=1. We will get a resultant tensor with shape (3,12).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=1)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([3, 12])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8675,\n         0.0161, 0.5472, 0.7002],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6551,\n         0.3049, 0.4088, 0.6341],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2363,\n         0.8951, 0.0335, 0.5779]])\n\n\nWe can concatenate these tensors row wise by using torch.cat with dim=0. We will get a resultant tensor with shape (9,4).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=0)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([9, 4])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n\n\n\n2.4.4 Arithmetic operations\nIn PyTorch, you can perform arithmetic operations on tensors in a similar way to how you would perform them on numpy arrays. Lets look at some common you can perform with PyTorch tensors include element wise addition, subtraction, multiplication, division."
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#slicing-and-dicing",
    "href": "basics/1_pytorch_tensors.html#slicing-and-dicing",
    "title": "2  Tensors",
    "section": "2.5 Slicing and Dicing",
    "text": "2.5 Slicing and Dicing"
  },
  {
    "objectID": "basics/1_pytorch_tensors.html#concatenation",
    "href": "basics/1_pytorch_tensors.html#concatenation",
    "title": "2  Tensors",
    "section": "2.5 Concatenation",
    "text": "2.5 Concatenation\nThe torch.cat function can be used to concatenate or join multiple tensors together, which is often useful when working with deep learning models.\nLet’s take our previous defined tensors and check their shape.\n\ntensor_ones.shape, tensor_zeros.shape, tensor_rand.shape\n\n(torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]))\n\n\nWe can concatenate these tensors column wise by using torch.cat with dim=1. We will get a resultant tensor with shape (3,12).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=1)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([3, 12])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8675,\n         0.0161, 0.5472, 0.7002],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6551,\n         0.3049, 0.4088, 0.6341],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2363,\n         0.8951, 0.0335, 0.5779]])\n\n\nWe can concatenate these tensors row wise by using torch.cat with dim=0. We will get a resultant tensor with shape (9,4).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=0)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([9, 4])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])"
  }
]