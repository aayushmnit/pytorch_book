[
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "The PyTorch Book",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis is a collection of content compiled during my journey to learn PyTorch, machine learning, and deep learning. The book will primarily cover practical applications of PyTorch and popular frameworks that build upon PyTorch, such as FastAI and Huggingface. Please note that this book is a work in progress and additional chapters will be added over time.I hope that this book, with its collection of theory and practical projects, will be useful for others who are also learning PyTorch."
  },
  {
    "objectID": "index.html#about-author",
    "href": "index.html#about-author",
    "title": "The PyTorch Book",
    "section": "About Author",
    "text": "About Author\n\n\nAayush Agrawal is a skilled data scientist with expertise in machine learning solutions. He is interested in staying up-to-date with the latest data technologies, including big data platforms, deep learning, optimization methods, and business analytics. At the time of writing, He is working on developing data-driven products to improve recommendations for Microsoft Partners, M365 service administrators, and end-users in order to optimize the use of M365 services. Aayush has previously worked in various industries such as agricultural technology, pharmaceuticals, retail, e-commerce, and ride-sharing.\nRead more about him here."
  },
  {
    "objectID": "index.html#contribute",
    "href": "index.html#contribute",
    "title": "The PyTorch Book",
    "section": "Contribute",
    "text": "Contribute\nThe Pytorch book is a comprehensive, open-source resource on PyTorch and deep learning using only free software in Python. Its goal is to be accessible both financially and intellectually.\nIf you found this book to be valuable, you can support it by sending a Amazon gift card to aayushmnit@gmail.com. These funds will be used to upgrade my machine and buying books/courses to continue my research. Alternatively, you can contribute by fixing typos, suggesting edits, or providing feedback on passages that were unclear. Simply go to the book’s repository and open an issue.\nFinally, if you enjoyed this content, please consider sharing it with others who might find it useful and consider giving it a star✨ on GitHub."
  },
  {
    "objectID": "basics/1_pytorch_intro.html#what-is-pytorch",
    "href": "basics/1_pytorch_intro.html#what-is-pytorch",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.1 What is PyTorch",
    "text": "1.1 What is PyTorch\n\n\n\nPyTorch is an open-source machine learning library for Python, based on the Torch library, used for applications such as deep learning and GPU based acceleration. It is primarily developed by Facebook’s artificial intelligence research group. PyTorch provides a high-level interface for working with large-scale machine learning models. It allows developers to define and train neural networks in a fast, flexible, and dynamic way.\nSome key features of PyTorch include:\n\nTensor computations with GPU acceleration: PyTorch includes GPU acceleration out of the box, allowing you to train your models on a single or multiple GPUs with ease.\nDynamic computation graphs: PyTorch allows you to define your computations as a dynamic graph, which allows you to change the structure of the graph on the fly and enables you to use dynamic control flow.\nEasy model building: PyTorch provides a wide range of predefined layers and architectures that you can use to build your models, making it easy to get started with machine learning.\nEcosystem support: PyTorch has a large and active community of users, and there are many tools and libraries available that are built on top of PyTorch, such as visualization tools, data loaders, and more.\n\nOverall, PyTorch is a powerful and flexible tool for working with large-scale machine learning models and is widely used in research and industry.\n\n\n\n\n\n\nNote\n\n\n\n\n\nOn a personal note, I have used many different Deep learning frameworks and I find Pytorch to be the most intuitive. It blends nicely with regular Python and have really good ecosystem. I am also big fan of FastAI library which is written on top of Pytorch and is one of the best framework to do deep learning based project."
  },
  {
    "objectID": "basics/1_pytorch_intro.html#installation",
    "href": "basics/1_pytorch_intro.html#installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.2 Installation",
    "text": "1.2 Installation\nSince Pytorch is under active development and the new version keeps getting released. You want to visit this website for the latest Pytorch installation instruction.\n\n\n\nFig 1.1: Pytorch Installation page.\n\n\nThe above interface is what you will typically see on the installation page, based on your OS and other selection, it will produce a command you can run on your command line interface and install Pytorch. I recommend installing through conda as it will take care of the dependencies automatically."
  },
  {
    "objectID": "basics/1_pytorch_intro.html#checking-installation",
    "href": "basics/1_pytorch_intro.html#checking-installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.3 Checking Installation",
    "text": "1.3 Checking Installation\nLet’s run the import command and see if we have Pytorch setup correctly.\n\nimport torch\nprint(torch.__version__)\n\n1.12.1\n\n\nIt is recommended to use a machine with a GPU when working with PyTorch because it is typically used for GPU-accelerated computations. If you are running PyTorch on a machine with a GPU and want to verify that it has access to the GPU, you can run the following command-\n\ntorch.cuda.is_available()\n\nTrue\n\n\nIf PyTorch is installed on a machine with a GPU, the above command will return True otherwise it will return False."
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#what-are-tensors",
    "href": "basics/2_pytorch_tensors.html#what-are-tensors",
    "title": "2  Tensors",
    "section": "2.1 What are Tensors?",
    "text": "2.1 What are Tensors?\nPyTorch provides tensors as its primary data structure. Tensors are similar to NumPy arrays, but they can be used on a GPU to accelerate the computation. PyTorch tensors are similar to NumPy arrays, but they have additional functionality (automatic differentiation) and are designed to take advantage of GPUs for acceleration. Similar to NumPy, tensors in PyTorch support a variety of operations, including indexing, slicing, math operations, linear algebra operations, and more. Let’s dive in by importing the library.\n\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#initializing-a-tensor",
    "href": "basics/2_pytorch_tensors.html#initializing-a-tensor",
    "title": "2  Tensors",
    "section": "2.2 Initializing a Tensor",
    "text": "2.2 Initializing a Tensor\nThere are several ways to initialize tensors in PyTorch. Here are some examples:\nInitializing from an iterator like a list\n\n# Initialize a tensor from a list\ntensor_from_list = torch.tensor([1, 2, 3, 4])\nprint(\"Tensor from list: \\n\", tensor_from_list)\n\n# Initialize a tensor from a nested list\ntensor_from_nested_list = torch.tensor([[1, 2], [3, 4]])\nprint(\"Tensor from nested list: \\n\", tensor_from_nested_list)\n\nTensor from list: \n tensor([1, 2, 3, 4])\nTensor from nested list: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from a numpy array\n\n# Create a NumPy array\nnumpy_array = np.array([[1, 2], [3, 4]])\n\n# Initialize a tensor from a NumPy array\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(\"Tensor from np array: \\n\", tensor_from_numpy)\n\nTensor from np array: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from another tensor\n\n# Create a tensor\noriginal_tensor = torch.tensor([1, 2, 3, 4])\n\n# Initialize a new tensor from the original tensor\nnew_tensor = original_tensor.clone()\nprint(\"Tensor from another tensor: \\n\", new_tensor)\n\nTensor from another tensor: \n tensor([1, 2, 3, 4])\n\n\nConstant or random initialization\n\n# Initialize a tensor with all elements set to zero\ntensor_zeros = torch.zeros(3, 4)\nprint(\"Tensor with all elements set to zero: \\n\", tensor_zeros)\n\n# Initialize a tensor with all elements set to one\ntensor_ones = torch.ones(3, 4)\nprint(\"\\n Tensor with all elements set to one: \\n\", tensor_ones)\n\n# Initialize a tensor with all elements set to a specific value\ntensor_full = torch.full((3, 4), fill_value=2.5)\nprint(\"\\n Tensor with all elements set to a specific value: \\n\", tensor_full)\n\n# Initialize a tensor with random values\ntensor_rand = torch.rand(3, 4)\nprint(\"\\n Tensor with random initialization: \\n\", tensor_rand)\n\n# Initialize a tensor with random values from a normal distribution\ntensor_randn = torch.randn(3, 4)\nprint(\"\\n Tensor with random values from a normal distribution: \\n\", tensor_randn)\n\nTensor with all elements set to zero: \n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n Tensor with all elements set to one: \n tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n Tensor with all elements set to a specific value: \n tensor([[2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000]])\n\n Tensor with random initialization: \n tensor([[0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n Tensor with random values from a normal distribution: \n tensor([[ 1.0550,  0.9214, -1.3023,  0.4119],\n        [-0.4691,  0.8733,  0.7910, -2.3932],\n        [-0.6304, -0.8792,  0.4188,  0.4221]])"
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#tensor-attributes",
    "href": "basics/2_pytorch_tensors.html#tensor-attributes",
    "title": "2  Tensors",
    "section": "2.3 Tensor Attributes",
    "text": "2.3 Tensor Attributes\nIt has several attributes that you can access to get information about the tensor. Here are some common attributes of a PyTorch tensor:\n\nshape: returns the shape of the tensor as a tuple of integers. For example, if the tensor has dimensions (batch_size, num_channels, height, width), the shape would be (batch_size, num_channels, height, width).\ndtype: returns the data type of the tensor. For example, the data type could be torch.float32 or torch.int64.\ndevice: returns the device on which the tensor is stored. This can be the CPU or a GPU.\nrequires_grad: a boolean flag indicating whether the tensor requires gradient computation. If set to True, the tensor’s gradients will be computed during backpropagation.\ngrad: a tensor containing the gradient of the tensor with respect to some scalar value. This attribute is typically used during training with gradient descent.\n\nYou can access these attributes by calling them on a tensor object. For example:\n\ntensor_randn = torch.randn(3, 4)\nprint(f\"Shape of tensor : {tensor_randn.shape}\")\nprint(f\"Type of tensor : {tensor_randn.dtype}\")\nprint(f\"Device tensor is stored on : {tensor_randn.device}\")\nprint(f\"Autograd enabled : {tensor_randn.requires_grad}\")\nprint(f\"Any stored gradient : {tensor_randn.grad}\")\n\nShape of tensor : torch.Size([3, 4])\nType of tensor : torch.float32\nDevice tensor is stored on : cpu\nAutograd enabled : False\nAny stored gradient : None\n\n\nAs we can see above we initialized a random tensor of shape (3,4) with a torch.float32 data type and it’s currently on a CPU device. Currently, automatic gradient calculations are disabled and no gradient is stored in the tensor.\nThere are several other attributes that you can access, such as ndim, size, numel, storage, etc. You can find more information about these attributes in the PyTorch Tensor documentation."
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#tensor-operations",
    "href": "basics/2_pytorch_tensors.html#tensor-operations",
    "title": "2  Tensors",
    "section": "2.4 Tensor Operations",
    "text": "2.4 Tensor Operations\nThere are several operations you can perform on tensors, let’s look at the most commonly used operations.\n\n2.4.1 Moving tensor from CPU to GPU\nTo move a tensor from CPU to GPU is a simple command but probably the one which people will use the most.\n\ntensor_randn.to(\"cuda\")\n\ntensor([[-0.0984, -1.3804,  0.3343, -0.1623],\n        [ 0.9155, -0.8620, -0.3943, -0.2997],\n        [-0.1336, -0.7395, -0.7143, -0.0735]], device='cuda:0')\n\n\nAs we can see the tensor_randn is now moved to a Cuda(GPU) device.\n\n\n2.4.2 Slicing and Indexing\nPyTorch tensors similar to NumPy arrays support various slicing and indexing operations.\n\ntensor_randn = torch.randn(3, 4)\ntensor_randn\n\ntensor([[-1.3470,  0.2204,  0.2963, -0.9745],\n        [ 0.1867, -1.8338, -1.1872, -1.2987],\n        [ 0.0517, -0.3206,  0.3584, -0.4778]])\n\n\n\nprint(f\"First row:  \\n{tensor_randn[0]}\")\nprint(f\"\\n First column: \\n {tensor_randn[:, 0]}\")\nprint(f\"\\n Last column: {tensor_randn[..., -1]}\")\nprint(f\"\\n Selected columns: \\n {tensor_randn[:,2:4]}\")\n## Assignment of column to zero\ntensor_randn[:,1] = 0\nprint(\"\\n Assigning column to zero: \\n\", tensor_randn)\n\nFirst row:  \ntensor([-1.3470,  0.2204,  0.2963, -0.9745])\n\n First column: \n tensor([-1.3470,  0.1867,  0.0517])\n\n Last column: tensor([-0.9745, -1.2987, -0.4778])\n\n Selected columns: \n tensor([[ 0.2963, -0.9745],\n        [-1.1872, -1.2987],\n        [ 0.3584, -0.4778]])\n\n Assigning column to zero: \n tensor([[-1.3470,  0.0000,  0.2963, -0.9745],\n        [ 0.1867,  0.0000, -1.1872, -1.2987],\n        [ 0.0517,  0.0000,  0.3584, -0.4778]])\n\n\n\n\n2.4.3 Concatenation\nThe torch.cat function can be used to concatenate or join multiple tensors together, which is often useful when working with deep learning models.\nLet’s take our previously defined tensors and check their shape.\n\ntensor_ones.shape, tensor_zeros.shape, tensor_rand.shape\n\n(torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]))\n\n\nWe can concatenate these tensors column wise by using torch.cat with dim=1. We will get a resultant tensor with shape (3,12).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=1)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([3, 12])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8675,\n         0.0161, 0.5472, 0.7002],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6551,\n         0.3049, 0.4088, 0.6341],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2363,\n         0.8951, 0.0335, 0.5779]])\n\n\nWe can concatenate these tensors row wise by using torch.cat with dim=0. We will get a resultant tensor with shape (9,4).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=0)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([9, 4])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n\n\n\n2.4.4 Arithmetic operations\nIn PyTorch, you can perform arithmetic operations on tensors in a similar way to how you would perform them on numpy arrays. Let’s look at some common arithmetic operations -\nElement wise addition\n\ntnsr1 = torch.randn((3,4))\nprint(f\"Tensor 1: \\n\", tnsr1)\ntnsr2 = torch.randn((3,4))\nprint(f\"\\n Tensor 2: \\n\", tnsr2)\n\n## Addition\ntensor_add = tnsr1 + tnsr2 \nprint(f\"\\n Tensor additions using + : \\n\", tensor_add)\n\ntensor_add = tnsr1.add(tnsr2)\nprint(f\"\\n Tensor additions using .add : \\n\", tensor_add)\n\nTensor 1: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n Tensor 2: \n tensor([[-0.3125,  1.0860,  0.7340,  0.2249],\n        [-0.9887,  0.2265, -0.5214, -1.5676],\n        [ 0.6817,  0.1099, -0.5298, -0.3109]])\n\n Tensor additions using + : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n Tensor additions using .add : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n\nElement wise subtraction\n\n## Subtraction\ntensor_sub = tnsr1 - tnsr2 \nprint(f\"\\n Tensor subtraction using - : \\n\", tensor_sub)\n\ntensor_sub = tnsr1.sub(tnsr2)\nprint(f\"\\n Tensor subtraction using .sub : \\n\", tensor_sub)\n\n\n Tensor subtraction using - : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n Tensor subtraction using .sub : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n\nElement wise multiplication\n\n## Multiplication\ntensor_mul = tnsr1 * tnsr2 \nprint(f\"\\n Tensor element-wise multiplication using * : \\n\", tensor_mul)\n\ntensor_mul = tnsr1.mul(tnsr2)\nprint(f\"\\n Tensor element-wise multiplication using .mul : \\n\", tensor_mul)\n\n\n Tensor element-wise multiplication using * : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n Tensor element-wise multiplication using .mul : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n\nElement wise division\n\n## Division\ntensor_div = tnsr1 / tnsr2 \nprint(f\"\\n Tensor element-wise division using / : \\n\", tensor_div)\n\ntensor_div = tnsr1.div(tnsr2)\nprint(f\"\\n Tensor element-wise division using .div : \\n\", tensor_div)\n\n\n Tensor element-wise division using + : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n Tensor element-wise division using .div : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n\nMatrix multiplication\n\ntensor_mm = tnsr1 @ tnsr2.T\nprint(f\"\\n Tensor matrix multiplication using @: \\n\", tensor_mm)\n\ntensor_mm = tnsr1.matmul(tnsr2.T)\nprint(f\"\\n Tensor matrix multiplication using .matmul: \\n\", tensor_mm)\n\n\n Tensor matrix multiplication using @: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n Tensor matrix multiplication using .matmul: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nObserve that tnsr1 and tnsr2 have shape (3,4). To perform matrix multiplication, we used the .T function to transpose tnsr2, which changed its shape to (4,3). The resulting matrix multiplication has a shape (3,3).\n\n\nSumming it up\nSumming tensors along rows and columns is a common operation. Here is the syntax for this operation:\n\nprint(f\"Tensor: \\n {tnsr1}\" )\nprint(f\"\\n All up sum: \\n {tnsr1.sum()}\")\nprint(f\"\\n Column wise sum: \\n {tnsr1.sum(0)}\")\nprint(f\"\\n Row wise sum: \\n {tnsr1.sum(1)}\")\n\nTensor: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n All up sum: \n -2.48371958732605\n\n Column wise sum: \n tensor([-1.9103, -1.4094,  1.2977, -0.4617])\n\n Row wise sum: \n tensor([-1.5841, -0.9375,  0.0378])"
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#why-gpus",
    "href": "basics/2_pytorch_tensors.html#why-gpus",
    "title": "2  Tensors",
    "section": "2.5 Why GPUs?",
    "text": "2.5 Why GPUs?\nDeep learning models often involve large amounts of matrix operations such as matrix multiplication. Let’s do a speed comparison b/w NumPy CPU implementation, Pytorch CPU, and GPU implementation.\n\n2.5.1 Matrix multiplication using NumPy\nLet’s initialize one tensor of size (1000,64,64) and one tensor of size (64,32) and let’s do a matrix multiplication speed comparison\n\narr1 = np.random.randn(1000, 64, 64)\narr2 = np.random.randn(64, 32)\n\n\n%timeit -n 50 res = np.matmul(arr1, arr2)\n\n9.7 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs we can see matrix multiplication on NumPy which uses a highly optimized matrix multiplication does the above operation in 9.7 milliseconds.\n\n\n2.5.2 Matrix multiplication using PyTorch on CPU\nNow let’s do the same operation using PyTorch tensors on CPU.\n\ntnsr1 = torch.from_numpy(arr1)\ntnsr2 = torch.from_numpy(arr2)\n\n\n%timeit -n 50 res = tnsr1 @ tnsr2\n\n2.78 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nWe can see that PyTorch on CPU performed the same operation in 2.78 milliseconds which is roughly 3 times faster than the NumPy version.\n\n\n2.5.3 Matrix multiplication using pytorch on GPU\nLet’s do the same operation on GPU using Pytorch.\n\ntnsr1 = tnsr1.to(\"cuda\")\ntnsr2 = tnsr2.to(\"cuda\")\n\n\n%timeit -n 50 res = (tnsr1 @ tnsr2)\n\n15.6 µs ± 4.32 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs demonstrated by the Matrix multiplication example, the GPU version was completed in 15.6 microseconds, a significant improvement over both the Pytorch CPU version (which took 2.8 milliseconds) and the NumPy implementation (which took 9.7 milliseconds). This speedup is even more pronounced when working with larger matrices."
  },
  {
    "objectID": "basics/2_pytorch_tensors.html#references",
    "href": "basics/2_pytorch_tensors.html#references",
    "title": "2  Tensors",
    "section": "2.6 References",
    "text": "2.6 References\n\nPytorch tensors tutorial documentation."
  },
  {
    "objectID": "basics/3_broadcasting.html#what-is-broadcasting",
    "href": "basics/3_broadcasting.html#what-is-broadcasting",
    "title": "3  Broadcasting",
    "section": "3.1 What is Broadcasting?",
    "text": "3.1 What is Broadcasting?\nIn PyTorch, broadcasting refers to the automatic expansion of a tensor’s dimensions to match the dimensions of another tensor during an operation. This allows for element-wise operations between tensors of different shapes, as long as certain rules are followed.\nFor example, consider the following operation:\n\nimport torch\n\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nc = a + b\nc\n\ntensor([5, 7, 9])\n\n\nHere, a and b are both 1-dimensional tensors with shape (3,). When performing the addition operation, PyTorch will “broadcast” a to have the same shape as b, resulting in a tensor c with shape (3,) and values [5, 7, 9].\nBroadcasting can also occur when one tensor has fewer dimensions than the other. For example:\n\na = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\nb = torch.tensor([7, 8, 9])\nc = a + b\n\nHere, a has shape (2, 3) and b has shape (3,). To perform the addition, PyTorch will broadcast b to have shape (1, 3), resulting in a tensor c with shape (2, 3) and values.\n\nc\n\ntensor([[ 8, 10, 12],\n        [11, 13, 15]])\n\n\nBroadcasting is a powerful feature in PyTorch that allows for efficient operations between tensors of different shapes and is an important concept to understand when working with deep learning models."
  },
  {
    "objectID": "basics/3_broadcasting.html#general-broadcasting-rules",
    "href": "basics/3_broadcasting.html#general-broadcasting-rules",
    "title": "3  Broadcasting",
    "section": "3.2 General Broadcasting rules",
    "text": "3.2 General Broadcasting rules\nThe rules for broadcasting in PyTorch are the same as those in NumPy.\nWhen comparing two arrays, PyTorch compares their shapes element-wise. It starts with the rightmost dimension and works its way left. Two dimensions are “broadcastable” when -\n\nthey are equal, or\none of them is 1\n\nIf these conditions are not met, a RuntimeError occurs, explaining the size of tensors is incompatible.\nWhen performing a broadcast operation, the resulting array will have the same number of dimensions as the input array with the most dimensions. The size of each dimension in the resulting array will be the maximum size of the corresponding dimension among the input arrays. If an input array is missing a dimension, it is assumed to have a size of one in that dimension.\nLet’s look at some broadcasting examples.\n\n3.2.1 Example 1 : Valid broadcasting\n\ntensor_a = torch.tensor([[1,2],[3,4],[5,6]])\ntensor_b = torch.tensor([[7,8]])\nprint(f\"Shape of tensor a is {tensor_a.shape}\")\nprint(f\"Shape of tensor b is {tensor_b.shape}\")\n\nShape of tensor a is torch.Size([3, 2])\nShape of tensor b is torch.Size([1, 2])\n\n\nIn the example above, tensor_a is of shape (3,2) and tensor_b is of shape (1,2). According to broadcasting principle if we start from the rightmost side -\n\nRightmost dimension is 2 for both tensor_a and tensor_b\nSecond rightmost dimension is 3 for tensor_a and 1 for tensor_b, as one of them is 1, it doesn’t violate the broadcasting rules\n\n\ntensor_c = tensor_a + tensor_b\nprint(f\"Shape of tensor c is {tensor_c.shape}\")\n\nShape of tensor c is torch.Size([3, 2])\n\n\nSince all the broadcasting rules are valid, the resultant sum of the tensors is (3,2) where tensor_b is expanded from size (1,2) to (3,2) and then added element wise with tensor_a.\n\n\n3.2.2 Example 2 : Invalid Broadcasting\n\ntensor_a = torch.tensor([[1,2],[3,4],[5,6]])\ntensor_b = torch.tensor([[7,8,9]])\nprint(f\"Shape of tensor a is {tensor_a.shape}\")\nprint(f\"Shape of tensor b is {tensor_b.shape}\")\n\nShape of tensor a is torch.Size([3, 2])\nShape of tensor b is torch.Size([1, 3])\n\n\nIn the example above, tensor_a is of shape (3,2) and tensor_b is of shape (1,3). According to broadcasting principle if we start from the rightmost side -\n\nRightmost dimension is 2 for tensor_a and 3 for tensor_b, there is a mismatch\nSecond rightmost dimension is 3 for tensor_a and 1 for tensor_b, as one of them is 1, it doesn’t violate the broadcasting rules\n\n\ntensor_c = tensor_a + tensor_b\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\nSince all the broadcasting rules are not valid, the resultant sum leads to a run time error explaining that the rightmost dimension of the tensors is not matching."
  },
  {
    "objectID": "basics/3_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "href": "basics/3_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "title": "3  Broadcasting",
    "section": "3.3 Exercise: K Means using Broadcasting principle",
    "text": "3.3 Exercise: K Means using Broadcasting principle\nIn this section, we will be using broadcasting principles to showcase the power of broadcasting and implement our version of K-means clustering which can run on PyTorch. K-Means is a clustering algorithm that is used to group a set of data points into a specified number of clusters. Here is the general pseudocode for the K-Means algorithm:\n\nInitialize the number of clusters, k, and the maximum number of iterations, max_iter.\nRandomly select k data points as the initial centroids for the clusters.\nIterate for a maximum of max_iter times:\n\nAssign each data point to the cluster with the nearest centroid.\nCalculate the new centroid for each cluster by taking the mean of all data points in the cluster.\n\nReturn the final clusters and their centroids.\n\n\n3.3.1 Create some random data\nLet’s try to create some random data using scikit-learn make_blobs function.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\nn_samples = 1500\nX, y = make_blobs(n_samples=n_samples, centers = 3, random_state=3)\n\n## Normalize\nX = (X - X.mean(axis=0))/X.std(axis=0)\nX = torch.from_numpy(X)\ndef plot_cluster(data, y, points=[]):\n    fig, ax = plt.subplots()\n    ax.scatter(data[:,0], data[:,1], c=y, cmap='plasma')\n    for i, point in enumerate(points):\n        ax.plot(*point, markersize=10, marker=\"x\", color='r', mew=5)\n        ax.plot(*point, markersize=5, marker=\"x\", color='b', mew=2)\n\nplot_cluster(X, y)\n\n\n\n\nFig 3.1. Visualizing randomly created clusters.\n\n\n\n\nAs we can see above, we have created 1500 samples with three clusters.\n\n\n3.3.2 Randomly initialize the centroids\nFor this exercise, we will use a random initialization for the centroids, although there are more sophisticated techniques such as the “kmeans++” method that can be used to improve the convergence of the algorithm. For the sake of simplicity, we will stick with random initialization.\n\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nprint(f\"Shape: {centroids.shape} \\n {centroids}\")\n\nShape: torch.Size([3, 2]) \n tensor([[ 0.3923, -0.2236],\n        [-0.3195, -1.2050],\n        [ 1.0445, -0.6332]])\n\n\nLet’s visualize the randomly initialized centroids.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.2. Visualizing randomly created centroid and cluster.\n\n\n\n\nAs we can see in the visualization above, the centroids are random.\n\n\n3.3.3 Assign cluster to the nearest centroid\nWe have 1500 samples and three randomly initialized centroids. Now to compute the distance between these centroids and samples we can make use of broadcasting which is vectorized and significantly improve our compute times as we don’t need to loop each sample and centroid to calculate distance.\nTo do broadcasting, we need to make sure that the two tensors are compatible for broadcasting, to achieve this we will add an additional dimension using unsqueeze method.\n\nprint(f\"Before unsqueeze: \\n Data shape: {X.shape}, Centroid shape: {centroids.shape}\")\nprint(f\"\\nAfter unsqueeae: \\n Data shape: {X.unsqueeze(1).shape}, Centroid shape: {centroids.unsqueeze(0).shape}\")\n\nBefore unsqueeze: \n Data shape: torch.Size([1500, 2]), Centroid shape: torch.Size([3, 2])\n\nAfter unsqueeae: \n Data shape: torch.Size([1500, 1, 2]), Centroid shape: torch.Size([1, 3, 2])\n\n\nWe can now compute the Euclidean distance between all 1500 samples and the three centroids in a vectorized format. To do this, we will subtract each centroid from the samples, square the differences, and sum them.\n\nsquare_dist = (X.unsqueeze(1) - centroids.unsqueeze(0)).square().sum(axis=-1)\nprint(f\"Distance of 1500 samples with three centroids : {square_dist.shape}\")\n\nDistance of 1500 samples with three centroids : torch.Size([1500, 3])\n\n\nFor assigning a sample to the nearest cluster we can use the argmin function to find the cluster with the smallest distance for each sample.\n\nprint(pd.DataFrame(square_dist.argmin(-1)).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nWe can see that 974 samples are close to centroid 0 and 515 samples are near centroid 1 and 11 samples are close to sample 2. Now let’s pack all of the above in a simple function.\n\ndef nearest_centroid(data, points):\n    '''\n    Find nearest centroid for each sample \n    '''\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\n## Check if it works as before\nnc = nearest_centroid(X,centroids)\nprint(pd.DataFrame(nc).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nLet’s visualize the cluster assignment.\n\nplot_cluster(X, nc, centroids)\n\n\n\n\nFig 3.3. Visualizing newly created cluster based on centroids\n\n\n\n\n\n\n3.3.4 Update centroids based on new clusters\nTo obtain the new centroid coordinates, we need to compute the mean of all the samples that are assigned to the cluster and update the centroids accordingly.\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\nnew_centroids = update_centroids(X, nc)\n\nLets visualize the new updated centroids.\n\nplot_cluster(X, nc, new_centroids)\n\n\n\n\nFig 3.4. Visualizing newly created centroids based on nearest cluster assignment\n\n\n\n\nWe can see updated centroids moved to the middle of the cluster.\n\n\n3.3.5 Iterate for a maximum of max_iter times\nLet’s set max_iter to 20 and run the cluster assignment and update the centroids for max_iter times.\n\nmax_iter = 20\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nfor _ in range(max_iter):\n    nc = nearest_centroid(X,centroids)\n    centroids = update_centroids(X, nc)\n\nLet’s visualize the centroids after running it max_iter times.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.5. Visualizing final cluster centers with original labels\n\n\n\n\nWe can see now that the centroids have converged to the desired cluster center.\n\n\n3.3.6 Packaging all up\nLet’s package all the above functions to do K means.\n\ndef nearest_centroid(data, points):\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\n\ndef k_means(X, k, max_iter=20, device=\"cpu\"):\n    ## Random initialization\n    if device == \"cpu\": \n        X = X.detach().cpu()\n    else: \n        X = X.to(device)\n    \n    centroids = torch.randn((k,X.shape[1])).to(device)\n    \n    ## Updating centroids for max_iter\n    for iter in range(max_iter): \n        new_centroids = update_centroids(X, nearest_centroid(X,centroids)).to(centroids.dtype)\n        \n        ## Early stopping\n        if torch.equal(centroids,new_centroids): break\n        else: centroids = new_centroids\n            \n    return centroids\n\nLet’s check if the function runs correctly.\n\ncentroids = k_means(X,3)\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.6. Visualizing output cluster centers with original labels\n\n\n\n\nLet’s run the function on GPU.\n\ncentroids = k_means(X,3, device=\"cuda\").detach().cpu()\nplot_cluster(X.detach().cpu(), y, centroids)\n\n\n\n\nFig 3.7. Visualizing output cluster centers with original labels"
  },
  {
    "objectID": "basics/3_broadcasting.html#conclusion",
    "href": "basics/3_broadcasting.html#conclusion",
    "title": "3  Broadcasting",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nBroadcasting is a powerful feature in PyTorch that allows you to perform arithmetic operations on tensors of different shapes, as long as they are “broadcastable.”\nAs we have seen above, broadcasting allows you to perform operations on tensors of different shapes as if they were the same shape, by repeating or “broadcasting” the values of the smaller tensor along the missing dimensions. This can be a useful way to perform element-wise operations on tensors without having to explicitly pad or resize them."
  },
  {
    "objectID": "basics/4_autograd.html#what-is-autograd",
    "href": "basics/4_autograd.html#what-is-autograd",
    "title": "4  Automatic Differentiation",
    "section": "4.1 What is Autograd?",
    "text": "4.1 What is Autograd?\nIn PyTorch, autograd automatically computes gradients. It is a key part of PyTorch’s deep learning framework and is used to optimize model parameters during training by computing gradients of the loss function with respect to the model’s parameters.\nAutograd can compute gradients for both scalar and vector-valued functions, and it can do so efficiently for a large variety of differentiable operations, including matrix and element-wise operations, as well as higher-order derivatives.\nLet’s take a simple example of looking at a function. \\[y = a^3 - b^2 + 3\\]\nDifferentiation of this function with respect to a and b is going to be:\n\\[\\frac{dy}{da} = 3a^2\\]\n\\[\\frac{dy}{db} = -2b\\]\nSo if: \\[a = 5, b = 6\\]\nGradient with respect to a and b will be: \\[\\frac{dy}{da} = 3a^2 => 3*5^2 => 75\\]\n\\[\\frac{dy}{db} = -2b => -2*6 => -12\\]\nNow let’s observe these in PyTorch. To make a tensor compute gradients automatically we can initialize them with requires_grad = True.\n\nimport torch\n\n## initializing a anb b with requires grad = True\na = torch.tensor([5.], requires_grad=True)\nb = torch.tensor([6.], requires_grad=True)\n\ny = a**3 - b**2\ny\n\ntensor([89.], grad_fn=<SubBackward0>)\n\n\nTo compute the derivatives we can call the backward method and retrieve gradients from a and b by calling a.grad and b.grad.\n\ny.backward()\nprint(f\"Gradient of a and b is {a.grad.item()} and {b.grad.item()} respectively.\")\n\nGradient of a and b is 75.0 and -12.0 respectively.\n\n\nAs computed above the gradient of a and b is 75 and -12 respectively."
  },
  {
    "objectID": "basics/4_autograd.html#exercise-linear-regression",
    "href": "basics/4_autograd.html#exercise-linear-regression",
    "title": "4  Automatic Differentiation",
    "section": "4.2 Exercise: Linear regression",
    "text": "4.2 Exercise: Linear regression\nIn this section, we will implement a linear regression model in PyTorch and use the Autograd package to optimize the model’s parameters through gradient descent.\n\n4.2.1 Creating dummy data\nLet’s begin by generating linear data that exhibits linear characteristics. We will use the sklearn make_regression function to do the same.\n\n\nCode\n## Importing required functions\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport time\nfrom IPython.display import clear_output\n\nsns.set_style(\"dark\")\n%matplotlib inline\n\ndef plot_data(x, y, y_pred=None, label=None):\n    clear_output(wait=True)\n    sns.scatterplot(x=X.squeeze(), y=y)\n    if y_pred is not None:\n        sns.lineplot(x=X.squeeze(), y=y_pred.squeeze(), color='red')\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Target\")\n    if label:\n        plt.title(label)\n    plt.show()\n    time.sleep(0.5)\n\n\n\n## Generate some dataset\nX, y, coef = make_regression(n_samples=1500,\n                             n_features=1,\n                             n_informative=1,\n                             noise=0.3,\n                             coef=True,\n                             random_state=0,\n                             bias=2)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\nplot_data(X, y, label=f\"Coefficient: {coef:.2f}, Bias:{2}\")\n\n\n\n\nFig 4.1. Visualizing our linear data\n\n\n\n\n\n\n4.2.2 Define a linear regression function\nSince we are only building a simple linear regression with one feature and one bias term. It can be defined as the following -\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n\n    def forward(self, x):\n        return x @ self.w + self.b\n\nThis code above defines a class called Linear that represents a simple linear regression model. In this case, the __init__ takes two arguments: n_in and n_out, which represent the dimensions of the input and output of the linear regression model. The __init__ method initializes the weight matrix w and the bias vector b to random values, and also sets them to be requires_grad to True, which means that the gradients of these parameters will be calculated during backpropagation. The forward method defines the forward pass of the linear regression model. It takes an input x and applies the linear transformation defined by w and b, returning the model prediction.\nLet’s initialize the model and make a random prediction.\n\n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\n## Making a random prediction\nwith torch.no_grad(): \n    y_pred = model.forward(X).numpy()\n\n## Plotting the prediction\nplot_data(X, y, y_pred)\n\n\n\n\nFig 4.2. Visualizing our data with random predictions\n\n\n\n\nThe code above generates random predictions that do not fit the data well. The torch.no_grad() context manager is used to prevent torch from calculating gradients for the operations within the context.\nTo improve the model’s performance, we can use the autograd function to create a simple gradient descent function called step, which runs one epoch of training. This will allow us to optimize the model’s parameters and improve the accuracy of our predictions.\n\n\n4.2.3 Stochastic Gradient Descent\n\ndef step(X, y, model, lr=0.1):\n    y_pred = model.forward(X)\n\n    ## Calculation mean square error\n    loss = torch.square(y - y_pred.squeeze()).mean()\n\n    ## Computing gradients\n    loss.backward()\n\n    ## Updating parameters\n    with torch.no_grad(): \n        for param in model.params:\n            param -= lr * param.grad.data\n            param.grad.data.zero_()\n    return loss\n\nLet’s walk through the step function:\n\nThe model performs a forward pass to generate predictions.\nThe mean squared error loss is calculated between the predicted values and the true values.\nThe backward method is used to compute gradients for the model’s parameters.\nThe gradients are updated with the specified learning rate.\nThe gradients are reset to zero for the next iteration.\n\n\nfor i in range(30):\n    # run one gradient descent epoch\n    loss = step(X, y, model)\n    with torch.no_grad(): \n        y_pred = model.forward(X).numpy()\n    # plot each step with delay\n    plot_data(X, y, y_pred, label=f\"Step: {i+1}, MSE = {loss:.2f}\")\n\n\n\n\nFig 4.3. Visualizing the predictions of our trained model over the data\n\n\n\n\nAs observed above, our model’s performance improved with each epoch and the mean squared error (MSE) decreased consistently.\n\nprint(f\"True coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 2.00."
  },
  {
    "objectID": "basics/4_autograd.html#conclusion",
    "href": "basics/4_autograd.html#conclusion",
    "title": "4  Automatic Differentiation",
    "section": "4.3 Conclusion",
    "text": "4.3 Conclusion\nAutograd is a key part of PyTorch’s deep learning framework and is an essential tool for optimizing and training neural network models. It is designed to make it easy to implement and train complex models by automatically computing gradients for differentiable operations."
  },
  {
    "objectID": "basics/4_autograd.html#references",
    "href": "basics/4_autograd.html#references",
    "title": "4  Automatic Differentiation",
    "section": "4.4 References",
    "text": "4.4 References\n\nPytorch Auto grad tutorial\nPytorch Autograd Mechanics"
  },
  {
    "objectID": "basics/5_datasets.html#introduction-to-pytorch-dataset",
    "href": "basics/5_datasets.html#introduction-to-pytorch-dataset",
    "title": "5  Datasets and Dataloaders",
    "section": "5.1 Introduction to Pytorch Dataset",
    "text": "5.1 Introduction to Pytorch Dataset\nFor training any machine learning models we need data. Typically, this data needs to be represented in form of a PyTorch tensor in order to be fed into a model. In PyTorch, a Dataset is an abstract class that represents a dataset. It provides a way to access the data and defines the way the data should be processed. The Dataset class is an abstract class and you need to create a subclass to use it. If you are not familiar with OOPs fundamentals like abstract base class and subclass, I suggest you to read this blog.\nThe main use of a dataset in PyTorch is to provide a way to access the data that you want to use to train a machine learning model. By creating a subclass of the Dataset class, you can define how the data should be loaded and processed. Once you have created a Dataset subclass, you can use it to create a PyTorch DataLoader, which is an iterator that will yield batches of data from your dataset. You can then use the DataLoader to train a model in PyTorch.\nLet’s look at the Dataset documentation.\n\nfrom torch.utils.data import Dataset\nprint(Dataset.__doc__)\n\nAn abstract class representing a :class:`Dataset`.\n\n    All datasets that represent a map from keys to data samples should subclass\n    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n    data sample for a given key. Subclasses could also optionally overwrite\n    :meth:`__len__`, which is expected to return the size of the dataset by many\n    :class:`~torch.utils.data.Sampler` implementations and the default options\n    of :class:`~torch.utils.data.DataLoader`.\n\n    .. note::\n      :class:`~torch.utils.data.DataLoader` by default constructs a index\n      sampler that yields integral indices.  To make it work with a map-style\n      dataset with non-integral indices/keys, a custom sampler must be provided.\n    \n\n\nAs we can see above Dataset is an abstract base class that requires us to implement the __getitem__ function and optionally overwrite the __len__ method for returning the size of the dataset."
  },
  {
    "objectID": "basics/5_datasets.html#exercise-creating-our-first-custom-dataset-class",
    "href": "basics/5_datasets.html#exercise-creating-our-first-custom-dataset-class",
    "title": "5  Datasets and Dataloaders",
    "section": "5.2 Exercise: Creating our first custom dataset class",
    "text": "5.2 Exercise: Creating our first custom dataset class\nIn this exercise, we will continue from our previous linear regression example where we trained a linear regression using batch gradient descent and replace it with mini-batch gradient descent using Dataset and Dataloaders.\nLet’s start by importing the required libraries and creating our linear data.\n\n\nCode\n## Importing required functions\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport time\nfrom IPython.display import clear_output\nsns.set_style(\"dark\")\n%matplotlib inline\n\ndef plot_data(x, y, y_pred=None, label=None):\n    clear_output(wait=True)\n    sns.scatterplot(x = X.squeeze(), y=y)\n    if y_pred is not None:\n        sns.lineplot(x = X.squeeze(), y=y_pred.squeeze(), color='red')\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Target\")\n    if label: \n        plt.title(label)\n    plt.show()\n    time.sleep(0.5)\n\n## Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=1,\n    n_informative=1,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2\n)\n## Converting it into a Pandas dataframe\ndata = pd.DataFrame({\"X\":X.squeeze(), \"y\":y})\n\n## Visualizing the relationship b/w X and Y\nplot_data(data.X, data.y, label=f\"Coefficient: {coef:.2f}, Bias:{2}\")\n\n## Printing top 5 rows\nprint(data.head())\n\n\n\n\n\nFig 5.1. Visualizing our linear data\n\n\n\n\n          X         y\n0 -0.234216  1.901007\n1 -2.030684  1.274535\n2  0.651781  1.832122\n3  2.014060  2.936113\n4  0.829986  2.488750\n\n\nLet’s create a custom Dataset class named TabularDataset by inheriting the Dataset abstract base class and implementing our __len__ and __getitem__ functions.\n\n\n\n\n\n\nNote\n\n\n\nMethods with double underscores (also known as “dunder” methods) are special methods in Python. When the len() function is called on an object, Python will automatically call the object’s __len__ method to get the length of the object. Similarly, when the object is indexed using the square bracket operator (e.g. obj[key]), Python will call the object’s __getitem__ method to retrieve the value at the specified index.\n\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, data):\n        self.data = data.X\n        self.targets = data.y\n\n    def __len__(self): \n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data.iloc[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float), \n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\nThe TabularDataset class has three methods: __init__, __len__, and __getitem__.\n\nThe __init__ method is called when the class is instantiated and takes a pandas dataframe as input.\nThe __len__ method returns the number of samples in the dataset.\nThe __getitem__ method returns a sample from the dataset at a given index idx, in the form of a dictionary with keys X and y.\n\nWe can create an object from the TabularDataset class using our regression example, and then call the __len__ and__getitem__ methods on it.\n\n# create an object of the TabularDataset class\ncustom_dataset = TabularDataset(data)\n\n# get the length of the dataset\nsize = len(custom_dataset)\nprint(f'Dataset size: {size} \\n')\n\n# get the sample at index 0\nsample = custom_dataset[0]\nprint(f'Indexing on 0 index: \\n {sample}')\n\nDataset size: 1500 \n\nIndexing on 0 index: \n {'X': tensor(-0.2342), 'y': tensor(1.9010)}"
  },
  {
    "objectID": "basics/5_datasets.html#dataloaders",
    "href": "basics/5_datasets.html#dataloaders",
    "title": "5  Datasets and Dataloaders",
    "section": "5.3 Dataloaders",
    "text": "5.3 Dataloaders\nWhile training a machine learning model, it is often more efficient to pass a group of samples, or a “mini-batch,” to the model at once, rather than processing one sample at a time. Additionally, we may want to reshuffle the data at the end of each epoch and use multiple threads to speed up the data loading process.\nThe PyTorch DataLoader class helps us achieve these goals by creating an iterable from our Dataset object. The DataLoader can be used to efficiently batch and shuffle the data, and it can use multiple threads to speed up the data loading process.\nLet’s create a dataloader object from our TabularDataset object.\n\nfrom torch.utils.data import DataLoader\ncustom_dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)\n\nNow let’s look at one minibatch:\n\nbatch = next(iter(custom_dataloader))\nbatch\n\n{'X': tensor([ 1.5328,  0.4394,  1.1542, -0.6743,  0.3194, -0.5863,  0.8216, -0.9489,\n         -1.5408, -1.0546,  0.9501,  0.3382, -0.0357, -0.4675,  0.7231,  0.9694,\n          0.8526, -1.4466, -1.0994, -1.2141, -0.7999,  1.3750, -1.1268, -0.7923,\n          0.0940, -0.1043, -0.0393,  1.2961, -0.4961,  1.0170, -0.6677, -0.7946,\n          0.9364,  2.5944, -0.2201, -0.5376,  1.6581,  0.2348,  0.5766, -1.6326,\n          0.0175, -0.3328, -1.7442, -1.4464,  0.1047,  0.0633, -0.5963,  0.7775,\n         -0.3005, -0.7565, -0.7994, -0.9605,  0.2461, -0.7047,  0.3769,  0.5410,\n         -0.6524,  1.5430,  1.0480, -0.5028,  1.3676, -0.2904,  0.2671,  1.3014]),\n 'y': tensor([2.2464, 2.5879, 2.7877, 1.5178, 2.2373, 1.9258, 2.1885, 2.2265, 1.4833,\n         1.4586, 2.7604, 2.4890, 1.9327, 1.5933, 2.4738, 2.4766, 2.4160, 1.3819,\n         1.4487, 0.8635, 1.4181, 2.8232, 1.2373, 2.0373, 1.7182, 2.0764, 2.1702,\n         2.8312, 1.7150, 2.3457, 1.9804, 1.5520, 2.5604, 3.3382, 1.9031, 1.2880,\n         2.9112, 1.9802, 2.0943, 1.3462, 2.0327, 1.9207, 1.2720, 1.8974, 2.5618,\n         2.4288, 2.0103, 2.5764, 1.4878, 1.6772, 1.6701, 1.5360, 2.3156, 1.7014,\n         2.3102, 2.1018, 2.4023, 2.0447, 2.8422, 1.3625, 2.6827, 1.9267, 2.1790,\n         2.7582])}\n\n\n\nprint(f\"Input feature shape: {batch['X'].shape}\")\nprint(f\"Target  shape: {batch['y'].shape}\")\n\nInput feature shape: torch.Size([64])\nTarget  shape: torch.Size([64])\n\n\nAs we can see above, we got our first batch of 64 data samples."
  },
  {
    "objectID": "basics/5_datasets.html#exercise-linear-regression-with-mini-batch-gradient-descent",
    "href": "basics/5_datasets.html#exercise-linear-regression-with-mini-batch-gradient-descent",
    "title": "5  Datasets and Dataloaders",
    "section": "5.4 Exercise: Linear regression with mini-batch gradient descent",
    "text": "5.4 Exercise: Linear regression with mini-batch gradient descent\nLet’s get our model definition from the last chapter.\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n        \n    def forward(self, x):\n        return x @ self.w + self.b\n    \n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\n## Making a random prediction\nloss = 0\nwith torch.no_grad():\n    for batch in iter(custom_dataloader):\n        y_pred = model.forward(batch['X'].unsqueeze(-1)).numpy()\n        y_true = batch['y'].numpy()\n        loss += sum((y_pred.squeeze() - y_true.squeeze())**2)\nprint(f\"MSE loss: {loss/len(custom_dataset):.4f}\")\n\nMSE loss: 7.2129\n\n\nThis MSE of 7.2129 is bad considering in the last chapter we were able to achieve 0.09. Let’s update the previous chapter step function to take mini-batches.\n\ndef step(custom_dataloader, model, lr = 5e-3):\n    ## Iterate through mini-batch\n    for batch in iter(custom_dataloader):\n        ## Taking one mini-batch\n        y_pred = model.forward(batch['X'].unsqueeze(-1))\n        y_true = batch['y']\n        \n        ## Calculation mean square error per min-batch\n        loss = sum((y_pred.squeeze() - y_true.squeeze())**2)\n    \n        ## Computing gradients per mini-batch\n        loss.backward()\n    \n        ## Updating parameters per mini-batch\n        with torch.no_grad():\n            for param in model.params:\n                param -= lr*param.grad.data\n                param.grad.data.zero_()\n                \n    ## Compute loss for the epoch\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(custom_dataloader):\n            y_pred = model.forward(batch['X'].unsqueeze(-1))\n            y_true = batch['y']\n            loss += sum((y_pred.squeeze() - y_true.squeeze())**2)\n    return loss/len(custom_dataset)\n\nLet’s run few epochs.\n\nmodel = Linear(1,1)\nfor epoch in range(3):\n    loss = step(custom_dataloader, model)\n    print(f\"Epoch: {epoch}, MSE: {loss:.4f}\")\n    \nprint(f\"\\nTrue coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nEpoch: 0, MSE: 0.0879\nEpoch: 1, MSE: 0.0881\nEpoch: 2, MSE: 0.0885\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 1.97.\n\n\nLet’s visualize the fit.\n\ny_pred = []\nwith torch.no_grad():\n        for batch in iter(DataLoader(custom_dataset, batch_size=64, shuffle=False)):\n            y_pred.append(model.forward(batch['X'].unsqueeze(-1)).detach().numpy())      \nplot_data(X, y, y_pred=np.concatenate(y_pred))\n\n\n\n\nFig 5.2. Visualizing our fit\n\n\n\n\nFrom the results above, it appears that the model’s performance improved with each epoch, as the mean squared error (MSE) consistently decreased. The performance of the model is now similar to the performance we observed in the last chapter."
  },
  {
    "objectID": "basics/5_datasets.html#conclusion",
    "href": "basics/5_datasets.html#conclusion",
    "title": "5  Datasets and Dataloaders",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn PyTorch, a Dataset is an abstract class that represents a dataset. It provides a way to access the data and defines the way the data should be processed. The Dataset class is an abstract class and you need to create a subclass to use it.\nA DataLoader is an iterator that provides access to a dataset. It can be used to efficiently batch and shuffle the data, and it can use multiple threads to speed up the data loading process.\nThe Dataset and DataLoader classes are an important part of PyTorch’s data loading and processing functionality. They are often used together to train machine learning model, because they provide a convenient and efficient way to access and process data."
  },
  {
    "objectID": "basics/5_datasets.html#references",
    "href": "basics/5_datasets.html#references",
    "title": "5  Datasets and Dataloaders",
    "section": "5.6 References",
    "text": "5.6 References\n\nPytorch tutorial\nPytorch 101: An applied tutorial - Abhishek Thakur Youtube channel"
  },
  {
    "objectID": "basics/6_optimizers.html#introduction-to-optimizers",
    "href": "basics/6_optimizers.html#introduction-to-optimizers",
    "title": "6  Optimizers and Learning loops",
    "section": "6.1 Introduction to Optimizers",
    "text": "6.1 Introduction to Optimizers\nIn previous chapters, we saw how to load data and trained a linear regression model using mini-batch gradient descent. In practice, we don’t need to write our own implementation of gradient descent as Pytorch provides various inbuilt optimizers algorithm. There are many different optimizers available in PyTorch, and each one has its own set of hyperparameters that can be tuned. Some of the most popular optimizers include:\n\nSGD (Stochastic Gradient Descent): This is a simple optimizer that updates the model’s parameters using the gradient of the loss with respect to the parameters\nAdam (Adaptive Moment Estimation): This optimizer is based on the concept of momentum, which can help the optimizer to converge more quickly to a good solution. Adam also includes adaptive learning rates, which means that the optimizer can automatically adjust the learning rates of different parameters based on the historical gradient information\nRMSprop (Root Mean Square Propagation): This optimizer is similar to Adam, but it uses a different weighting for the gradient history\nAdagrad (Adaptive Gradient Algorithm): This optimizer is designed to handle sparse data, and it adjusts the learning rate for each parameter based on the historical gradient information\nAdadelta: This optimizer is an extension of Adagrad that seeks to reduce its aggressive, monotonically declining learning rate"
  },
  {
    "objectID": "basics/6_optimizers.html#exercise-linear-regression",
    "href": "basics/6_optimizers.html#exercise-linear-regression",
    "title": "6  Optimizers and Learning loops",
    "section": "6.2 Exercise: Linear Regression",
    "text": "6.2 Exercise: Linear Regression\nLet’s look at how we can start using Pytorch’s optimizer by continuing the previous linear regression example. Notice, this time we will use four input features instead of one in our previous examples.\n\n# Importing required functions\nimport torch\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=4,  # Using four features\n    n_informative=4,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2\n)\n\nprint(f'Input feature size: {X.shape}')\n\nInput feature size: (1500, 4)\n\n\nNow we will create a custom Dataset class.\n\n# Creating our custom TabularDataset\nclass TabularDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float),\n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\nWe have modified the TabularDataset class to handle additional features. Now, the class takes two inputs: data which includes our four features, and targets which is our target variable.\n\n# Making a train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33)\n\n# Creating Tabular Dataset\ntrain_dataset = TabularDataset(X_train, y_train)\ntest_dataset = TabularDataset(X_test, y_test)\n\n# Creating Dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nWe have divided our sample into a training set and a test set and used the TabularDataset class to create train and test objects. Finally, we created data loaders for the training set and test set using these objects.\n\n\n\n\n\n\nNote\n\n\n\nIn the code, the training data is shuffled using the Dataloader while the testing data is not. This is a common practice when training a machine learning model.\n\n\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n\n    def forward(self, x):\n        return x @ self.w + self.b\n\n\n# Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\nprint(f\"Shape of weights: {model.w.shape}\")\nprint(f\"Shape of bias: {model.b.shape}\")\n\nShape of weights: torch.Size([4, 1])\nShape of bias: torch.Size([1])\n\n\nWe are using the same linear model as last time but this time it will take four inputs instead of one input.\n\noptimizer = torch.optim.SGD(model.params, lr=1e-3)\n\nNext, we will define our optimizer. We will use PyTorch’s implementation of stochastic gradient descent (SGD) by initializing torch.optim.SGD. Here we are passing the model parameters which need to get modified during the training process and a hyperparameter learning rate (lr) of 1e-3.\nFor more information about other available optimizers and their hyperparameters, you can refer to the PyTorch optimizer documentation at this link.\n\ndef train_one_epoch(model, data_loader, optimizer):\n    for batch in iter(data_loader):\n        # Taking one mini-batch\n        y_pred = model.forward(batch['X']).squeeze()\n        y_true = batch['y']\n\n        # Calculation mean square error per min-batch\n        loss = torch.square(y_pred - y_true).sum()\n\n        # Computing gradients per mini-batch\n        loss.backward()\n\n        # Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n\n\ndef validate_one_epoch(model, data_loader, optimizer):\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(data_loader):\n            y_pred = model.forward(batch['X']).squeeze()\n            y_true = batch['y']\n            loss += torch.square(y_pred - y_true).sum()\n    return loss/len(data_loader)\n\nFor the training loop (defined in train_one_epoch), we will go through each mini-batch and do the following:\n\nUse the model to make a prediction\nCalculate the Mean Squared Error (MSE) and the gradients\nUpdate the model parameters using the optimizer’s step() function\nReset the gradients to zero for the next mini-batch using the optimizer’s zero_grad() function”\n\nIn the validation loop (defined in validate_one_epoch), we will process each mini-batch as follows:\n\nUse the trained model to make a prediction\nCalculate the Mean Squared Error (MSE) loss and return the overall MSE at the end\n\nNow let’s run through some epochs and train our model.\n\nfor epoch in range(10):\n    # run one training loop\n    train_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on training to compute training loss\n    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on testing to compute test loss\n    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n\n    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n\nprint(f\"Actual coefficients are: \\n{np.round(coef,4)} \\nTrained model weights are: \\n{np.round(model.w.squeeze().detach().numpy(),4)}\")\nprint(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{model.b.squeeze().detach().numpy().item():.4f}\")\n\nEpoch 0,Train MSE: 13657.7461 Test MSE: 16039.912\nEpoch 1,Train MSE: 267.4445 Test MSE: 319.128\nEpoch 2,Train MSE: 11.0232 Test MSE: 11.422\nEpoch 3,Train MSE: 5.9071 Test MSE: 5.284\nEpoch 4,Train MSE: 5.8251 Test MSE: 5.184\nEpoch 5,Train MSE: 5.8193 Test MSE: 5.183\nEpoch 6,Train MSE: 5.8243 Test MSE: 5.176\nEpoch 7,Train MSE: 5.8181 Test MSE: 5.243\nEpoch 8,Train MSE: 5.8192 Test MSE: 5.192\nEpoch 9,Train MSE: 5.8160 Test MSE: 5.230\nActual coefficients are: \n[63.0061 44.1452 84.3648  9.3378] \nTrained model weights are: \n[63.0008 44.1527 84.3725  9.3218]\nActual Bias term is 2 \nTrained model bias term is \n1.9968\n\n\nAs shown above, our model has fit the data well. The actual coefficients and bias used to generate the random data roughly match the weights and bias terms of our model."
  },
  {
    "objectID": "basics/6_optimizers.html#conclusion",
    "href": "basics/6_optimizers.html#conclusion",
    "title": "6  Optimizers and Learning loops",
    "section": "6.3 Conclusion",
    "text": "6.3 Conclusion\nIn PyTorch, optimizers are used to update the parameters of a model during training. Optimizers adjust the parameters of the model based on the gradients of the loss function with respect to the parameters, in order to minimize the loss.\nThere are many different optimizers available in PyTorch, including SGD, Adam, RMSprop, and more. You can choose the optimizer that works best for your specific problem and model architecture."
  },
  {
    "objectID": "basics/6_optimizers.html#references",
    "href": "basics/6_optimizers.html#references",
    "title": "6  Optimizers and Learning loops",
    "section": "6.4 References",
    "text": "6.4 References\n\nPytorch Optim Documentation"
  },
  {
    "objectID": "basics/7_defining_models.html#introduction-to-the-torch.nn-module",
    "href": "basics/7_defining_models.html#introduction-to-the-torch.nn-module",
    "title": "7  Defining Model",
    "section": "7.1 Introduction to the torch.nn module",
    "text": "7.1 Introduction to the torch.nn module\nSo far, we have explored various components of Pytorch, such as tensor manipulation, data loading, and parameter optimization. In this chapter, we will delve further into Pytorch by learning about the torch.nn module, which is designed for building and training machine learning models, particularly neural networks. The torch.nn module has a simple and pythonic API that makes it easy to prototype and create complex models with just a few lines of code."
  },
  {
    "objectID": "basics/7_defining_models.html#exercise-linear-regression",
    "href": "basics/7_defining_models.html#exercise-linear-regression",
    "title": "7  Defining Model",
    "section": "7.2 Exercise: Linear Regression",
    "text": "7.2 Exercise: Linear Regression\nTo continue our example of linear regression, we will now see how to use the torch.nn module to replace our custom model class. Before we do that, we will first generate a random linear dataset with four features and split the data into training and testing sets. Then, we will create custom Dataset and DataLoader objects to load the training and testing data in mini-batches.\n\n\nCode\n## Importing required functions\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n## Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=4,  ## Using four features\n    n_informative=4,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2)\n\n\n## Creating our custom TabularDataset\nclass TabularDataset(Dataset):\n\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float),\n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\n\n## Making a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\n## Creating Tabular Dataset\ntrain_dataset = TabularDataset(X_train, y_train)\ntest_dataset = TabularDataset(X_test, y_test)\n\n## Creating Dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n## Training loop\ndef train_one_epoch(model, data_loader, optimizer):\n    for batch in iter(data_loader):\n        ## Taking one mini-batch\n        y_pred = model.forward(batch['X']).squeeze()\n        y_true = batch['y']\n        \n        ## Calculation mean square error per min-batch\n        loss = torch.square(y_pred - y_true).sum()\n    \n        ## Computing gradients per mini-batch\n        loss.backward()\n        \n        ## Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n        \n## Validation loop\ndef validate_one_epoch(model, data_loader, optimizer):\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(data_loader):\n            y_pred = model.forward(batch['X']).squeeze()\n            y_true = batch['y']\n            loss += torch.square(y_pred- y_true).sum()\n    return loss/len(data_loader)\n\n\nThe torch.nn module contains several predefined layers that can be used to create neural networks. These layers can be found in the official PyTorch documentation for the torch.nn module. By using these predefined layers, we can simplify the process of building and training our model, as we don’t have to worry about implementing the details of each layer ourselves. Instead, we can simply specify the layers we want to use and let PyTorch handle the rest.\nNow let’s rewrite the model class using torch.nn module.\n\nclass Linear(nn.Module):\n\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.linear = nn.Linear(n_in, n_out)\n\n    def forward(self, x):\n        return self.linear(x)\n\n## Initializing model\nmodel = Linear(X.shape[1], 1)\nprint(f\"Model: \\n{model}\")\n\nprint(f\"Weights\")\nprint(list(model.parameters())[0])\n\nprint(f\"Bias\")\nprint(list(model.parameters())[1])\n\nModel: \nLinear(\n  (linear): Linear(in_features=4, out_features=1, bias=True)\n)\nWeights\nParameter containing:\ntensor([[ 0.4191,  0.2242, -0.1830,  0.0542]], requires_grad=True)\nBias\nParameter containing:\ntensor([0.4944], requires_grad=True)\n\n\nThe code above defines a class called Linear which extends the functionality of the nn.Module class from PyTorch’s torch.nn module. The Linear class has two methods: __init__ and forward.\n\nThe __init__ method is the constructor for the class. It takes two arguments: n_in and n_out, which represent the number of input and output features, respectively. The method initializes the parent class using super().__init__() and then creates a linear layer using nn.Linear. This layer will have n_in input features and n_out output features.\nThe forward method takes an input tensor x and applies the linear layer to it, returning the result.\n\nAfter the Linear class is defined, an instance of the class is created and assigned to the model variable. The model object has two learnable parameters: the weights and the bias of the linear layer. These parameters can be accessed using the parameters method and indexed using square brackets. The weights are the first element in the list of parameters, and the bias is the second element.\nNow let’s run through some epochs and train our model. We are using the same optimizer, train_one_epoch, and validate_one_epoch from the last chapter.\n\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\nfor epoch in range(10):    \n    # run one training loop\n    train_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on training to compute training loss\n    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on testing to compute test loss\n    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n    \n    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n    \nprint(f\"Actual coefficients are: \\n{np.round(coef,4)} \\nTrained model weights are: \\n{np.round(list(model.parameters())[0].detach().numpy()[0],4)}\")\nprint(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{list(model.parameters())[1].detach().numpy()[0]:.4f}\")\n\nEpoch 0,Train MSE: 14168.0879 Test MSE: 16699.098\nEpoch 1,Train MSE: 285.8107 Test MSE: 347.672\nEpoch 2,Train MSE: 11.2080 Test MSE: 13.391\nEpoch 3,Train MSE: 5.7762 Test MSE: 5.876\nEpoch 4,Train MSE: 5.6652 Test MSE: 5.653\nEpoch 5,Train MSE: 5.6483 Test MSE: 5.556\nEpoch 6,Train MSE: 5.6559 Test MSE: 5.576\nEpoch 7,Train MSE: 5.6767 Test MSE: 5.539\nEpoch 8,Train MSE: 5.6488 Test MSE: 5.557\nEpoch 9,Train MSE: 5.6495 Test MSE: 5.552\nActual coefficients are: \n[63.0061 44.1452 84.3648  9.3378] \nTrained model weights are: \n[62.9917 44.1615 84.3643  9.3292]\nActual Bias term is 2 \nTrained model bias term is \n1.9978\n\n\nAs shown above, our model has fit the data well, just like the last chapter."
  },
  {
    "objectID": "basics/7_defining_models.html#saving-and-loading-models",
    "href": "basics/7_defining_models.html#saving-and-loading-models",
    "title": "7  Defining Model",
    "section": "7.3 Saving and Loading models",
    "text": "7.3 Saving and Loading models\nIf we want to save only the learned parameters from the model, we can use torch.save(model.state_dict()) as follows:\n\npath = \"../models/linear_model.pt\"\ntorch.save(model.state_dict(), path)\n\nTo reload the saved parameters, we first need to initiate the model object and feed the saved model parameters.\n\nmodel_new = Linear(X.shape[1], 1)\nmodel_new.load_state_dict(torch.load(path))\nprint(f\"Loaded model weights are: \\n{np.round(list(model_new.parameters())[0].detach().numpy()[0],4)}\")\nprint(f\"\\nLoaded model bias term is \\n{list(model_new.parameters())[1].detach().numpy()[0]:.4f}\")\n\nLoaded model weights are: \n[62.9917 44.1615 84.3643  9.3292]\n\nLoaded model bias term is \n1.9978"
  },
  {
    "objectID": "basics/7_defining_models.html#exercise-what-is-torch.nn-really",
    "href": "basics/7_defining_models.html#exercise-what-is-torch.nn-really",
    "title": "7  Defining Model",
    "section": "7.4 Exercise: What is torch.nn really?",
    "text": "7.4 Exercise: What is torch.nn really?\nNow that we have a good understanding of the fundamental concepts of Pytorch, I highly recommend reading the tutorial by Jeremy Howard from fast.ai titled “WHAT IS TORCH.NN REALLY?”. This tutorial covers everything we have learned so far and goes into more depth on the torch.nn module by showing how to implement it from scratch. It also introduces a new design pattern for building models using the nn.Sequential object, which allows you to define a model as a sequential chain of different layers. This is a simpler way of creating neural networks compared to writing them from scratch using the nn.Module class"
  },
  {
    "objectID": "basics/7_defining_models.html#references",
    "href": "basics/7_defining_models.html#references",
    "title": "7  Defining Model",
    "section": "7.5 References",
    "text": "7.5 References\n\nWHAT IS TORCH.NN REALLY?\nPytorch Tutorial - Build model\nPytorch torch.nn module"
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#modeling-pipeline-overview",
    "href": "basics/8_NeuralNetworks.html#modeling-pipeline-overview",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.1 Modeling Pipeline overview",
    "text": "8.1 Modeling Pipeline overview\n\n\n\nFig 8.1: Modeling Pipeline in Pytorch\n\n\nLet’s take a look at the different steps involved in creating a typical modeling pipeline in PyTorch -\n\nGetting the data - PyTorch provides several tools for loading and preprocessing data, such as the torchvision library for image-related tasks or torchtext for natural language processing. You can also create custom data loaders to load data in your desired format.\nBuild Dataloaders -Once you have your data, you’ll need to create data loaders, which are responsible for batching and shuffling the data during training. Data loaders are essential for efficient training, as they allow you to load and preprocess data in parallel, making use of the GPU capabilities for faster training.\nDefine Model - Next, you’ll need to define your model architecture. PyTorch provides a wide range of pre-defined layers and modules that you can use to build your neural network. You can also create custom layers or models by subclassing PyTorch’s nn.Module class. Defining your model involves specifying the layers, their connectivity, and any other parameters or hyperparameters that you need for your specific task.\nBuild Optimizer and Scheduler - Once your model is defined, you’ll need to configure an optimizer and a scheduler. The optimizer is responsible for updating the model’s parameters during training to minimize the loss, while the scheduler adjusts the learning rate to optimize the model’s performance. PyTorch provides various optimization algorithms, such as SGD, Adam, or RMSprop, and scheduling techniques like learning rate decay or cyclical learning rates.\nRun training and validation loops - With your data loaders, model, optimizer, and scheduler in place, you’re ready to start the training loop. The training loop typically involves iterating over the data loaders, forwarding the inputs through the model, computing the loss, and backpropagating the gradients to update the model’s parameters. You’ll also need to evaluate your model’s performance on a validation set to monitor its progress during training and avoid overfitting.\nDeploy - Once your model has been trained, you can deploy it for inference on new data. PyTorch provides tools for saving and loading model checkpoints, which allows you to reuse your trained model in different applications. You can deploy your model in a variety of environments, such as edge devices, cloud servers, or web applications, depending on your specific requirements.\n\nIn summary, a typical modeling pipeline in PyTorch involves getting the data, building data loaders, defining the model architecture, configuring the optimizer and scheduler, implementing the training and validation loop, and finally deploying the trained model for inference in various environments.\nLet’s dive into the practical implementation of a modeling pipeline in PyTorch using the popular MNISTdataset as an example. We’ll follow the steps outlined above to build our first neural network from scratch."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#downloading-data-from-kaggle",
    "href": "basics/8_NeuralNetworks.html#downloading-data-from-kaggle",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.2 Downloading Data from Kaggle",
    "text": "8.2 Downloading Data from Kaggle\nThe dataset we will be utilizing is the MNIST png dataset from Kaggle, as opposed to the CSV version, for a more practical experience.\nHere are few steps you need to perform before we download the data -\n\nIf you don’t have a Kaggle account, you can make one for free here.\nTo download the dataset, you will need kaggle installed, you can run the following command in notebook or CLI.\n!pip install kaggle >> /dev/null\nHave a kaggle.json stored in ~/.kaggle. You can get your token by going to Your Profile -> Account -> Create New API Token.\n\nOnce you have the above three steps done, run the API command provided:\n\n!kaggle datasets download -d jidhumohan/mnist-png -p \"../data/\"\n\nDownloading mnist-png.zip to ../data\n 87%|█████████████████████████████████     | 51.0M/58.6M [00:01<00:00, 33.4MB/s]\n100%|██████████████████████████████████████| 58.6M/58.6M [00:01<00:00, 33.9MB/s]\n\n\nTo examine the file system, we will utilize the fastcore Path function. It enhances the functionality of python’s Path class and simplifies the process of inspecting directories and folders.\n\nfrom fastcore.xtras import Path\nzip_path = Path(\"../data/mnist-png.zip\")\nzip_path.exists() # Check if the file exist\n\nTrue\n\n\nThe data has been persisted to the mnist-png.zip file on the local system, within the ../data directory. The next step is to utilize the zipfile package to extract the contents of the archive.\n\n\n\n\n\n\nWarning\n\n\n\nThe execution of the following code block will take a significant amount of time(6-10 mins) as it involves the extraction of 70,000 PNG images.\n\n\n\n# Output directory\ndPath = Path(\"../data/\")\n\n# Unzipping data file in output directory\nimport zipfile\nwith zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n    zip_ref.extractall(str(dPath))\n\n# Removing the original zip file\nzip_path.unlink()\n\n# Removing duplicate folder in the unzipped data\nimport shutil\ndPath = dPath/'mnist_png'\nshutil.rmtree(dPath/'mnist_png')\n\nNext, we inspect the extracted folder.\n\ndPath.ls()\n\n(#2) [Path('../data/mnist_png/testing'),Path('../data/mnist_png/training')]\n\n\nData contains of two folder training and testing. Next, we inspect training folder.\n\n(dPath/'training').ls()\n\n(#10) [Path('../data/mnist_png/training/0'),Path('../data/mnist_png/training/1'),Path('../data/mnist_png/training/2'),Path('../data/mnist_png/training/3'),Path('../data/mnist_png/training/4'),Path('../data/mnist_png/training/5'),Path('../data/mnist_png/training/6'),Path('../data/mnist_png/training/7'),Path('../data/mnist_png/training/8'),Path('../data/mnist_png/training/9')]\n\n\nThe training folder comprises of subfolders for each digit ranging from 0 to 9.\n\n(dPath/'training/0').ls()\n\n(#5923) [Path('../data/mnist_png/training/0/1.png'),Path('../data/mnist_png/training/0/1000.png'),Path('../data/mnist_png/training/0/10005.png'),Path('../data/mnist_png/training/0/10010.png'),Path('../data/mnist_png/training/0/10022.png'),Path('../data/mnist_png/training/0/10025.png'),Path('../data/mnist_png/training/0/10026.png'),Path('../data/mnist_png/training/0/10045.png'),Path('../data/mnist_png/training/0/10069.png'),Path('../data/mnist_png/training/0/10071.png')...]\n\n\nEach of these digit subfolders contains images. We will proceed to load a few of these images.\n\nfrom PIL import Image\nfrom IPython.display import display\nfor img in [Image.open((dPath/'training/0').ls()[0]), \n            Image.open((dPath/'training/1').ls()[0]),\n            Image.open((dPath/'training/2').ls()[0]),\n            Image.open((dPath/'training/3').ls()[0])]: \n    display(img)"
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#creating-dataset-object",
    "href": "basics/8_NeuralNetworks.html#creating-dataset-object",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.3 Creating Dataset Object",
    "text": "8.3 Creating Dataset Object\nAs previously discussed, prior to training the model, it is necessary to establish a data pipeline in PyTorch. This includes defining a Dataset object and subsequently loading it via a PyTorch Dataloader.\n\n8.3.1 Using Pure Pytorch\nInitially, we will demonstrate the process of constructing a custom image Dataset object using pure PyTorch. To begin, we will import the necessary libraries.\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport glob\nimport numpy as np\n\nThe glob library can be utilized to obtain the filepaths of all images within a directory.\n\ntrain_paths = glob.glob(str(dPath/'training/**/*.png'))\ntest_paths = glob.glob(str(dPath/'testing/**/*.png'))\nprint(f'Training images count: {len(train_paths)} \\nTesting images count: {len(test_paths)}')\nprint(train_paths[0:5])\n\nTraining images count: 60000 \nTesting images count: 10000\n['../data/mnist_png/training/0/1.png', '../data/mnist_png/training/0/1000.png', '../data/mnist_png/training/0/10005.png', '../data/mnist_png/training/0/10010.png', '../data/mnist_png/training/0/10022.png']\n\n\nBy utilizing glob, we have successfully obtained the filepaths of all images within the training and testing folders. We can see there are 60,000 training images and 10,000 testing images. The next step is to extract the labels from the folder names.\n\ntrain_targets = list(map(lambda x: int(x.split('/')[-2]), train_paths))\ntest_targets  = list(map(lambda x: int(x.split('/')[-2]), test_paths))\nprint(f'Training labels count: {len(train_targets)} \\nTesting labels count: {len(test_targets)}')\nprint(np.random.choice(np.array(train_targets),5))\n\nTraining labels count: 60000 \nTesting labels count: 10000\n[2 3 8 5 7]\n\n\nNow let’s define our custom image Dataset class.\n\nclass ImageDataset(Dataset):\n    def __init__(self, X, y):\n        self.img_paths = X\n        self.targets  = y\n\n    def __len__(self): \n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        current_sample = torch.tensor(np.array(Image.open(self.img_paths[idx]))).flatten()/255.\n        current_target = self.targets[idx]\n        return (\n            current_sample, \n            current_target\n        )\n\nAs we can see above, ImageDataset is a custom PyTorch Dataset class. Let’s walk through the components -\n\nThe class takes two inputs in its constructor, X and y, which are lists of image file paths and corresponding labels respectively. These are stored as class variables self.img_paths and self.targets.\nThe __len__ method returns the number of images in the dataset by returning the length of self.img_paths list.\nThe __getitem__ method is called when a specific sample is requested from the dataset. It takes an index as an argument, and returns a tuple of the image data and the corresponding label for that index. The image is processed as follows -\n\nIt opens the image file at the index passed in the argument using PIL(Python Imaging Library) Image.open function\nConverts it to a numpy array\nFlattens it (convert it from 28x28 2d array to 784 1-d array)\nNormalizes it by dividing by 255 floating number\n\n\nWe will now proceed to instantiate our ImageDataset class for both thetraining and testing datasets\n\ntrain_ds = ImageDataset(X=train_paths, y=train_targets)\ntest_ds = ImageDataset(X=test_paths, y=test_targets)\n\nprint(f'One object: Image Tensor of shape {train_ds[0][0].shape}, Label: {train_ds[0][1]}')\nprint(f'One object: Image Tensor of shape {train_ds[20000][0].shape}, Label: {train_ds[20000][1]}')\n\nOne object: Image Tensor of shape torch.Size([784]), Label: 0\nOne object: Image Tensor of shape torch.Size([784]), Label: 3\n\n\n\n\n8.3.2 Using Torchvision\nWe have demonstrated the procedure of creating a custom ImageDataset object. Now we will examine how to simplify this process by utilizing the torchvision package. The torchvision package encompasses commonly used datasets, model architectures, and image transformations for computer vision tasks.\nTo begin, we will import the datasets and transforms modules from the torchvision package.\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\n\nNext we will use datasets and transform modules to load our MNIST images.\n\ntransform = transforms.Compose([\n    transforms.Grayscale(),\n    transforms.ToTensor(), \n    transforms.Lambda(lambda x: torch.flatten(x))\n    ])\n\n## Create a dataset\ntrain_ds = datasets.ImageFolder(root = dPath/'training/', \n                                      transform=transform)\n\ntest_ds = datasets.ImageFolder(root=dPath/'testing', transform=transform)\n\nprint(f'Length of train dataset: {len(train_ds)}, test_dataset: {len(test_ds)}')\nprint(f'One object: Image Tensor of shape {train_ds[0][0].shape}, Label: {train_ds[0][1]}')\n\nLength of train dataset: 60000, test_dataset: 10000\nOne object: Image Tensor of shape torch.Size([784]), Label: 0\n\n\nLet’s look at the code above:\nThe first step is to define a transform object using the transforms.Compose function. This function takes a list of transformation functions as an argument and applies them in the order they are passed in. In this case, the following transformations are applied:\n\ntransforms.Grayscale(): Convert the images to grayscale\ntransforms.ToTensor(): Converts the images to PyTorch tensors\ntransforms.Lambda(lambda x: torch.flatten(x)): Flatten the tensors from 28x28 2-D arrayto 784 1-D array\n\nNext, it creates two datasets for training and testing using the datasets.ImageFolder class. It takes the root directory of the dataset and the transform object as the arguments. It automatically creates a label for each image by taking the name of the folder where the image is stored.\nThe code then prints the length of the train and test datasets and the shape and label of the first object in the train dataset. The datasets.ImageFolder class is a convenient way to create a Pytorch dataset from a directory of images and it is useful when you have the data in a structured way."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#create-a-dataloader",
    "href": "basics/8_NeuralNetworks.html#create-a-dataloader",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.4 Create a Dataloader",
    "text": "8.4 Create a Dataloader\nCreate a dataloader using torch.utils.data.DataLoader function.\n\nimport os\nnum_workers = int(os.cpu_count()/2)\ntrain_dls = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=num_workers)\ntest_dls = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=num_workers)\n\nThe torch.utils.data.DataLoader class takes a dataset object as an argument and returns an iterator over the dataset object. It can be used to load the data in batches, shuffle the data, and apply other useful functionality.\nIn the above code, following parameters are passed to the DataLoader:\n\ntrain_ds and test_ds are the training and testing datasets respectively.\nbatch_size=128: The number of samples per batch.\nshuffle=True for the training dataset, and shuffle=False for the testing dataset: whether to shuffle the data before iterating through it.\nnum_workers=num_workers: the number of worker threads to use for loading the data. Here it is set to half of the number of CPU cores using os.cpu_count() method.\n\nIt returns two data loaders, one for the training dataset and one for the testing dataset. The data loaders can be used as iterators to access the data in batches. This allows to load the data in smaller chunks, making it more memory efficient and faster to train.\nLet’s look at one batch.\n\nbatch = next(iter(train_dls))\nbatch[0].shape, batch[1]\n\n(torch.Size([128, 784]),\n tensor([5, 4, 1, 5, 7, 5, 4, 7, 2, 1, 5, 7, 6, 5, 8, 6, 3, 7, 8, 0, 4, 4, 4, 0,\n         6, 7, 1, 4, 0, 6, 3, 9, 1, 0, 1, 9, 4, 1, 0, 1, 9, 3, 8, 2, 6, 2, 1, 2,\n         1, 0, 2, 4, 7, 4, 7, 3, 3, 4, 3, 3, 4, 4, 7, 3, 3, 4, 6, 5, 1, 0, 2, 3,\n         0, 4, 5, 7, 1, 5, 0, 1, 1, 3, 0, 0, 1, 4, 0, 6, 2, 3, 8, 1, 8, 1, 2, 5,\n         5, 8, 9, 9, 9, 3, 1, 1, 3, 4, 1, 7, 8, 0, 1, 1, 2, 9, 1, 5, 3, 4, 0, 6,\n         1, 4, 0, 8, 9, 1, 7, 4]))\n\n\nAs we can observe, each batch comprises of an input tensor of shape (128x784) representing 128 images of flattened (28x28) dimension, and a label tensor of shape (128) representing the corresponding digit labels for the images."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#defining-our-training-and-validation-loops",
    "href": "basics/8_NeuralNetworks.html#defining-our-training-and-validation-loops",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.5 Defining our Training and Validation loops",
    "text": "8.5 Defining our Training and Validation loops\nWe will now implement the training loop. It is similar to the training loop we constructed in chapter 6.\n\n## Training loop\ndef train_one_epoch(model, data_loader, optimizer, loss_func):\n    total_loss, nums = 0, 0\n    for batch in tqdm(iter(data_loader)):\n        ## Taking one mini-batch\n        xb, yb = batch[0].to(dev), batch[1].to(dev)\n        y_pred = model.forward(xb)\n        \n        ## Calculation mean square error per min-batch\n        nums += len(yb)\n        loss = loss_func(y_pred, yb)\n        total_loss += loss.item() * len(yb)\n\n        ## Computing gradients per mini-batch\n        loss.backward()\n        \n        ## Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return  total_loss / nums\n        \n\nThe train_one_epoch function takes 4 arguments:\n\nmodel: The model to be trained\ndata_loader: The data loader for the training dataset\noptimizer: The optimizer used to update the model parameters\nloss_func: The loss function used to calculate the error of the model\n\nThe function uses a for loop to iterate through the data loader. For each mini-batch of data, it performs the following steps:\n\nIt loads the data and the labels from the data loader and sends it to the device.\nIt makes a forward pass through the model to get the predictions and then calculates the loss using the loss function.\nIt computes the gradients of the model parameters with respect to the loss.\nIt updates the model parameters using the optimizer and zero the gradients.\nThe total_loss and nums variables are used to keep track of the total loss and number of samples seen during the epoch.\n\n\ndef validate_one_epoch(model, data_loader, loss_func):\n    loss, nums, acc = 0, 0, 0\n    with torch.no_grad():\n        for batch in tqdm(iter(data_loader)):\n            xb, yb = batch[0].to(dev), batch[1].to(dev)\n            y_pred = model.forward(xb)\n            nums += len(yb)\n            loss += loss_func(y_pred, yb).item() * len(yb)\n            acc += sum(y_pred.argmax(axis=1) == yb).item()\n    return loss/nums, acc/nums\n\nThe validate_one_epoch function takes 3 arguments:\n\nmodel: The model to be validated\ndata_loader: The data loader for the validation dataset\nloss_func: The loss function used to calculate the error of the model\n\nThis function also uses a for loop to iterate through the data loader. For each mini-batch of data, it performs the following steps:\n\nIt loads the data and the labels from the data loader and sends it to the device.\nIt makes a forward pass through the model to get the predictions and then calculates the loss using the loss function.\nIt compares the predictions to the labels to calculate the accuracy.\nThe loss, nums, and acc variables are used to keep track of the total loss, number of samples seen during the epoch and accuracy respectively."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#training-using-a-fully-connected-multi-layer-perceptron-model",
    "href": "basics/8_NeuralNetworks.html#training-using-a-fully-connected-multi-layer-perceptron-model",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.6 Training using a Fully Connected/ Multi Layer Perceptron Model",
    "text": "8.6 Training using a Fully Connected/ Multi Layer Perceptron Model\nLet’s define our model.\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(n_in, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, n_out)\n        )\n    def forward(self, x):\n        return self.model(x)\n\nThe code above defines an MLP model as a Pytorch nn.Module class. The class takes in two arguments, n_in and n_out which represents the number of input features and the number of output features of the model respectively. The class is a simple Multi-layer Perceptron model with 3 hidden layers. Each hidden layer have a linear layer with a ReLU activation function. The forward method takes in input tensor x and returns the output by passing it through the defined sequential model.\nLet’s define our training parameters.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nloss_func = nn.CrossEntropyLoss()\nmodel = MLP(784,10).to(dev)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\nepochs = 5\n\nThis code is preparing the model, loss function, optimizer, and the number of training epochs to train the MLP model.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"): This line of code is determining which device to use for training. If a CUDA-enabled GPU is available, the model and data will be moved to the GPU for faster training, otherwise it will use the CPU.\nloss_func = nn.CrossEntropyLoss(): This line of code is defining the loss function for the model. CrossEntropyLoss is a commonly used loss function for multi-class classification problems.\nmodel = MLP(784,10).to(dev): This line of code is instantiating the MLP model with 784 input features and 10 output features, and then moving it to the device.\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3): This line of code is creating an optimizer with Stochastic Gradient Descent (SGD) algorithm and a learning rate of 1e-3. The optimizer updates the model parameters during training to minimize the loss function.\nepochs = 5: This line of code is specifying the number of training epochs. An epoch is one complete pass through the entire training dataset.\n\nWe will now evaluate the performance of our model on the validation dataset before training.\n\ntest_loss, test_acc = validate_one_epoch(model=model, data_loader=test_dls, loss_func=loss_func)\nprint(f\"Random model: Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n\n\n\nRandom model: Test Loss: 2.3025, Test Accuracy: 0.0772\n\n\nAs anticipated, the model’s accuracy is low, around 7-8%, due to the fact that it has not been trained yet.\nWe will now encapsulate our previously defined functions in a fit function, which will be responsible for both training and evaluating the model.\n\ndef fit(epochs, model, loss_func, opt, train_dls, valid_dls):\n    for epoch in range(5):    \n        train_loss = train_one_epoch(model=model, data_loader=train_dls, optimizer=optimizer, loss_func=loss_func)\n        test_loss, test_acc = validate_one_epoch(model=model, data_loader=valid_dls, loss_func=loss_func)\n        print(f\"Epoch {epoch+1},Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Valid Accuracy: {test_acc:.4f}\")\n\nThe fit function uses a for loop to iterate over the number of training epochs. In each iteration, it calls the following functions:\n\ntrain_one_epoch: It trains the model for one epoch using the training data and optimizer.\nvalidate_one_epoch: It evaluates the model on the validation dataset and returns the loss and accuracy.\n\nIt prints the training loss, validation loss and validation accuracy for each epoch. Let’s use the fit function to train our model.\n\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 2.2945, Test Loss: 2.2852, Valid Accuracy: 0.1614\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 2.2770, Test Loss: 2.2659, Valid Accuracy: 0.2339\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 2.2564, Test Loss: 2.2426, Valid Accuracy: 0.3347\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 2.2310, Test Loss: 2.2131, Valid Accuracy: 0.4615\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 2.1983, Test Loss: 2.1752, Valid Accuracy: 0.5695\n\n\nAs we can observe, the model is training effectively and we were able to increase the accuracy from 7-8% to 57% by training for only five epochs.\nNow, we will replace the optimizer in our fitfunction to the AdamW optimizer from the torch.optim module, and rerun the fit function.\n\nmodel = MLP(784,10).to(dev)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 0.3480, Test Loss: 0.1660, Valid Accuracy: 0.9501\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 0.1392, Test Loss: 0.1289, Valid Accuracy: 0.9597\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 0.0895, Test Loss: 0.0931, Valid Accuracy: 0.9699\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 0.0659, Test Loss: 0.0758, Valid Accuracy: 0.9759\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 0.0490, Test Loss: 0.0700, Valid Accuracy: 0.9797\n\n\nBy utilizing the AdamW optimizer and MLP model, we can see that after 5 epochs, we have a highly accurate model with a 98% accuracy as compared to random prediction of 7-8%."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#training-using-a-simple-cnn-model",
    "href": "basics/8_NeuralNetworks.html#training-using-a-simple-cnn-model",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.7 Training using a simple CNN model",
    "text": "8.7 Training using a simple CNN model\nAs previously demonstrated, the fit function is highly adaptable as we were able to change our optimizer without making any modifications to the function. Now, we will replace our MLP model with a CNN (Convolutional Neural Network) model. We will begin by defining a basic CNN network.\n\nimport torch.nn.functional as F\nclass Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nThe code above defines a class called Mnist_CNN which is a subclass of nn.Module. It creates an object of the class and initiates three 2D convolutional layers(conv1, conv2, conv3) with different input and output channels, kernel size, stride and padding. The forward method applies the convolution operation on the input tensor with relu activation function, then average pooling is applied to the output tensor and the final output tensor is reshaped to a 1-D tensor.\nNow, we can pass an instance of this model to the fit function for training and validation.\n\nmodel = Mnist_CNN().to(dev)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 1.8228, Test Loss: 1.4976, Valid Accuracy: 0.5442\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 1.3562, Test Loss: 1.2602, Valid Accuracy: 0.5958\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 1.2113, Test Loss: 1.1522, Valid Accuracy: 0.6144\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 1.1286, Test Loss: 1.0886, Valid Accuracy: 0.6187\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 1.0741, Test Loss: 1.0454, Valid Accuracy: 0.6308\n\n\nAs can be observed, we are able to seamlessly switch from an MLP to a CNN model by utilizing the adaptable fit function and train the model."
  },
  {
    "objectID": "basics/8_NeuralNetworks.html#conclusion",
    "href": "basics/8_NeuralNetworks.html#conclusion",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.8 Conclusion",
    "text": "8.8 Conclusion\nIn this chapter, we progressed from a basic linear regression example to building an image classifier using MLP and CNN models. We gained practical experience in creating custom Dataset and Dataloader objects and were introduced to the torchvision library for simplifying this process. Additionally, we developed a versatile fit function, which can be utilized with various models, optimizers, and loss functions for training our models.\nThe idea of flexibility as demonstrated in the fit function is not unique, and there are many frameworks that aim to simplify the model training process by offering high-level APIs, allowing machine learning scientists to focus on building and solving problems, while the frameworks handle the majority of the complexity. Later in the book, we will repeat the same exercise using the fastai library, which is a highly flexible and performant framework built on top of PyTorch, and observe how we can construct neural networks with minimal lines of code."
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#what-is-fastai",
    "href": "basics/9_IntroductionToFastAI.html#what-is-fastai",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.1 What is fastai?",
    "text": "9.1 What is fastai?\nAs mentioned in the previous chapter, we will now look at a extremely popular deep learning framework called fastai. fastai is an open-source software library for machine learning, which provides high-level APIs for deep learning applications. It is built on top of PyTorch and is designed to be easy to use and flexible, allowing developers to quickly prototype and implement their ideas. fastai provides a wide range of pre-trained models, including state-of-the-art models for computer vision, natural language processing, and recommendation systems. Additionally, fastai offers a number of useful tools and utilities for data processing and model evaluation, making it a popular choice among researchers and practitioners alike.\nThe creators of fastai have also created accompanying educational resources to assist in learning deep learning through the use of their framework. Both the course and book are highly recommended.\n\nHere is the link to the free course: Link\nHere is the link to the book: Link\n\nThe creators also have a peer-review paper published explaining high-level functionality and layered approach to the fastai library- Link.\n\n\n\nFig 9.1: The layered API from fastai."
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#introduction-to-fastai-api",
    "href": "basics/9_IntroductionToFastAI.html#introduction-to-fastai-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.2 Introduction to fastai API",
    "text": "9.2 Introduction to fastai API"
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#mnist-example-using-high-level-api",
    "href": "basics/9_IntroductionToFastAI.html#mnist-example-using-high-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.3 MNIST example using High level API",
    "text": "9.3 MNIST example using High level API"
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#mnist-example-using-mid-level-api",
    "href": "basics/9_IntroductionToFastAI.html#mnist-example-using-mid-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.4 MNIST example using Mid level API",
    "text": "9.4 MNIST example using Mid level API"
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#mnist-example-using-low-level-api",
    "href": "basics/9_IntroductionToFastAI.html#mnist-example-using-low-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.5 MNIST example using Low level API",
    "text": "9.5 MNIST example using Low level API"
  },
  {
    "objectID": "basics/9_IntroductionToFastAI.html#creating-a-dataloader",
    "href": "basics/9_IntroductionToFastAI.html#creating-a-dataloader",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.6 Creating a dataloader",
    "text": "9.6 Creating a dataloader\n\nfrom fastai.vision.all import *\n\n\ndPath = Path(\"../data/mnist_png/\")\n\n\ndPath.ls()\n\n(#3) [Path('../data/mnist_png/models'),Path('../data/mnist_png/testing'),Path('../data/mnist_png/training')]\n\n\n\nget_image_files(dPath)\n\n(#70000) [Path('../data/mnist_png/testing/0/10.png'),Path('../data/mnist_png/testing/0/1001.png'),Path('../data/mnist_png/testing/0/1009.png'),Path('../data/mnist_png/testing/0/101.png'),Path('../data/mnist_png/testing/0/1034.png'),Path('../data/mnist_png/testing/0/1047.png'),Path('../data/mnist_png/testing/0/1061.png'),Path('../data/mnist_png/testing/0/1084.png'),Path('../data/mnist_png/testing/0/1094.png'),Path('../data/mnist_png/testing/0/1121.png')...]\n\n\n\ndataset = DataBlock(\n                blocks = (ImageBlock(cls = PILImageBW), CategoryBlock),\n                get_items = get_image_files,\n                splitter = GrandparentSplitter(train_name='training', valid_name='testing'),\n                get_y = parent_label,\n                item_tfms = Resize(28),\n                batch_tfms = None\n            )\n\ndls = dataset.dataloaders(dPath, bs=128)\n\n\nprint(dls.vocab) ## Prints class labels\nprint(dls.c) ## Prints number of classes\ndls.show_batch(max_n=24,figsize=(10,6)) ## Show sample data\n\n['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n10\n\n\n\n\n\n\ndls.one_batch()[0].shape, dls.one_batch()[1].shape\n\n(torch.Size([128, 1, 28, 28]), torch.Size([128]))\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(n_in, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, n_out)\n        )\n    def forward(self, x):\n        return self.model(x.view(-1,784))\n\n\n## Defining the learner\nmodel = MLP(784, 10)\nmlp_learner = Learner(\n    dls = dls, \n    model=model, \n    loss_func=F.cross_entropy, \n    model_dir=dPath/\"models\",\n    metrics=accuracy)\n\n\n## Finidng Ideal learning late\nmlp_learner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0005754399462603033)\n\n\n\n\n\n\nmlp_learner.fit_one_cycle(5,5e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.415048\n      0.640518\n      0.835600\n      00:37\n    \n    \n      1\n      0.299409\n      0.296634\n      0.935100\n      00:37\n    \n    \n      2\n      0.198500\n      0.212431\n      0.950100\n      00:38\n    \n    \n      3\n      0.119415\n      0.128112\n      0.967800\n      00:38\n    \n    \n      4\n      0.065490\n      0.106387\n      0.973300\n      00:38"
  },
  {
    "objectID": "nbs/1_pytorch_intro.html#what-is-pytorch",
    "href": "nbs/1_pytorch_intro.html#what-is-pytorch",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.1 What is PyTorch",
    "text": "1.1 What is PyTorch\n\n\n\nPyTorch is an open-source machine learning library for Python, based on the Torch library, used for applications such as deep learning and GPU based acceleration. It is primarily developed by Facebook’s artificial intelligence research group. PyTorch provides a high-level interface for working with large-scale machine learning models. It allows developers to define and train neural networks in a fast, flexible, and dynamic way.\nSome key features of PyTorch include:\n\nTensor computations with GPU acceleration: PyTorch includes GPU acceleration out of the box, allowing you to train your models on a single or multiple GPUs with ease.\nDynamic computation graphs: PyTorch allows you to define your computations as a dynamic graph, which allows you to change the structure of the graph on the fly and enables you to use dynamic control flow.\nEasy model building: PyTorch provides a wide range of predefined layers and architectures that you can use to build your models, making it easy to get started with machine learning.\nEcosystem support: PyTorch has a large and active community of users, and there are many tools and libraries available that are built on top of PyTorch, such as visualization tools, data loaders, and more.\n\nOverall, PyTorch is a powerful and flexible tool for working with large-scale machine learning models and is widely used in research and industry.\n\n\n\n\n\n\nNote\n\n\n\n\n\nOn a personal note, I have used many different Deep learning frameworks and I find Pytorch to be the most intuitive. It blends nicely with regular Python and have really good ecosystem. I am also big fan of FastAI library which is written on top of Pytorch and is one of the best framework to do deep learning based project."
  },
  {
    "objectID": "nbs/1_pytorch_intro.html#installation",
    "href": "nbs/1_pytorch_intro.html#installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.2 Installation",
    "text": "1.2 Installation\nSince Pytorch is under active development and the new version keeps getting released. You want to visit this website for the latest Pytorch installation instruction.\n\n\n\nFig 1.1: Pytorch Installation page.\n\n\nThe above interface is what you will typically see on the installation page, based on your OS and other selection, it will produce a command you can run on your command line interface and install Pytorch. I recommend installing through conda as it will take care of the dependencies automatically."
  },
  {
    "objectID": "nbs/1_pytorch_intro.html#checking-installation",
    "href": "nbs/1_pytorch_intro.html#checking-installation",
    "title": "1  PyTorch Introduction and Installation",
    "section": "1.3 Checking Installation",
    "text": "1.3 Checking Installation\nLet’s run the import command and see if we have Pytorch setup correctly.\n\nimport torch\nprint(torch.__version__)\n\n1.12.1\n\n\nIt is recommended to use a machine with a GPU when working with PyTorch because it is typically used for GPU-accelerated computations. If you are running PyTorch on a machine with a GPU and want to verify that it has access to the GPU, you can run the following command-\n\ntorch.cuda.is_available()\n\nTrue\n\n\nIf PyTorch is installed on a machine with a GPU, the above command will return True otherwise it will return False."
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#what-are-tensors",
    "href": "nbs/2_pytorch_tensors.html#what-are-tensors",
    "title": "2  Tensors",
    "section": "2.1 What are Tensors?",
    "text": "2.1 What are Tensors?\nPyTorch provides tensors as its primary data structure. Tensors are similar to NumPy arrays, but they can be used on a GPU to accelerate the computation. PyTorch tensors are similar to NumPy arrays, but they have additional functionality (automatic differentiation) and are designed to take advantage of GPUs for acceleration. Similar to NumPy, tensors in PyTorch support a variety of operations, including indexing, slicing, math operations, linear algebra operations, and more. Let’s dive in by importing the library.\n\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#initializing-a-tensor",
    "href": "nbs/2_pytorch_tensors.html#initializing-a-tensor",
    "title": "2  Tensors",
    "section": "2.2 Initializing a Tensor",
    "text": "2.2 Initializing a Tensor\nThere are several ways to initialize tensors in PyTorch. Here are some examples:\nInitializing from an iterator like a list\n\n# Initialize a tensor from a list\ntensor_from_list = torch.tensor([1, 2, 3, 4])\nprint(\"Tensor from list: \\n\", tensor_from_list)\n\n# Initialize a tensor from a nested list\ntensor_from_nested_list = torch.tensor([[1, 2], [3, 4]])\nprint(\"Tensor from nested list: \\n\", tensor_from_nested_list)\n\nTensor from list: \n tensor([1, 2, 3, 4])\nTensor from nested list: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from a numpy array\n\n# Create a NumPy array\nnumpy_array = np.array([[1, 2], [3, 4]])\n\n# Initialize a tensor from a NumPy array\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(\"Tensor from np array: \\n\", tensor_from_numpy)\n\nTensor from np array: \n tensor([[1, 2],\n        [3, 4]])\n\n\nInitializing from another tensor\n\n# Create a tensor\noriginal_tensor = torch.tensor([1, 2, 3, 4])\n\n# Initialize a new tensor from the original tensor\nnew_tensor = original_tensor.clone()\nprint(\"Tensor from another tensor: \\n\", new_tensor)\n\nTensor from another tensor: \n tensor([1, 2, 3, 4])\n\n\nConstant or random initialization\n\n# Initialize a tensor with all elements set to zero\ntensor_zeros = torch.zeros(3, 4)\nprint(\"Tensor with all elements set to zero: \\n\", tensor_zeros)\n\n# Initialize a tensor with all elements set to one\ntensor_ones = torch.ones(3, 4)\nprint(\"\\n Tensor with all elements set to one: \\n\", tensor_ones)\n\n# Initialize a tensor with all elements set to a specific value\ntensor_full = torch.full((3, 4), fill_value=2.5)\nprint(\"\\n Tensor with all elements set to a specific value: \\n\", tensor_full)\n\n# Initialize a tensor with random values\ntensor_rand = torch.rand(3, 4)\nprint(\"\\n Tensor with random initialization: \\n\", tensor_rand)\n\n# Initialize a tensor with random values from a normal distribution\ntensor_randn = torch.randn(3, 4)\nprint(\"\\n Tensor with random values from a normal distribution: \\n\", tensor_randn)\n\nTensor with all elements set to zero: \n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])\n\n Tensor with all elements set to one: \n tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n Tensor with all elements set to a specific value: \n tensor([[2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000],\n        [2.5000, 2.5000, 2.5000, 2.5000]])\n\n Tensor with random initialization: \n tensor([[0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n Tensor with random values from a normal distribution: \n tensor([[ 1.0550,  0.9214, -1.3023,  0.4119],\n        [-0.4691,  0.8733,  0.7910, -2.3932],\n        [-0.6304, -0.8792,  0.4188,  0.4221]])"
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#tensor-attributes",
    "href": "nbs/2_pytorch_tensors.html#tensor-attributes",
    "title": "2  Tensors",
    "section": "2.3 Tensor Attributes",
    "text": "2.3 Tensor Attributes\nIt has several attributes that you can access to get information about the tensor. Here are some common attributes of a PyTorch tensor:\n\nshape: returns the shape of the tensor as a tuple of integers. For example, if the tensor has dimensions (batch_size, num_channels, height, width), the shape would be (batch_size, num_channels, height, width).\ndtype: returns the data type of the tensor. For example, the data type could be torch.float32 or torch.int64.\ndevice: returns the device on which the tensor is stored. This can be the CPU or a GPU.\nrequires_grad: a boolean flag indicating whether the tensor requires gradient computation. If set to True, the tensor’s gradients will be computed during backpropagation.\ngrad: a tensor containing the gradient of the tensor with respect to some scalar value. This attribute is typically used during training with gradient descent.\n\nYou can access these attributes by calling them on a tensor object. For example:\n\ntensor_randn = torch.randn(3, 4)\nprint(f\"Shape of tensor : {tensor_randn.shape}\")\nprint(f\"Type of tensor : {tensor_randn.dtype}\")\nprint(f\"Device tensor is stored on : {tensor_randn.device}\")\nprint(f\"Autograd enabled : {tensor_randn.requires_grad}\")\nprint(f\"Any stored gradient : {tensor_randn.grad}\")\n\nShape of tensor : torch.Size([3, 4])\nType of tensor : torch.float32\nDevice tensor is stored on : cpu\nAutograd enabled : False\nAny stored gradient : None\n\n\nAs we can see above we initialized a random tensor of shape (3,4) with a torch.float32 data type and it’s currently on a CPU device. Currently, automatic gradient calculations are disabled and no gradient is stored in the tensor.\nThere are several other attributes that you can access, such as ndim, size, numel, storage, etc. You can find more information about these attributes in the PyTorch Tensor documentation."
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#tensor-operations",
    "href": "nbs/2_pytorch_tensors.html#tensor-operations",
    "title": "2  Tensors",
    "section": "2.4 Tensor Operations",
    "text": "2.4 Tensor Operations\nThere are several operations you can perform on tensors, let’s look at the most commonly used operations.\n\n2.4.1 Moving tensor from CPU to GPU\nTo move a tensor from CPU to GPU is a simple command but probably the one which people will use the most.\n\ntensor_randn.to(\"cuda\")\n\ntensor([[-0.0984, -1.3804,  0.3343, -0.1623],\n        [ 0.9155, -0.8620, -0.3943, -0.2997],\n        [-0.1336, -0.7395, -0.7143, -0.0735]], device='cuda:0')\n\n\nAs we can see the tensor_randn is now moved to a Cuda(GPU) device.\n\n\n2.4.2 Slicing and Indexing\nPyTorch tensors similar to NumPy arrays support various slicing and indexing operations.\n\ntensor_randn = torch.randn(3, 4)\ntensor_randn\n\ntensor([[-1.3470,  0.2204,  0.2963, -0.9745],\n        [ 0.1867, -1.8338, -1.1872, -1.2987],\n        [ 0.0517, -0.3206,  0.3584, -0.4778]])\n\n\n\nprint(f\"First row:  \\n{tensor_randn[0]}\")\nprint(f\"\\n First column: \\n {tensor_randn[:, 0]}\")\nprint(f\"\\n Last column: {tensor_randn[..., -1]}\")\nprint(f\"\\n Selected columns: \\n {tensor_randn[:,2:4]}\")\n## Assignment of column to zero\ntensor_randn[:,1] = 0\nprint(\"\\n Assigning column to zero: \\n\", tensor_randn)\n\nFirst row:  \ntensor([-1.3470,  0.2204,  0.2963, -0.9745])\n\n First column: \n tensor([-1.3470,  0.1867,  0.0517])\n\n Last column: tensor([-0.9745, -1.2987, -0.4778])\n\n Selected columns: \n tensor([[ 0.2963, -0.9745],\n        [-1.1872, -1.2987],\n        [ 0.3584, -0.4778]])\n\n Assigning column to zero: \n tensor([[-1.3470,  0.0000,  0.2963, -0.9745],\n        [ 0.1867,  0.0000, -1.1872, -1.2987],\n        [ 0.0517,  0.0000,  0.3584, -0.4778]])\n\n\n\n\n2.4.3 Concatenation\nThe torch.cat function can be used to concatenate or join multiple tensors together, which is often useful when working with deep learning models.\nLet’s take our previously defined tensors and check their shape.\n\ntensor_ones.shape, tensor_zeros.shape, tensor_rand.shape\n\n(torch.Size([3, 4]), torch.Size([3, 4]), torch.Size([3, 4]))\n\n\nWe can concatenate these tensors column wise by using torch.cat with dim=1. We will get a resultant tensor with shape (3,12).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=1)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([3, 12])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8675,\n         0.0161, 0.5472, 0.7002],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6551,\n         0.3049, 0.4088, 0.6341],\n        [1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2363,\n         0.8951, 0.0335, 0.5779]])\n\n\nWe can concatenate these tensors row wise by using torch.cat with dim=0. We will get a resultant tensor with shape (9,4).\n\nconcat_tensor = torch.cat([tensor_ones, tensor_zeros, tensor_rand], dim=0)\nprint(concat_tensor.shape)\nconcat_tensor\n\ntorch.Size([9, 4])\n\n\ntensor([[1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [1.0000, 1.0000, 1.0000, 1.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000],\n        [0.8675, 0.0161, 0.5472, 0.7002],\n        [0.6551, 0.3049, 0.4088, 0.6341],\n        [0.2363, 0.8951, 0.0335, 0.5779]])\n\n\n\n\n2.4.4 Arithmetic operations\nIn PyTorch, you can perform arithmetic operations on tensors in a similar way to how you would perform them on numpy arrays. Let’s look at some common arithmetic operations -\nElement wise addition\n\ntnsr1 = torch.randn((3,4))\nprint(f\"Tensor 1: \\n\", tnsr1)\ntnsr2 = torch.randn((3,4))\nprint(f\"\\n Tensor 2: \\n\", tnsr2)\n\n## Addition\ntensor_add = tnsr1 + tnsr2 \nprint(f\"\\n Tensor additions using + : \\n\", tensor_add)\n\ntensor_add = tnsr1.add(tnsr2)\nprint(f\"\\n Tensor additions using .add : \\n\", tensor_add)\n\nTensor 1: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n Tensor 2: \n tensor([[-0.3125,  1.0860,  0.7340,  0.2249],\n        [-0.9887,  0.2265, -0.5214, -1.5676],\n        [ 0.6817,  0.1099, -0.5298, -0.3109]])\n\n Tensor additions using + : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n Tensor additions using .add : \n tensor([[-0.7810,  0.3013,  0.3142,  0.3139],\n        [-0.7391,  0.4843,  0.1152, -3.6492],\n        [-1.0097, -0.7725,  0.5511,  1.2199]])\n\n\nElement wise subtraction\n\n## Subtraction\ntensor_sub = tnsr1 - tnsr2 \nprint(f\"\\n Tensor subtraction using - : \\n\", tensor_sub)\n\ntensor_sub = tnsr1.sub(tnsr2)\nprint(f\"\\n Tensor subtraction using .sub : \\n\", tensor_sub)\n\n\n Tensor subtraction using - : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n Tensor subtraction using .sub : \n tensor([[-0.1561, -1.8708, -1.1537, -0.1359],\n        [ 1.2384,  0.0313,  1.1580, -0.5139],\n        [-2.3732, -0.9923,  1.6107,  1.8417]])\n\n\nElement wise multiplication\n\n## Multiplication\ntensor_mul = tnsr1 * tnsr2 \nprint(f\"\\n Tensor element-wise multiplication using * : \\n\", tensor_mul)\n\ntensor_mul = tnsr1.mul(tnsr2)\nprint(f\"\\n Tensor element-wise multiplication using .mul : \\n\", tensor_mul)\n\n\n Tensor element-wise multiplication using * : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n Tensor element-wise multiplication using .mul : \n tensor([[ 0.1464, -0.8523, -0.3081,  0.0200],\n        [-0.2468,  0.0584, -0.3319,  3.2631],\n        [-1.1531, -0.0970, -0.5727, -0.4759]])\n\n\nElement wise division\n\n## Division\ntensor_div = tnsr1 / tnsr2 \nprint(f\"\\n Tensor element-wise division using / : \\n\", tensor_div)\n\ntensor_div = tnsr1.div(tnsr2)\nprint(f\"\\n Tensor element-wise division using .div : \\n\", tensor_div)\n\n\n Tensor element-wise division using + : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n Tensor element-wise division using .div : \n tensor([[ 1.4994, -0.7226, -0.5719,  0.3958],\n        [-0.2525,  1.1381, -1.2209,  1.3278],\n        [-2.4811, -8.0272, -2.0401, -4.9238]])\n\n\nMatrix multiplication\n\ntensor_mm = tnsr1 @ tnsr2.T\nprint(f\"\\n Tensor matrix multiplication using @: \\n\", tensor_mm)\n\ntensor_mm = tnsr1.matmul(tnsr2.T)\nprint(f\"\\n Tensor matrix multiplication using .matmul: \\n\", tensor_mm)\n\n\n Tensor matrix multiplication using @: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n Tensor matrix multiplication using .matmul: \n tensor([[-0.9940,  0.3648, -0.2109],\n        [ 0.2010,  2.7427,  0.5084],\n        [ 0.7078, -1.4908, -2.2987]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nObserve that tnsr1 and tnsr2 have shape (3,4). To perform matrix multiplication, we used the .T function to transpose tnsr2, which changed its shape to (4,3). The resulting matrix multiplication has a shape (3,3).\n\n\nSumming it up\nSumming tensors along rows and columns is a common operation. Here is the syntax for this operation:\n\nprint(f\"Tensor: \\n {tnsr1}\" )\nprint(f\"\\n All up sum: \\n {tnsr1.sum()}\")\nprint(f\"\\n Column wise sum: \\n {tnsr1.sum(0)}\")\nprint(f\"\\n Row wise sum: \\n {tnsr1.sum(1)}\")\n\nTensor: \n tensor([[-0.4685, -0.7848, -0.4198,  0.0890],\n        [ 0.2496,  0.2578,  0.6366, -2.0815],\n        [-1.6914, -0.8824,  1.0809,  1.5308]])\n\n All up sum: \n -2.48371958732605\n\n Column wise sum: \n tensor([-1.9103, -1.4094,  1.2977, -0.4617])\n\n Row wise sum: \n tensor([-1.5841, -0.9375,  0.0378])"
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#why-gpus",
    "href": "nbs/2_pytorch_tensors.html#why-gpus",
    "title": "2  Tensors",
    "section": "2.5 Why GPUs?",
    "text": "2.5 Why GPUs?\nDeep learning models often involve large amounts of matrix operations such as matrix multiplication. Let’s do a speed comparison b/w NumPy CPU implementation, Pytorch CPU, and GPU implementation.\n\n2.5.1 Matrix multiplication using NumPy\nLet’s initialize one tensor of size (1000,64,64) and one tensor of size (64,32) and let’s do a matrix multiplication speed comparison\n\narr1 = np.random.randn(1000, 64, 64)\narr2 = np.random.randn(64, 32)\n\n\n%timeit -n 50 res = np.matmul(arr1, arr2)\n\n9.7 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs we can see matrix multiplication on NumPy which uses a highly optimized matrix multiplication does the above operation in 9.7 milliseconds.\n\n\n2.5.2 Matrix multiplication using PyTorch on CPU\nNow let’s do the same operation using PyTorch tensors on CPU.\n\ntnsr1 = torch.from_numpy(arr1)\ntnsr2 = torch.from_numpy(arr2)\n\n\n%timeit -n 50 res = tnsr1 @ tnsr2\n\n2.78 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nWe can see that PyTorch on CPU performed the same operation in 2.78 milliseconds which is roughly 3 times faster than the NumPy version.\n\n\n2.5.3 Matrix multiplication using pytorch on GPU\nLet’s do the same operation on GPU using Pytorch.\n\ntnsr1 = tnsr1.to(\"cuda\")\ntnsr2 = tnsr2.to(\"cuda\")\n\n\n%timeit -n 50 res = (tnsr1 @ tnsr2)\n\n15.6 µs ± 4.32 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\nAs demonstrated by the Matrix multiplication example, the GPU version was completed in 15.6 microseconds, a significant improvement over both the Pytorch CPU version (which took 2.8 milliseconds) and the NumPy implementation (which took 9.7 milliseconds). This speedup is even more pronounced when working with larger matrices."
  },
  {
    "objectID": "nbs/2_pytorch_tensors.html#references",
    "href": "nbs/2_pytorch_tensors.html#references",
    "title": "2  Tensors",
    "section": "2.6 References",
    "text": "2.6 References\n\nPytorch tensors tutorial documentation."
  },
  {
    "objectID": "nbs/3_broadcasting.html#what-is-broadcasting",
    "href": "nbs/3_broadcasting.html#what-is-broadcasting",
    "title": "3  Broadcasting",
    "section": "3.1 What is Broadcasting?",
    "text": "3.1 What is Broadcasting?\nIn PyTorch, broadcasting refers to the automatic expansion of a tensor’s dimensions to match the dimensions of another tensor during an operation. This allows for element-wise operations between tensors of different shapes, as long as certain rules are followed.\nFor example, consider the following operation:\n\nimport torch\n\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nc = a + b\nc\n\ntensor([5, 7, 9])\n\n\nHere, a and b are both 1-dimensional tensors with shape (3,). When performing the addition operation, PyTorch will “broadcast” a to have the same shape as b, resulting in a tensor c with shape (3,) and values [5, 7, 9].\nBroadcasting can also occur when one tensor has fewer dimensions than the other. For example:\n\na = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\nb = torch.tensor([7, 8, 9])\nc = a + b\n\nHere, a has shape (2, 3) and b has shape (3,). To perform the addition, PyTorch will broadcast b to have shape (1, 3), resulting in a tensor c with shape (2, 3) and values.\n\nc\n\ntensor([[ 8, 10, 12],\n        [11, 13, 15]])\n\n\nBroadcasting is a powerful feature in PyTorch that allows for efficient operations between tensors of different shapes and is an important concept to understand when working with deep learning models."
  },
  {
    "objectID": "nbs/3_broadcasting.html#general-broadcasting-rules",
    "href": "nbs/3_broadcasting.html#general-broadcasting-rules",
    "title": "3  Broadcasting",
    "section": "3.2 General Broadcasting rules",
    "text": "3.2 General Broadcasting rules\nThe rules for broadcasting in PyTorch are the same as those in NumPy.\nWhen comparing two arrays, PyTorch compares their shapes element-wise. It starts with the rightmost dimension and works its way left. Two dimensions are “broadcastable” when -\n\nthey are equal, or\none of them is 1\n\nIf these conditions are not met, a RuntimeError occurs, explaining the size of tensors is incompatible.\nWhen performing a broadcast operation, the resulting array will have the same number of dimensions as the input array with the most dimensions. The size of each dimension in the resulting array will be the maximum size of the corresponding dimension among the input arrays. If an input array is missing a dimension, it is assumed to have a size of one in that dimension.\nLet’s look at some broadcasting examples.\n\n3.2.1 Example 1 : Valid broadcasting\n\ntensor_a = torch.tensor([[1,2],[3,4],[5,6]])\ntensor_b = torch.tensor([[7,8]])\nprint(f\"Shape of tensor a is {tensor_a.shape}\")\nprint(f\"Shape of tensor b is {tensor_b.shape}\")\n\nShape of tensor a is torch.Size([3, 2])\nShape of tensor b is torch.Size([1, 2])\n\n\nIn the example above, tensor_a is of shape (3,2) and tensor_b is of shape (1,2). According to broadcasting principle if we start from the rightmost side -\n\nRightmost dimension is 2 for both tensor_a and tensor_b\nSecond rightmost dimension is 3 for tensor_a and 1 for tensor_b, as one of them is 1, it doesn’t violate the broadcasting rules\n\n\ntensor_c = tensor_a + tensor_b\nprint(f\"Shape of tensor c is {tensor_c.shape}\")\n\nShape of tensor c is torch.Size([3, 2])\n\n\nSince all the broadcasting rules are valid, the resultant sum of the tensors is (3,2) where tensor_b is expanded from size (1,2) to (3,2) and then added element wise with tensor_a.\n\n\n3.2.2 Example 2 : Invalid Broadcasting\n\ntensor_a = torch.tensor([[1,2],[3,4],[5,6]])\ntensor_b = torch.tensor([[7,8,9]])\nprint(f\"Shape of tensor a is {tensor_a.shape}\")\nprint(f\"Shape of tensor b is {tensor_b.shape}\")\n\nShape of tensor a is torch.Size([3, 2])\nShape of tensor b is torch.Size([1, 3])\n\n\nIn the example above, tensor_a is of shape (3,2) and tensor_b is of shape (1,3). According to broadcasting principle if we start from the rightmost side -\n\nRightmost dimension is 2 for tensor_a and 3 for tensor_b, there is a mismatch\nSecond rightmost dimension is 3 for tensor_a and 1 for tensor_b, as one of them is 1, it doesn’t violate the broadcasting rules\n\n\ntensor_c = tensor_a + tensor_b\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\nSince all the broadcasting rules are not valid, the resultant sum leads to a run time error explaining that the rightmost dimension of the tensors is not matching."
  },
  {
    "objectID": "nbs/3_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "href": "nbs/3_broadcasting.html#exercise-k-means-using-broadcasting-principle",
    "title": "3  Broadcasting",
    "section": "3.3 Exercise: K Means using Broadcasting principle",
    "text": "3.3 Exercise: K Means using Broadcasting principle\nIn this section, we will be using broadcasting principles to showcase the power of broadcasting and implement our version of K-means clustering which can run on PyTorch. K-Means is a clustering algorithm that is used to group a set of data points into a specified number of clusters. Here is the general pseudocode for the K-Means algorithm:\n\nInitialize the number of clusters, k, and the maximum number of iterations, max_iter.\nRandomly select k data points as the initial centroids for the clusters.\nIterate for a maximum of max_iter times:\n\nAssign each data point to the cluster with the nearest centroid.\nCalculate the new centroid for each cluster by taking the mean of all data points in the cluster.\n\nReturn the final clusters and their centroids.\n\n\n3.3.1 Create some random data\nLet’s try to create some random data using scikit-learn make_blobs function.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\nn_samples = 1500\nX, y = make_blobs(n_samples=n_samples, centers = 3, random_state=3)\n\n## Normalize\nX = (X - X.mean(axis=0))/X.std(axis=0)\nX = torch.from_numpy(X)\ndef plot_cluster(data, y, points=[]):\n    fig, ax = plt.subplots()\n    ax.scatter(data[:,0], data[:,1], c=y, cmap='plasma')\n    for i, point in enumerate(points):\n        ax.plot(*point, markersize=10, marker=\"x\", color='r', mew=5)\n        ax.plot(*point, markersize=5, marker=\"x\", color='b', mew=2)\n\nplot_cluster(X, y)\n\n\n\n\nFig 3.1. Visualizing randomly created clusters.\n\n\n\n\nAs we can see above, we have created 1500 samples with three clusters.\n\n\n3.3.2 Randomly initialize the centroids\nFor this exercise, we will use a random initialization for the centroids, although there are more sophisticated techniques such as the “kmeans++” method that can be used to improve the convergence of the algorithm. For the sake of simplicity, we will stick with random initialization.\n\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nprint(f\"Shape: {centroids.shape} \\n {centroids}\")\n\nShape: torch.Size([3, 2]) \n tensor([[ 0.3923, -0.2236],\n        [-0.3195, -1.2050],\n        [ 1.0445, -0.6332]])\n\n\nLet’s visualize the randomly initialized centroids.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.2. Visualizing randomly created centroid and cluster.\n\n\n\n\nAs we can see in the visualization above, the centroids are random.\n\n\n3.3.3 Assign cluster to the nearest centroid\nWe have 1500 samples and three randomly initialized centroids. Now to compute the distance between these centroids and samples we can make use of broadcasting which is vectorized and significantly improve our compute times as we don’t need to loop each sample and centroid to calculate distance.\nTo do broadcasting, we need to make sure that the two tensors are compatible for broadcasting, to achieve this we will add an additional dimension using unsqueeze method.\n\nprint(f\"Before unsqueeze: \\n Data shape: {X.shape}, Centroid shape: {centroids.shape}\")\nprint(f\"\\nAfter unsqueeae: \\n Data shape: {X.unsqueeze(1).shape}, Centroid shape: {centroids.unsqueeze(0).shape}\")\n\nBefore unsqueeze: \n Data shape: torch.Size([1500, 2]), Centroid shape: torch.Size([3, 2])\n\nAfter unsqueeae: \n Data shape: torch.Size([1500, 1, 2]), Centroid shape: torch.Size([1, 3, 2])\n\n\nWe can now compute the Euclidean distance between all 1500 samples and the three centroids in a vectorized format. To do this, we will subtract each centroid from the samples, square the differences, and sum them.\n\nsquare_dist = (X.unsqueeze(1) - centroids.unsqueeze(0)).square().sum(axis=-1)\nprint(f\"Distance of 1500 samples with three centroids : {square_dist.shape}\")\n\nDistance of 1500 samples with three centroids : torch.Size([1500, 3])\n\n\nFor assigning a sample to the nearest cluster we can use the argmin function to find the cluster with the smallest distance for each sample.\n\nprint(pd.DataFrame(square_dist.argmin(-1)).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nWe can see that 974 samples are close to centroid 0 and 515 samples are near centroid 1 and 11 samples are close to sample 2. Now let’s pack all of the above in a simple function.\n\ndef nearest_centroid(data, points):\n    '''\n    Find nearest centroid for each sample \n    '''\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\n## Check if it works as before\nnc = nearest_centroid(X,centroids)\nprint(pd.DataFrame(nc).value_counts())\n\n0    974\n1    515\n2     11\ndtype: int64\n\n\nLet’s visualize the cluster assignment.\n\nplot_cluster(X, nc, centroids)\n\n\n\n\nFig 3.3. Visualizing newly created cluster based on centroids\n\n\n\n\n\n\n3.3.4 Update centroids based on new clusters\nTo obtain the new centroid coordinates, we need to compute the mean of all the samples that are assigned to the cluster and update the centroids accordingly.\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\nnew_centroids = update_centroids(X, nc)\n\nLets visualize the new updated centroids.\n\nplot_cluster(X, nc, new_centroids)\n\n\n\n\nFig 3.4. Visualizing newly created centroids based on nearest cluster assignment\n\n\n\n\nWe can see updated centroids moved to the middle of the cluster.\n\n\n3.3.5 Iterate for a maximum of max_iter times\nLet’s set max_iter to 20 and run the cluster assignment and update the centroids for max_iter times.\n\nmax_iter = 20\ntorch.manual_seed(2)\ncentroids = torch.randn((3,X.shape[1]))\nfor _ in range(max_iter):\n    nc = nearest_centroid(X,centroids)\n    centroids = update_centroids(X, nc)\n\nLet’s visualize the centroids after running it max_iter times.\n\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.5. Visualizing final cluster centers with original labels\n\n\n\n\nWe can see now that the centroids have converged to the desired cluster center.\n\n\n3.3.6 Packaging all up\nLet’s package all the above functions to do K means.\n\ndef nearest_centroid(data, points):\n    return (data.unsqueeze(1) - points.unsqueeze(0)).square().sum(axis=-1).argmin(-1)\n\ndef update_centroids(data, nc):\n    return  torch.cat([data[nc == val,:].mean(axis=0, keepdim=True) for val in torch.unique(nc)], 0)\n\ndef k_means(X, k, max_iter=20, device=\"cpu\"):\n    ## Random initialization\n    if device == \"cpu\": \n        X = X.detach().cpu()\n    else: \n        X = X.to(device)\n    \n    centroids = torch.randn((k,X.shape[1])).to(device)\n    \n    ## Updating centroids for max_iter\n    for iter in range(max_iter): \n        new_centroids = update_centroids(X, nearest_centroid(X,centroids)).to(centroids.dtype)\n        \n        ## Early stopping\n        if torch.equal(centroids,new_centroids): break\n        else: centroids = new_centroids\n            \n    return centroids\n\nLet’s check if the function runs correctly.\n\ncentroids = k_means(X,3)\nplot_cluster(X, y, centroids)\n\n\n\n\nFig 3.6. Visualizing output cluster centers with original labels\n\n\n\n\nLet’s run the function on GPU.\n\ncentroids = k_means(X,3, device=\"cuda\").detach().cpu()\nplot_cluster(X.detach().cpu(), y, centroids)\n\n\n\n\nFig 3.7. Visualizing output cluster centers with original labels"
  },
  {
    "objectID": "nbs/3_broadcasting.html#conclusion",
    "href": "nbs/3_broadcasting.html#conclusion",
    "title": "3  Broadcasting",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nBroadcasting is a powerful feature in PyTorch that allows you to perform arithmetic operations on tensors of different shapes, as long as they are “broadcastable.”\nAs we have seen above, broadcasting allows you to perform operations on tensors of different shapes as if they were the same shape, by repeating or “broadcasting” the values of the smaller tensor along the missing dimensions. This can be a useful way to perform element-wise operations on tensors without having to explicitly pad or resize them."
  },
  {
    "objectID": "nbs/4_autograd.html#what-is-autograd",
    "href": "nbs/4_autograd.html#what-is-autograd",
    "title": "4  Automatic Differentiation",
    "section": "4.1 What is Autograd?",
    "text": "4.1 What is Autograd?\nIn PyTorch, autograd automatically computes gradients. It is a key part of PyTorch’s deep learning framework and is used to optimize model parameters during training by computing gradients of the loss function with respect to the model’s parameters.\nAutograd can compute gradients for both scalar and vector-valued functions, and it can do so efficiently for a large variety of differentiable operations, including matrix and element-wise operations, as well as higher-order derivatives.\nLet’s take a simple example of looking at a function. \\[y = a^3 - b^2 + 3\\]\nDifferentiation of this function with respect to a and b is going to be:\n\\[\\frac{dy}{da} = 3a^2\\]\n\\[\\frac{dy}{db} = -2b\\]\nSo if: \\[a = 5, b = 6\\]\nGradient with respect to a and b will be: \\[\\frac{dy}{da} = 3a^2 => 3*5^2 => 75\\]\n\\[\\frac{dy}{db} = -2b => -2*6 => -12\\]\nNow let’s observe these in PyTorch. To make a tensor compute gradients automatically we can initialize them with requires_grad = True.\n\nimport torch\n\n## initializing a anb b with requires grad = True\na = torch.tensor([5.], requires_grad=True)\nb = torch.tensor([6.], requires_grad=True)\n\ny = a**3 - b**2\ny\n\ntensor([89.], grad_fn=<SubBackward0>)\n\n\nTo compute the derivatives we can call the backward method and retrieve gradients from a and b by calling a.grad and b.grad.\n\ny.backward()\nprint(f\"Gradient of a and b is {a.grad.item()} and {b.grad.item()} respectively.\")\n\nGradient of a and b is 75.0 and -12.0 respectively.\n\n\nAs computed above the gradient of a and b is 75 and -12 respectively."
  },
  {
    "objectID": "nbs/4_autograd.html#exercise-linear-regression",
    "href": "nbs/4_autograd.html#exercise-linear-regression",
    "title": "4  Automatic Differentiation",
    "section": "4.2 Exercise: Linear regression",
    "text": "4.2 Exercise: Linear regression\nIn this section, we will implement a linear regression model in PyTorch and use the Autograd package to optimize the model’s parameters through gradient descent.\n\n4.2.1 Creating dummy data\nLet’s begin by generating linear data that exhibits linear characteristics. We will use the sklearn make_regression function to do the same.\n\n\nCode\n## Importing required functions\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport time\nfrom IPython.display import clear_output\n\nsns.set_style(\"dark\")\n%matplotlib inline\n\ndef plot_data(x, y, y_pred=None, label=None):\n    clear_output(wait=True)\n    sns.scatterplot(x=X.squeeze(), y=y)\n    if y_pred is not None:\n        sns.lineplot(x=X.squeeze(), y=y_pred.squeeze(), color='red')\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Target\")\n    if label:\n        plt.title(label)\n    plt.show()\n    time.sleep(0.5)\n\n\n\n## Generate some dataset\nX, y, coef = make_regression(n_samples=1500,\n                             n_features=1,\n                             n_informative=1,\n                             noise=0.3,\n                             coef=True,\n                             random_state=0,\n                             bias=2)\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.float32)\nplot_data(X, y, label=f\"Coefficient: {coef:.2f}, Bias:{2}\")\n\n\n\n\nFig 4.1. Visualizing our linear data\n\n\n\n\n\n\n4.2.2 Define a linear regression function\nSince we are only building a simple linear regression with one feature and one bias term. It can be defined as the following -\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n\n    def forward(self, x):\n        return x @ self.w + self.b\n\nThis code above defines a class called Linear that represents a simple linear regression model. In this case, the __init__ takes two arguments: n_in and n_out, which represent the dimensions of the input and output of the linear regression model. The __init__ method initializes the weight matrix w and the bias vector b to random values, and also sets them to be requires_grad to True, which means that the gradients of these parameters will be calculated during backpropagation. The forward method defines the forward pass of the linear regression model. It takes an input x and applies the linear transformation defined by w and b, returning the model prediction.\nLet’s initialize the model and make a random prediction.\n\n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\n## Making a random prediction\nwith torch.no_grad(): \n    y_pred = model.forward(X).numpy()\n\n## Plotting the prediction\nplot_data(X, y, y_pred)\n\n\n\n\nFig 4.2. Visualizing our data with random predictions\n\n\n\n\nThe code above generates random predictions that do not fit the data well. The torch.no_grad() context manager is used to prevent torch from calculating gradients for the operations within the context.\nTo improve the model’s performance, we can use the autograd function to create a simple gradient descent function called step, which runs one epoch of training. This will allow us to optimize the model’s parameters and improve the accuracy of our predictions.\n\n\n4.2.3 Stochastic Gradient Descent\n\ndef step(X, y, model, lr=0.1):\n    y_pred = model.forward(X)\n\n    ## Calculation mean square error\n    loss = torch.square(y - y_pred.squeeze()).mean()\n\n    ## Computing gradients\n    loss.backward()\n\n    ## Updating parameters\n    with torch.no_grad(): \n        for param in model.params:\n            param -= lr * param.grad.data\n            param.grad.data.zero_()\n    return loss\n\nLet’s walk through the step function:\n\nThe model performs a forward pass to generate predictions.\nThe mean squared error loss is calculated between the predicted values and the true values.\nThe backward method is used to compute gradients for the model’s parameters.\nThe gradients are updated with the specified learning rate.\nThe gradients are reset to zero for the next iteration.\n\n\nfor i in range(30):\n    # run one gradient descent epoch\n    loss = step(X, y, model)\n    with torch.no_grad(): \n        y_pred = model.forward(X).numpy()\n    # plot each step with delay\n    plot_data(X, y, y_pred, label=f\"Step: {i+1}, MSE = {loss:.2f}\")\n\n\n\n\nFig 4.3. Visualizing the predictions of our trained model over the data\n\n\n\n\nAs observed above, our model’s performance improved with each epoch and the mean squared error (MSE) decreased consistently.\n\nprint(f\"True coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 2.00."
  },
  {
    "objectID": "nbs/4_autograd.html#conclusion",
    "href": "nbs/4_autograd.html#conclusion",
    "title": "4  Automatic Differentiation",
    "section": "4.3 Conclusion",
    "text": "4.3 Conclusion\nAutograd is a key part of PyTorch’s deep learning framework and is an essential tool for optimizing and training neural network models. It is designed to make it easy to implement and train complex models by automatically computing gradients for differentiable operations."
  },
  {
    "objectID": "nbs/4_autograd.html#references",
    "href": "nbs/4_autograd.html#references",
    "title": "4  Automatic Differentiation",
    "section": "4.4 References",
    "text": "4.4 References\n\nPytorch Auto grad tutorial\nPytorch Autograd Mechanics"
  },
  {
    "objectID": "nbs/5_datasets.html#introduction-to-pytorch-dataset",
    "href": "nbs/5_datasets.html#introduction-to-pytorch-dataset",
    "title": "5  Datasets and Dataloaders",
    "section": "5.1 Introduction to Pytorch Dataset",
    "text": "5.1 Introduction to Pytorch Dataset\nFor training any machine learning models we need data. Typically, this data needs to be represented in form of a PyTorch tensor in order to be fed into a model. In PyTorch, a Dataset is an abstract class that represents a dataset. It provides a way to access the data and defines the way the data should be processed. The Dataset class is an abstract class and you need to create a subclass to use it. If you are not familiar with OOPs fundamentals like abstract base class and subclass, I suggest you to read this blog.\nThe main use of a dataset in PyTorch is to provide a way to access the data that you want to use to train a machine learning model. By creating a subclass of the Dataset class, you can define how the data should be loaded and processed. Once you have created a Dataset subclass, you can use it to create a PyTorch DataLoader, which is an iterator that will yield batches of data from your dataset. You can then use the DataLoader to train a model in PyTorch.\nLet’s look at the Dataset documentation.\n\nfrom torch.utils.data import Dataset\nprint(Dataset.__doc__)\n\nAn abstract class representing a :class:`Dataset`.\n\n    All datasets that represent a map from keys to data samples should subclass\n    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n    data sample for a given key. Subclasses could also optionally overwrite\n    :meth:`__len__`, which is expected to return the size of the dataset by many\n    :class:`~torch.utils.data.Sampler` implementations and the default options\n    of :class:`~torch.utils.data.DataLoader`.\n\n    .. note::\n      :class:`~torch.utils.data.DataLoader` by default constructs a index\n      sampler that yields integral indices.  To make it work with a map-style\n      dataset with non-integral indices/keys, a custom sampler must be provided.\n    \n\n\nAs we can see above Dataset is an abstract base class that requires us to implement the __getitem__ function and optionally overwrite the __len__ method for returning the size of the dataset."
  },
  {
    "objectID": "nbs/5_datasets.html#exercise-creating-our-first-custom-dataset-class",
    "href": "nbs/5_datasets.html#exercise-creating-our-first-custom-dataset-class",
    "title": "5  Datasets and Dataloaders",
    "section": "5.2 Exercise: Creating our first custom dataset class",
    "text": "5.2 Exercise: Creating our first custom dataset class\nIn this exercise, we will continue from our previous linear regression example where we trained a linear regression using batch gradient descent and replace it with mini-batch gradient descent using Dataset and Dataloaders.\nLet’s start by importing the required libraries and creating our linear data.\n\n\nCode\n## Importing required functions\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nimport seaborn as sns\nimport time\nfrom IPython.display import clear_output\nsns.set_style(\"dark\")\n%matplotlib inline\n\ndef plot_data(x, y, y_pred=None, label=None):\n    clear_output(wait=True)\n    sns.scatterplot(x = X.squeeze(), y=y)\n    if y_pred is not None:\n        sns.lineplot(x = X.squeeze(), y=y_pred.squeeze(), color='red')\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Target\")\n    if label: \n        plt.title(label)\n    plt.show()\n    time.sleep(0.5)\n\n## Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=1,\n    n_informative=1,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2\n)\n## Converting it into a Pandas dataframe\ndata = pd.DataFrame({\"X\":X.squeeze(), \"y\":y})\n\n## Visualizing the relationship b/w X and Y\nplot_data(data.X, data.y, label=f\"Coefficient: {coef:.2f}, Bias:{2}\")\n\n## Printing top 5 rows\nprint(data.head())\n\n\n\n\n\nFig 5.1. Visualizing our linear data\n\n\n\n\n          X         y\n0 -0.234216  1.901007\n1 -2.030684  1.274535\n2  0.651781  1.832122\n3  2.014060  2.936113\n4  0.829986  2.488750\n\n\nLet’s create a custom Dataset class named TabularDataset by inheriting the Dataset abstract base class and implementing our __len__ and __getitem__ functions.\n\n\n\n\n\n\nNote\n\n\n\nMethods with double underscores (also known as “dunder” methods) are special methods in Python. When the len() function is called on an object, Python will automatically call the object’s __len__ method to get the length of the object. Similarly, when the object is indexed using the square bracket operator (e.g. obj[key]), Python will call the object’s __getitem__ method to retrieve the value at the specified index.\n\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, data):\n        self.data = data.X\n        self.targets = data.y\n\n    def __len__(self): \n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data.iloc[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float), \n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\nThe TabularDataset class has three methods: __init__, __len__, and __getitem__.\n\nThe __init__ method is called when the class is instantiated and takes a pandas dataframe as input.\nThe __len__ method returns the number of samples in the dataset.\nThe __getitem__ method returns a sample from the dataset at a given index idx, in the form of a dictionary with keys X and y.\n\nWe can create an object from the TabularDataset class using our regression example, and then call the __len__ and__getitem__ methods on it.\n\n# create an object of the TabularDataset class\ncustom_dataset = TabularDataset(data)\n\n# get the length of the dataset\nsize = len(custom_dataset)\nprint(f'Dataset size: {size} \\n')\n\n# get the sample at index 0\nsample = custom_dataset[0]\nprint(f'Indexing on 0 index: \\n {sample}')\n\nDataset size: 1500 \n\nIndexing on 0 index: \n {'X': tensor(-0.2342), 'y': tensor(1.9010)}"
  },
  {
    "objectID": "nbs/5_datasets.html#dataloaders",
    "href": "nbs/5_datasets.html#dataloaders",
    "title": "5  Datasets and Dataloaders",
    "section": "5.3 Dataloaders",
    "text": "5.3 Dataloaders\nWhile training a machine learning model, it is often more efficient to pass a group of samples, or a “mini-batch,” to the model at once, rather than processing one sample at a time. Additionally, we may want to reshuffle the data at the end of each epoch and use multiple threads to speed up the data loading process.\nThe PyTorch DataLoader class helps us achieve these goals by creating an iterable from our Dataset object. The DataLoader can be used to efficiently batch and shuffle the data, and it can use multiple threads to speed up the data loading process.\nLet’s create a dataloader object from our TabularDataset object.\n\nfrom torch.utils.data import DataLoader\ncustom_dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)\n\nNow let’s look at one minibatch:\n\nbatch = next(iter(custom_dataloader))\nbatch\n\n{'X': tensor([ 1.5328,  0.4394,  1.1542, -0.6743,  0.3194, -0.5863,  0.8216, -0.9489,\n         -1.5408, -1.0546,  0.9501,  0.3382, -0.0357, -0.4675,  0.7231,  0.9694,\n          0.8526, -1.4466, -1.0994, -1.2141, -0.7999,  1.3750, -1.1268, -0.7923,\n          0.0940, -0.1043, -0.0393,  1.2961, -0.4961,  1.0170, -0.6677, -0.7946,\n          0.9364,  2.5944, -0.2201, -0.5376,  1.6581,  0.2348,  0.5766, -1.6326,\n          0.0175, -0.3328, -1.7442, -1.4464,  0.1047,  0.0633, -0.5963,  0.7775,\n         -0.3005, -0.7565, -0.7994, -0.9605,  0.2461, -0.7047,  0.3769,  0.5410,\n         -0.6524,  1.5430,  1.0480, -0.5028,  1.3676, -0.2904,  0.2671,  1.3014]),\n 'y': tensor([2.2464, 2.5879, 2.7877, 1.5178, 2.2373, 1.9258, 2.1885, 2.2265, 1.4833,\n         1.4586, 2.7604, 2.4890, 1.9327, 1.5933, 2.4738, 2.4766, 2.4160, 1.3819,\n         1.4487, 0.8635, 1.4181, 2.8232, 1.2373, 2.0373, 1.7182, 2.0764, 2.1702,\n         2.8312, 1.7150, 2.3457, 1.9804, 1.5520, 2.5604, 3.3382, 1.9031, 1.2880,\n         2.9112, 1.9802, 2.0943, 1.3462, 2.0327, 1.9207, 1.2720, 1.8974, 2.5618,\n         2.4288, 2.0103, 2.5764, 1.4878, 1.6772, 1.6701, 1.5360, 2.3156, 1.7014,\n         2.3102, 2.1018, 2.4023, 2.0447, 2.8422, 1.3625, 2.6827, 1.9267, 2.1790,\n         2.7582])}\n\n\n\nprint(f\"Input feature shape: {batch['X'].shape}\")\nprint(f\"Target  shape: {batch['y'].shape}\")\n\nInput feature shape: torch.Size([64])\nTarget  shape: torch.Size([64])\n\n\nAs we can see above, we got our first batch of 64 data samples."
  },
  {
    "objectID": "nbs/5_datasets.html#exercise-linear-regression-with-mini-batch-gradient-descent",
    "href": "nbs/5_datasets.html#exercise-linear-regression-with-mini-batch-gradient-descent",
    "title": "5  Datasets and Dataloaders",
    "section": "5.4 Exercise: Linear regression with mini-batch gradient descent",
    "text": "5.4 Exercise: Linear regression with mini-batch gradient descent\nLet’s get our model definition from the last chapter.\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n        \n    def forward(self, x):\n        return x @ self.w + self.b\n    \n## Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\n## Making a random prediction\nloss = 0\nwith torch.no_grad():\n    for batch in iter(custom_dataloader):\n        y_pred = model.forward(batch['X'].unsqueeze(-1)).numpy()\n        y_true = batch['y'].numpy()\n        loss += sum((y_pred.squeeze() - y_true.squeeze())**2)\nprint(f\"MSE loss: {loss/len(custom_dataset):.4f}\")\n\nMSE loss: 7.2129\n\n\nThis MSE of 7.2129 is bad considering in the last chapter we were able to achieve 0.09. Let’s update the previous chapter step function to take mini-batches.\n\ndef step(custom_dataloader, model, lr = 5e-3):\n    ## Iterate through mini-batch\n    for batch in iter(custom_dataloader):\n        ## Taking one mini-batch\n        y_pred = model.forward(batch['X'].unsqueeze(-1))\n        y_true = batch['y']\n        \n        ## Calculation mean square error per min-batch\n        loss = sum((y_pred.squeeze() - y_true.squeeze())**2)\n    \n        ## Computing gradients per mini-batch\n        loss.backward()\n    \n        ## Updating parameters per mini-batch\n        with torch.no_grad():\n            for param in model.params:\n                param -= lr*param.grad.data\n                param.grad.data.zero_()\n                \n    ## Compute loss for the epoch\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(custom_dataloader):\n            y_pred = model.forward(batch['X'].unsqueeze(-1))\n            y_true = batch['y']\n            loss += sum((y_pred.squeeze() - y_true.squeeze())**2)\n    return loss/len(custom_dataset)\n\nLet’s run few epochs.\n\nmodel = Linear(1,1)\nfor epoch in range(3):\n    loss = step(custom_dataloader, model)\n    print(f\"Epoch: {epoch}, MSE: {loss:.4f}\")\n    \nprint(f\"\\nTrue coefficient is {coef.item():.2f} and predicted coefficient is {model.w.item():.2f}.\")\nprint(f\"True bias term is {2} and predicted coefficient is {model.b.item():.2f}.\")\n\nEpoch: 0, MSE: 0.0879\nEpoch: 1, MSE: 0.0881\nEpoch: 2, MSE: 0.0885\n\nTrue coefficient is 0.48 and predicted coefficient is 0.47.\nTrue bias term is 2 and predicted coefficient is 1.97.\n\n\nLet’s visualize the fit.\n\ny_pred = []\nwith torch.no_grad():\n        for batch in iter(DataLoader(custom_dataset, batch_size=64, shuffle=False)):\n            y_pred.append(model.forward(batch['X'].unsqueeze(-1)).detach().numpy())      \nplot_data(X, y, y_pred=np.concatenate(y_pred))\n\n\n\n\nFig 5.2. Visualizing our fit\n\n\n\n\nFrom the results above, it appears that the model’s performance improved with each epoch, as the mean squared error (MSE) consistently decreased. The performance of the model is now similar to the performance we observed in the last chapter."
  },
  {
    "objectID": "nbs/5_datasets.html#conclusion",
    "href": "nbs/5_datasets.html#conclusion",
    "title": "5  Datasets and Dataloaders",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn PyTorch, a Dataset is an abstract class that represents a dataset. It provides a way to access the data and defines the way the data should be processed. The Dataset class is an abstract class and you need to create a subclass to use it.\nA DataLoader is an iterator that provides access to a dataset. It can be used to efficiently batch and shuffle the data, and it can use multiple threads to speed up the data loading process.\nThe Dataset and DataLoader classes are an important part of PyTorch’s data loading and processing functionality. They are often used together to train machine learning model, because they provide a convenient and efficient way to access and process data."
  },
  {
    "objectID": "nbs/5_datasets.html#references",
    "href": "nbs/5_datasets.html#references",
    "title": "5  Datasets and Dataloaders",
    "section": "5.6 References",
    "text": "5.6 References\n\nPytorch tutorial\nPytorch 101: An applied tutorial - Abhishek Thakur Youtube channel"
  },
  {
    "objectID": "nbs/6_optimizers.html#introduction-to-optimizers",
    "href": "nbs/6_optimizers.html#introduction-to-optimizers",
    "title": "6  Optimizers and Learning loops",
    "section": "6.1 Introduction to Optimizers",
    "text": "6.1 Introduction to Optimizers\nIn previous chapters, we saw how to load data and trained a linear regression model using mini-batch gradient descent. In practice, we don’t need to write our own implementation of gradient descent as Pytorch provides various inbuilt optimizers algorithm. There are many different optimizers available in PyTorch, and each one has its own set of hyperparameters that can be tuned. Some of the most popular optimizers include:\n\nSGD (Stochastic Gradient Descent): This is a simple optimizer that updates the model’s parameters using the gradient of the loss with respect to the parameters\nAdam (Adaptive Moment Estimation): This optimizer is based on the concept of momentum, which can help the optimizer to converge more quickly to a good solution. Adam also includes adaptive learning rates, which means that the optimizer can automatically adjust the learning rates of different parameters based on the historical gradient information\nRMSprop (Root Mean Square Propagation): This optimizer is similar to Adam, but it uses a different weighting for the gradient history\nAdagrad (Adaptive Gradient Algorithm): This optimizer is designed to handle sparse data, and it adjusts the learning rate for each parameter based on the historical gradient information\nAdadelta: This optimizer is an extension of Adagrad that seeks to reduce its aggressive, monotonically declining learning rate"
  },
  {
    "objectID": "nbs/6_optimizers.html#exercise-linear-regression",
    "href": "nbs/6_optimizers.html#exercise-linear-regression",
    "title": "6  Optimizers and Learning loops",
    "section": "6.2 Exercise: Linear Regression",
    "text": "6.2 Exercise: Linear Regression\nLet’s look at how we can start using Pytorch’s optimizer by continuing the previous linear regression example. Notice, this time we will use four input features instead of one in our previous examples.\n\n# Importing required functions\nimport torch\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=4,  # Using four features\n    n_informative=4,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2\n)\n\nprint(f'Input feature size: {X.shape}')\n\nInput feature size: (1500, 4)\n\n\nNow we will create a custom Dataset class.\n\n# Creating our custom TabularDataset\nclass TabularDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float),\n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\nWe have modified the TabularDataset class to handle additional features. Now, the class takes two inputs: data which includes our four features, and targets which is our target variable.\n\n# Making a train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33)\n\n# Creating Tabular Dataset\ntrain_dataset = TabularDataset(X_train, y_train)\ntest_dataset = TabularDataset(X_test, y_test)\n\n# Creating Dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nWe have divided our sample into a training set and a test set and used the TabularDataset class to create train and test objects. Finally, we created data loaders for the training set and test set using these objects.\n\n\n\n\n\n\nNote\n\n\n\nIn the code, the training data is shuffled using the Dataloader while the testing data is not. This is a common practice when training a machine learning model.\n\n\n\nclass Linear:\n    def __init__(self, n_in, n_out):\n        self.w = torch.randn(n_in, n_out).requires_grad_(True)\n        self.b = torch.randn(n_out).requires_grad_(True)\n        self.params = [self.w, self.b]\n\n    def forward(self, x):\n        return x @ self.w + self.b\n\n\n# Initializing model\ntorch.manual_seed(4)\nmodel = Linear(X.shape[1], 1)\n\nprint(f\"Shape of weights: {model.w.shape}\")\nprint(f\"Shape of bias: {model.b.shape}\")\n\nShape of weights: torch.Size([4, 1])\nShape of bias: torch.Size([1])\n\n\nWe are using the same linear model as last time but this time it will take four inputs instead of one input.\n\noptimizer = torch.optim.SGD(model.params, lr=1e-3)\n\nNext, we will define our optimizer. We will use PyTorch’s implementation of stochastic gradient descent (SGD) by initializing torch.optim.SGD. Here we are passing the model parameters which need to get modified during the training process and a hyperparameter learning rate (lr) of 1e-3.\nFor more information about other available optimizers and their hyperparameters, you can refer to the PyTorch optimizer documentation at this link.\n\ndef train_one_epoch(model, data_loader, optimizer):\n    for batch in iter(data_loader):\n        # Taking one mini-batch\n        y_pred = model.forward(batch['X']).squeeze()\n        y_true = batch['y']\n\n        # Calculation mean square error per min-batch\n        loss = torch.square(y_pred - y_true).sum()\n\n        # Computing gradients per mini-batch\n        loss.backward()\n\n        # Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n\n\ndef validate_one_epoch(model, data_loader, optimizer):\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(data_loader):\n            y_pred = model.forward(batch['X']).squeeze()\n            y_true = batch['y']\n            loss += torch.square(y_pred - y_true).sum()\n    return loss/len(data_loader)\n\nFor the training loop (defined in train_one_epoch), we will go through each mini-batch and do the following:\n\nUse the model to make a prediction\nCalculate the Mean Squared Error (MSE) and the gradients\nUpdate the model parameters using the optimizer’s step() function\nReset the gradients to zero for the next mini-batch using the optimizer’s zero_grad() function”\n\nIn the validation loop (defined in validate_one_epoch), we will process each mini-batch as follows:\n\nUse the trained model to make a prediction\nCalculate the Mean Squared Error (MSE) loss and return the overall MSE at the end\n\nNow let’s run through some epochs and train our model.\n\nfor epoch in range(10):\n    # run one training loop\n    train_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on training to compute training loss\n    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on testing to compute test loss\n    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n\n    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n\nprint(f\"Actual coefficients are: \\n{np.round(coef,4)} \\nTrained model weights are: \\n{np.round(model.w.squeeze().detach().numpy(),4)}\")\nprint(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{model.b.squeeze().detach().numpy().item():.4f}\")\n\nEpoch 0,Train MSE: 13657.7461 Test MSE: 16039.912\nEpoch 1,Train MSE: 267.4445 Test MSE: 319.128\nEpoch 2,Train MSE: 11.0232 Test MSE: 11.422\nEpoch 3,Train MSE: 5.9071 Test MSE: 5.284\nEpoch 4,Train MSE: 5.8251 Test MSE: 5.184\nEpoch 5,Train MSE: 5.8193 Test MSE: 5.183\nEpoch 6,Train MSE: 5.8243 Test MSE: 5.176\nEpoch 7,Train MSE: 5.8181 Test MSE: 5.243\nEpoch 8,Train MSE: 5.8192 Test MSE: 5.192\nEpoch 9,Train MSE: 5.8160 Test MSE: 5.230\nActual coefficients are: \n[63.0061 44.1452 84.3648  9.3378] \nTrained model weights are: \n[63.0008 44.1527 84.3725  9.3218]\nActual Bias term is 2 \nTrained model bias term is \n1.9968\n\n\nAs shown above, our model has fit the data well. The actual coefficients and bias used to generate the random data roughly match the weights and bias terms of our model."
  },
  {
    "objectID": "nbs/6_optimizers.html#conclusion",
    "href": "nbs/6_optimizers.html#conclusion",
    "title": "6  Optimizers and Learning loops",
    "section": "6.3 Conclusion",
    "text": "6.3 Conclusion\nIn PyTorch, optimizers are used to update the parameters of a model during training. Optimizers adjust the parameters of the model based on the gradients of the loss function with respect to the parameters, in order to minimize the loss.\nThere are many different optimizers available in PyTorch, including SGD, Adam, RMSprop, and more. You can choose the optimizer that works best for your specific problem and model architecture."
  },
  {
    "objectID": "nbs/6_optimizers.html#references",
    "href": "nbs/6_optimizers.html#references",
    "title": "6  Optimizers and Learning loops",
    "section": "6.4 References",
    "text": "6.4 References\n\nPytorch Optim Documentation"
  },
  {
    "objectID": "nbs/7_defining_models.html#introduction-to-the-torch.nn-module",
    "href": "nbs/7_defining_models.html#introduction-to-the-torch.nn-module",
    "title": "7  Defining Model",
    "section": "7.1 Introduction to the torch.nn module",
    "text": "7.1 Introduction to the torch.nn module\nSo far, we have explored various components of Pytorch, such as tensor manipulation, data loading, and parameter optimization. In this chapter, we will delve further into Pytorch by learning about the torch.nn module, which is designed for building and training machine learning models, particularly neural networks. The torch.nn module has a simple and pythonic API that makes it easy to prototype and create complex models with just a few lines of code."
  },
  {
    "objectID": "nbs/7_defining_models.html#exercise-linear-regression",
    "href": "nbs/7_defining_models.html#exercise-linear-regression",
    "title": "7  Defining Model",
    "section": "7.2 Exercise: Linear Regression",
    "text": "7.2 Exercise: Linear Regression\nTo continue our example of linear regression, we will now see how to use the torch.nn module to replace our custom model class. Before we do that, we will first generate a random linear dataset with four features and split the data into training and testing sets. Then, we will create custom Dataset and DataLoader objects to load the training and testing data in mini-batches.\n\n\nCode\n## Importing required functions\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n## Generate dataset with linear property\nX, y, coef = make_regression(\n    n_samples=1500,\n    n_features=4,  ## Using four features\n    n_informative=4,\n    noise=0.3,\n    coef=True,\n    random_state=0,\n    bias=2)\n\n\n## Creating our custom TabularDataset\nclass TabularDataset(Dataset):\n\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        current_sample = self.data[idx]\n        current_target = self.targets[idx]\n        return {\n            \"X\": torch.tensor(current_sample, dtype=torch.float),\n            \"y\": torch.tensor(current_target, dtype=torch.float)\n        }\n\n\n## Making a train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\n## Creating Tabular Dataset\ntrain_dataset = TabularDataset(X_train, y_train)\ntest_dataset = TabularDataset(X_test, y_test)\n\n## Creating Dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n## Training loop\ndef train_one_epoch(model, data_loader, optimizer):\n    for batch in iter(data_loader):\n        ## Taking one mini-batch\n        y_pred = model.forward(batch['X']).squeeze()\n        y_true = batch['y']\n        \n        ## Calculation mean square error per min-batch\n        loss = torch.square(y_pred - y_true).sum()\n    \n        ## Computing gradients per mini-batch\n        loss.backward()\n        \n        ## Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n        \n## Validation loop\ndef validate_one_epoch(model, data_loader, optimizer):\n    loss = 0\n    with torch.no_grad():\n        for batch in iter(data_loader):\n            y_pred = model.forward(batch['X']).squeeze()\n            y_true = batch['y']\n            loss += torch.square(y_pred- y_true).sum()\n    return loss/len(data_loader)\n\n\nThe torch.nn module contains several predefined layers that can be used to create neural networks. These layers can be found in the official PyTorch documentation for the torch.nn module. By using these predefined layers, we can simplify the process of building and training our model, as we don’t have to worry about implementing the details of each layer ourselves. Instead, we can simply specify the layers we want to use and let PyTorch handle the rest.\nNow let’s rewrite the model class using torch.nn module.\n\nclass Linear(nn.Module):\n\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.linear = nn.Linear(n_in, n_out)\n\n    def forward(self, x):\n        return self.linear(x)\n\n## Initializing model\nmodel = Linear(X.shape[1], 1)\nprint(f\"Model: \\n{model}\")\n\nprint(f\"Weights\")\nprint(list(model.parameters())[0])\n\nprint(f\"Bias\")\nprint(list(model.parameters())[1])\n\nModel: \nLinear(\n  (linear): Linear(in_features=4, out_features=1, bias=True)\n)\nWeights\nParameter containing:\ntensor([[ 0.4191,  0.2242, -0.1830,  0.0542]], requires_grad=True)\nBias\nParameter containing:\ntensor([0.4944], requires_grad=True)\n\n\nThe code above defines a class called Linear which extends the functionality of the nn.Module class from PyTorch’s torch.nn module. The Linear class has two methods: __init__ and forward.\n\nThe __init__ method is the constructor for the class. It takes two arguments: n_in and n_out, which represent the number of input and output features, respectively. The method initializes the parent class using super().__init__() and then creates a linear layer using nn.Linear. This layer will have n_in input features and n_out output features.\nThe forward method takes an input tensor x and applies the linear layer to it, returning the result.\n\nAfter the Linear class is defined, an instance of the class is created and assigned to the model variable. The model object has two learnable parameters: the weights and the bias of the linear layer. These parameters can be accessed using the parameters method and indexed using square brackets. The weights are the first element in the list of parameters, and the bias is the second element.\nNow let’s run through some epochs and train our model. We are using the same optimizer, train_one_epoch, and validate_one_epoch from the last chapter.\n\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\nfor epoch in range(10):    \n    # run one training loop\n    train_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on training to compute training loss\n    train_loss = validate_one_epoch(model, train_dataloader, optimizer)\n    # run validation loop on testing to compute test loss\n    test_loss = validate_one_epoch(model, test_dataloader, optimizer)\n    \n    print(f\"Epoch {epoch},Train MSE: {train_loss:.4f} Test MSE: {test_loss:.3f}\")\n    \nprint(f\"Actual coefficients are: \\n{np.round(coef,4)} \\nTrained model weights are: \\n{np.round(list(model.parameters())[0].detach().numpy()[0],4)}\")\nprint(f\"Actual Bias term is {2} \\nTrained model bias term is \\n{list(model.parameters())[1].detach().numpy()[0]:.4f}\")\n\nEpoch 0,Train MSE: 14168.0879 Test MSE: 16699.098\nEpoch 1,Train MSE: 285.8107 Test MSE: 347.672\nEpoch 2,Train MSE: 11.2080 Test MSE: 13.391\nEpoch 3,Train MSE: 5.7762 Test MSE: 5.876\nEpoch 4,Train MSE: 5.6652 Test MSE: 5.653\nEpoch 5,Train MSE: 5.6483 Test MSE: 5.556\nEpoch 6,Train MSE: 5.6559 Test MSE: 5.576\nEpoch 7,Train MSE: 5.6767 Test MSE: 5.539\nEpoch 8,Train MSE: 5.6488 Test MSE: 5.557\nEpoch 9,Train MSE: 5.6495 Test MSE: 5.552\nActual coefficients are: \n[63.0061 44.1452 84.3648  9.3378] \nTrained model weights are: \n[62.9917 44.1615 84.3643  9.3292]\nActual Bias term is 2 \nTrained model bias term is \n1.9978\n\n\nAs shown above, our model has fit the data well, just like the last chapter."
  },
  {
    "objectID": "nbs/7_defining_models.html#saving-and-loading-models",
    "href": "nbs/7_defining_models.html#saving-and-loading-models",
    "title": "7  Defining Model",
    "section": "7.3 Saving and Loading models",
    "text": "7.3 Saving and Loading models\nIf we want to save only the learned parameters from the model, we can use torch.save(model.state_dict()) as follows:\n\npath = \"../models/linear_model.pt\"\ntorch.save(model.state_dict(), path)\n\nTo reload the saved parameters, we first need to initiate the model object and feed the saved model parameters.\n\nmodel_new = Linear(X.shape[1], 1)\nmodel_new.load_state_dict(torch.load(path))\nprint(f\"Loaded model weights are: \\n{np.round(list(model_new.parameters())[0].detach().numpy()[0],4)}\")\nprint(f\"\\nLoaded model bias term is \\n{list(model_new.parameters())[1].detach().numpy()[0]:.4f}\")\n\nLoaded model weights are: \n[62.9917 44.1615 84.3643  9.3292]\n\nLoaded model bias term is \n1.9978"
  },
  {
    "objectID": "nbs/7_defining_models.html#exercise-what-is-torch.nn-really",
    "href": "nbs/7_defining_models.html#exercise-what-is-torch.nn-really",
    "title": "7  Defining Model",
    "section": "7.4 Exercise: What is torch.nn really?",
    "text": "7.4 Exercise: What is torch.nn really?\nNow that we have a good understanding of the fundamental concepts of Pytorch, I highly recommend reading the tutorial by Jeremy Howard from fast.ai titled “WHAT IS TORCH.NN REALLY?”. This tutorial covers everything we have learned so far and goes into more depth on the torch.nn module by showing how to implement it from scratch. It also introduces a new design pattern for building models using the nn.Sequential object, which allows you to define a model as a sequential chain of different layers. This is a simpler way of creating neural networks compared to writing them from scratch using the nn.Module class"
  },
  {
    "objectID": "nbs/7_defining_models.html#references",
    "href": "nbs/7_defining_models.html#references",
    "title": "7  Defining Model",
    "section": "7.5 References",
    "text": "7.5 References\n\nWHAT IS TORCH.NN REALLY?\nPytorch Tutorial - Build model\nPytorch torch.nn module"
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#modeling-pipeline-overview",
    "href": "nbs/8_NeuralNetworks.html#modeling-pipeline-overview",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.1 Modeling Pipeline overview",
    "text": "8.1 Modeling Pipeline overview\n\n\n\nFig 8.1: Modeling Pipeline in Pytorch\n\n\nLet’s take a look at the different steps involved in creating a typical modeling pipeline in PyTorch -\n\nGetting the data - PyTorch provides several tools for loading and preprocessing data, such as the torchvision library for image-related tasks or torchtext for natural language processing. You can also create custom data loaders to load data in your desired format.\nBuild Dataloaders -Once you have your data, you’ll need to create data loaders, which are responsible for batching and shuffling the data during training. Data loaders are essential for efficient training, as they allow you to load and preprocess data in parallel, making use of the GPU capabilities for faster training.\nDefine Model - Next, you’ll need to define your model architecture. PyTorch provides a wide range of pre-defined layers and modules that you can use to build your neural network. You can also create custom layers or models by subclassing PyTorch’s nn.Module class. Defining your model involves specifying the layers, their connectivity, and any other parameters or hyperparameters that you need for your specific task.\nBuild Optimizer and Scheduler - Once your model is defined, you’ll need to configure an optimizer and a scheduler. The optimizer is responsible for updating the model’s parameters during training to minimize the loss, while the scheduler adjusts the learning rate to optimize the model’s performance. PyTorch provides various optimization algorithms, such as SGD, Adam, or RMSprop, and scheduling techniques like learning rate decay or cyclical learning rates.\nRun training and validation loops - With your data loaders, model, optimizer, and scheduler in place, you’re ready to start the training loop. The training loop typically involves iterating over the data loaders, forwarding the inputs through the model, computing the loss, and backpropagating the gradients to update the model’s parameters. You’ll also need to evaluate your model’s performance on a validation set to monitor its progress during training and avoid overfitting.\nDeploy - Once your model has been trained, you can deploy it for inference on new data. PyTorch provides tools for saving and loading model checkpoints, which allows you to reuse your trained model in different applications. You can deploy your model in a variety of environments, such as edge devices, cloud servers, or web applications, depending on your specific requirements.\n\nIn summary, a typical modeling pipeline in PyTorch involves getting the data, building data loaders, defining the model architecture, configuring the optimizer and scheduler, implementing the training and validation loop, and finally deploying the trained model for inference in various environments.\nLet’s dive into the practical implementation of a modeling pipeline in PyTorch using the popular MNISTdataset as an example. We’ll follow the steps outlined above to build our first neural network from scratch."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#downloading-data-from-kaggle",
    "href": "nbs/8_NeuralNetworks.html#downloading-data-from-kaggle",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.2 Downloading Data from Kaggle",
    "text": "8.2 Downloading Data from Kaggle\nThe dataset we will be utilizing is the MNIST png dataset from Kaggle, as opposed to the CSV version, for a more practical experience.\nHere are few steps you need to perform before we download the data -\n\nIf you don’t have a Kaggle account, you can make one for free here.\nTo download the dataset, you will need kaggle installed, you can run the following command in notebook or CLI.\n!pip install kaggle >> /dev/null\nHave a kaggle.json stored in ~/.kaggle. You can get your token by going to Your Profile -> Account -> Create New API Token.\n\nOnce you have the above three steps done, run the API command provided:\n\n!kaggle datasets download -d jidhumohan/mnist-png -p \"../data/\"\n\nDownloading mnist-png.zip to ../data\n 87%|█████████████████████████████████     | 51.0M/58.6M [00:01<00:00, 33.4MB/s]\n100%|██████████████████████████████████████| 58.6M/58.6M [00:01<00:00, 33.9MB/s]\n\n\nTo examine the file system, we will utilize the fastcore Path function. It enhances the functionality of python’s Path class and simplifies the process of inspecting directories and folders.\n\nfrom fastcore.xtras import Path\nzip_path = Path(\"../data/mnist-png.zip\")\nzip_path.exists() # Check if the file exist\n\nTrue\n\n\nThe data has been persisted to the mnist-png.zip file on the local system, within the ../data directory. The next step is to utilize the zipfile package to extract the contents of the archive.\n\n\n\n\n\n\nWarning\n\n\n\nThe execution of the following code block will take a significant amount of time(6-10 mins) as it involves the extraction of 70,000 PNG images.\n\n\n\n# Output directory\ndPath = Path(\"../data/\")\n\n# Unzipping data file in output directory\nimport zipfile\nwith zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n    zip_ref.extractall(str(dPath))\n\n# Removing the original zip file\nzip_path.unlink()\n\n# Removing duplicate folder in the unzipped data\nimport shutil\ndPath = dPath/'mnist_png'\nshutil.rmtree(dPath/'mnist_png')\n\nNext, we inspect the extracted folder.\n\ndPath.ls()\n\n(#2) [Path('../data/mnist_png/testing'),Path('../data/mnist_png/training')]\n\n\nData contains of two folder training and testing. Next, we inspect training folder.\n\n(dPath/'training').ls()\n\n(#10) [Path('../data/mnist_png/training/0'),Path('../data/mnist_png/training/1'),Path('../data/mnist_png/training/2'),Path('../data/mnist_png/training/3'),Path('../data/mnist_png/training/4'),Path('../data/mnist_png/training/5'),Path('../data/mnist_png/training/6'),Path('../data/mnist_png/training/7'),Path('../data/mnist_png/training/8'),Path('../data/mnist_png/training/9')]\n\n\nThe training folder comprises of subfolders for each digit ranging from 0 to 9.\n\n(dPath/'training/0').ls()\n\n(#5923) [Path('../data/mnist_png/training/0/1.png'),Path('../data/mnist_png/training/0/1000.png'),Path('../data/mnist_png/training/0/10005.png'),Path('../data/mnist_png/training/0/10010.png'),Path('../data/mnist_png/training/0/10022.png'),Path('../data/mnist_png/training/0/10025.png'),Path('../data/mnist_png/training/0/10026.png'),Path('../data/mnist_png/training/0/10045.png'),Path('../data/mnist_png/training/0/10069.png'),Path('../data/mnist_png/training/0/10071.png')...]\n\n\nEach of these digit subfolders contains images. We will proceed to load a few of these images.\n\nfrom PIL import Image\nfrom IPython.display import display\nfor img in [Image.open((dPath/'training/0').ls()[0]), \n            Image.open((dPath/'training/1').ls()[0]),\n            Image.open((dPath/'training/2').ls()[0]),\n            Image.open((dPath/'training/3').ls()[0])]: \n    display(img)"
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#creating-dataset-object",
    "href": "nbs/8_NeuralNetworks.html#creating-dataset-object",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.3 Creating Dataset Object",
    "text": "8.3 Creating Dataset Object\nAs previously discussed, prior to training the model, it is necessary to establish a data pipeline in PyTorch. This includes defining a Dataset object and subsequently loading it via a PyTorch Dataloader.\n\n8.3.1 Using Pure Pytorch\nInitially, we will demonstrate the process of constructing a custom image Dataset object using pure PyTorch. To begin, we will import the necessary libraries.\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport glob\nimport numpy as np\n\nThe glob library can be utilized to obtain the filepaths of all images within a directory.\n\ntrain_paths = glob.glob(str(dPath/'training/**/*.png'))\ntest_paths = glob.glob(str(dPath/'testing/**/*.png'))\nprint(f'Training images count: {len(train_paths)} \\nTesting images count: {len(test_paths)}')\nprint(train_paths[0:5])\n\nTraining images count: 60000 \nTesting images count: 10000\n['../data/mnist_png/training/0/1.png', '../data/mnist_png/training/0/1000.png', '../data/mnist_png/training/0/10005.png', '../data/mnist_png/training/0/10010.png', '../data/mnist_png/training/0/10022.png']\n\n\nBy utilizing glob, we have successfully obtained the filepaths of all images within the training and testing folders. We can see there are 60,000 training images and 10,000 testing images. The next step is to extract the labels from the folder names.\n\ntrain_targets = list(map(lambda x: int(x.split('/')[-2]), train_paths))\ntest_targets  = list(map(lambda x: int(x.split('/')[-2]), test_paths))\nprint(f'Training labels count: {len(train_targets)} \\nTesting labels count: {len(test_targets)}')\nprint(np.random.choice(np.array(train_targets),5))\n\nTraining labels count: 60000 \nTesting labels count: 10000\n[2 3 8 5 7]\n\n\nNow let’s define our custom image Dataset class.\n\nclass ImageDataset(Dataset):\n    def __init__(self, X, y):\n        self.img_paths = X\n        self.targets  = y\n\n    def __len__(self): \n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        current_sample = torch.tensor(np.array(Image.open(self.img_paths[idx]))).flatten()/255.\n        current_target = self.targets[idx]\n        return (\n            current_sample, \n            current_target\n        )\n\nAs we can see above, ImageDataset is a custom PyTorch Dataset class. Let’s walk through the components -\n\nThe class takes two inputs in its constructor, X and y, which are lists of image file paths and corresponding labels respectively. These are stored as class variables self.img_paths and self.targets.\nThe __len__ method returns the number of images in the dataset by returning the length of self.img_paths list.\nThe __getitem__ method is called when a specific sample is requested from the dataset. It takes an index as an argument, and returns a tuple of the image data and the corresponding label for that index. The image is processed as follows -\n\nIt opens the image file at the index passed in the argument using PIL(Python Imaging Library) Image.open function\nConverts it to a numpy array\nFlattens it (convert it from 28x28 2d array to 784 1-d array)\nNormalizes it by dividing by 255 floating number\n\n\nWe will now proceed to instantiate our ImageDataset class for both thetraining and testing datasets\n\ntrain_ds = ImageDataset(X=train_paths, y=train_targets)\ntest_ds = ImageDataset(X=test_paths, y=test_targets)\n\nprint(f'One object: Image Tensor of shape {train_ds[0][0].shape}, Label: {train_ds[0][1]}')\nprint(f'One object: Image Tensor of shape {train_ds[20000][0].shape}, Label: {train_ds[20000][1]}')\n\nOne object: Image Tensor of shape torch.Size([784]), Label: 0\nOne object: Image Tensor of shape torch.Size([784]), Label: 3\n\n\n\n\n8.3.2 Using Torchvision\nWe have demonstrated the procedure of creating a custom ImageDataset object. Now we will examine how to simplify this process by utilizing the torchvision package. The torchvision package encompasses commonly used datasets, model architectures, and image transformations for computer vision tasks.\nTo begin, we will import the datasets and transforms modules from the torchvision package.\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\n\nNext we will use datasets and transform modules to load our MNIST images.\n\ntransform = transforms.Compose([\n    transforms.Grayscale(),\n    transforms.ToTensor(), \n    transforms.Lambda(lambda x: torch.flatten(x))\n    ])\n\n## Create a dataset\ntrain_ds = datasets.ImageFolder(root = dPath/'training/', \n                                      transform=transform)\n\ntest_ds = datasets.ImageFolder(root=dPath/'testing', transform=transform)\n\nprint(f'Length of train dataset: {len(train_ds)}, test_dataset: {len(test_ds)}')\nprint(f'One object: Image Tensor of shape {train_ds[0][0].shape}, Label: {train_ds[0][1]}')\n\nLength of train dataset: 60000, test_dataset: 10000\nOne object: Image Tensor of shape torch.Size([784]), Label: 0\n\n\nLet’s look at the code above:\nThe first step is to define a transform object using the transforms.Compose function. This function takes a list of transformation functions as an argument and applies them in the order they are passed in. In this case, the following transformations are applied:\n\ntransforms.Grayscale(): Convert the images to grayscale\ntransforms.ToTensor(): Converts the images to PyTorch tensors\ntransforms.Lambda(lambda x: torch.flatten(x)): Flatten the tensors from 28x28 2-D arrayto 784 1-D array\n\nNext, it creates two datasets for training and testing using the datasets.ImageFolder class. It takes the root directory of the dataset and the transform object as the arguments. It automatically creates a label for each image by taking the name of the folder where the image is stored.\nThe code then prints the length of the train and test datasets and the shape and label of the first object in the train dataset. The datasets.ImageFolder class is a convenient way to create a Pytorch dataset from a directory of images and it is useful when you have the data in a structured way."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#create-a-dataloader",
    "href": "nbs/8_NeuralNetworks.html#create-a-dataloader",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.4 Create a Dataloader",
    "text": "8.4 Create a Dataloader\nCreate a dataloader using torch.utils.data.DataLoader function.\n\nimport os\nnum_workers = int(os.cpu_count()/2)\ntrain_dls = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=num_workers)\ntest_dls = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=num_workers)\n\nThe torch.utils.data.DataLoader class takes a dataset object as an argument and returns an iterator over the dataset object. It can be used to load the data in batches, shuffle the data, and apply other useful functionality.\nIn the above code, following parameters are passed to the DataLoader:\n\ntrain_ds and test_ds are the training and testing datasets respectively.\nbatch_size=128: The number of samples per batch.\nshuffle=True for the training dataset, and shuffle=False for the testing dataset: whether to shuffle the data before iterating through it.\nnum_workers=num_workers: the number of worker threads to use for loading the data. Here it is set to half of the number of CPU cores using os.cpu_count() method.\n\nIt returns two data loaders, one for the training dataset and one for the testing dataset. The data loaders can be used as iterators to access the data in batches. This allows to load the data in smaller chunks, making it more memory efficient and faster to train.\nLet’s look at one batch.\n\nbatch = next(iter(train_dls))\nbatch[0].shape, batch[1]\n\n(torch.Size([128, 784]),\n tensor([5, 4, 1, 5, 7, 5, 4, 7, 2, 1, 5, 7, 6, 5, 8, 6, 3, 7, 8, 0, 4, 4, 4, 0,\n         6, 7, 1, 4, 0, 6, 3, 9, 1, 0, 1, 9, 4, 1, 0, 1, 9, 3, 8, 2, 6, 2, 1, 2,\n         1, 0, 2, 4, 7, 4, 7, 3, 3, 4, 3, 3, 4, 4, 7, 3, 3, 4, 6, 5, 1, 0, 2, 3,\n         0, 4, 5, 7, 1, 5, 0, 1, 1, 3, 0, 0, 1, 4, 0, 6, 2, 3, 8, 1, 8, 1, 2, 5,\n         5, 8, 9, 9, 9, 3, 1, 1, 3, 4, 1, 7, 8, 0, 1, 1, 2, 9, 1, 5, 3, 4, 0, 6,\n         1, 4, 0, 8, 9, 1, 7, 4]))\n\n\nAs we can observe, each batch comprises of an input tensor of shape (128x784) representing 128 images of flattened (28x28) dimension, and a label tensor of shape (128) representing the corresponding digit labels for the images."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#defining-our-training-and-validation-loops",
    "href": "nbs/8_NeuralNetworks.html#defining-our-training-and-validation-loops",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.5 Defining our Training and Validation loops",
    "text": "8.5 Defining our Training and Validation loops\nWe will now implement the training loop. It is similar to the training loop we constructed in chapter 6.\n\n## Training loop\ndef train_one_epoch(model, data_loader, optimizer, loss_func):\n    total_loss, nums = 0, 0\n    for batch in tqdm(iter(data_loader)):\n        ## Taking one mini-batch\n        xb, yb = batch[0].to(dev), batch[1].to(dev)\n        y_pred = model.forward(xb)\n        \n        ## Calculation mean square error per min-batch\n        nums += len(yb)\n        loss = loss_func(y_pred, yb)\n        total_loss += loss.item() * len(yb)\n\n        ## Computing gradients per mini-batch\n        loss.backward()\n        \n        ## Update model parameters and zero grad\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return  total_loss / nums\n        \n\nThe train_one_epoch function takes 4 arguments:\n\nmodel: The model to be trained\ndata_loader: The data loader for the training dataset\noptimizer: The optimizer used to update the model parameters\nloss_func: The loss function used to calculate the error of the model\n\nThe function uses a for loop to iterate through the data loader. For each mini-batch of data, it performs the following steps:\n\nIt loads the data and the labels from the data loader and sends it to the device.\nIt makes a forward pass through the model to get the predictions and then calculates the loss using the loss function.\nIt computes the gradients of the model parameters with respect to the loss.\nIt updates the model parameters using the optimizer and zero the gradients.\nThe total_loss and nums variables are used to keep track of the total loss and number of samples seen during the epoch.\n\n\ndef validate_one_epoch(model, data_loader, loss_func):\n    loss, nums, acc = 0, 0, 0\n    with torch.no_grad():\n        for batch in tqdm(iter(data_loader)):\n            xb, yb = batch[0].to(dev), batch[1].to(dev)\n            y_pred = model.forward(xb)\n            nums += len(yb)\n            loss += loss_func(y_pred, yb).item() * len(yb)\n            acc += sum(y_pred.argmax(axis=1) == yb).item()\n    return loss/nums, acc/nums\n\nThe validate_one_epoch function takes 3 arguments:\n\nmodel: The model to be validated\ndata_loader: The data loader for the validation dataset\nloss_func: The loss function used to calculate the error of the model\n\nThis function also uses a for loop to iterate through the data loader. For each mini-batch of data, it performs the following steps:\n\nIt loads the data and the labels from the data loader and sends it to the device.\nIt makes a forward pass through the model to get the predictions and then calculates the loss using the loss function.\nIt compares the predictions to the labels to calculate the accuracy.\nThe loss, nums, and acc variables are used to keep track of the total loss, number of samples seen during the epoch and accuracy respectively."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#training-using-a-fully-connected-multi-layer-perceptron-model",
    "href": "nbs/8_NeuralNetworks.html#training-using-a-fully-connected-multi-layer-perceptron-model",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.6 Training using a Fully Connected/ Multi Layer Perceptron Model",
    "text": "8.6 Training using a Fully Connected/ Multi Layer Perceptron Model\nLet’s define our model.\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(n_in, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, n_out)\n        )\n    def forward(self, x):\n        return self.model(x)\n\nThe code above defines an MLP model as a Pytorch nn.Module class. The class takes in two arguments, n_in and n_out which represents the number of input features and the number of output features of the model respectively. The class is a simple Multi-layer Perceptron model with 3 hidden layers. Each hidden layer have a linear layer with a ReLU activation function. The forward method takes in input tensor x and returns the output by passing it through the defined sequential model.\nLet’s define our training parameters.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nloss_func = nn.CrossEntropyLoss()\nmodel = MLP(784,10).to(dev)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\nepochs = 5\n\nThis code is preparing the model, loss function, optimizer, and the number of training epochs to train the MLP model.\n\ndev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"): This line of code is determining which device to use for training. If a CUDA-enabled GPU is available, the model and data will be moved to the GPU for faster training, otherwise it will use the CPU.\nloss_func = nn.CrossEntropyLoss(): This line of code is defining the loss function for the model. CrossEntropyLoss is a commonly used loss function for multi-class classification problems.\nmodel = MLP(784,10).to(dev): This line of code is instantiating the MLP model with 784 input features and 10 output features, and then moving it to the device.\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3): This line of code is creating an optimizer with Stochastic Gradient Descent (SGD) algorithm and a learning rate of 1e-3. The optimizer updates the model parameters during training to minimize the loss function.\nepochs = 5: This line of code is specifying the number of training epochs. An epoch is one complete pass through the entire training dataset.\n\nWe will now evaluate the performance of our model on the validation dataset before training.\n\ntest_loss, test_acc = validate_one_epoch(model=model, data_loader=test_dls, loss_func=loss_func)\nprint(f\"Random model: Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n\n\n\nRandom model: Test Loss: 2.3025, Test Accuracy: 0.0772\n\n\nAs anticipated, the model’s accuracy is low, around 7-8%, due to the fact that it has not been trained yet.\nWe will now encapsulate our previously defined functions in a fit function, which will be responsible for both training and evaluating the model.\n\ndef fit(epochs, model, loss_func, opt, train_dls, valid_dls):\n    for epoch in range(5):    \n        train_loss = train_one_epoch(model=model, data_loader=train_dls, optimizer=optimizer, loss_func=loss_func)\n        test_loss, test_acc = validate_one_epoch(model=model, data_loader=valid_dls, loss_func=loss_func)\n        print(f\"Epoch {epoch+1},Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Valid Accuracy: {test_acc:.4f}\")\n\nThe fit function uses a for loop to iterate over the number of training epochs. In each iteration, it calls the following functions:\n\ntrain_one_epoch: It trains the model for one epoch using the training data and optimizer.\nvalidate_one_epoch: It evaluates the model on the validation dataset and returns the loss and accuracy.\n\nIt prints the training loss, validation loss and validation accuracy for each epoch. Let’s use the fit function to train our model.\n\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 2.2945, Test Loss: 2.2852, Valid Accuracy: 0.1614\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 2.2770, Test Loss: 2.2659, Valid Accuracy: 0.2339\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 2.2564, Test Loss: 2.2426, Valid Accuracy: 0.3347\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 2.2310, Test Loss: 2.2131, Valid Accuracy: 0.4615\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 2.1983, Test Loss: 2.1752, Valid Accuracy: 0.5695\n\n\nAs we can observe, the model is training effectively and we were able to increase the accuracy from 7-8% to 57% by training for only five epochs.\nNow, we will replace the optimizer in our fitfunction to the AdamW optimizer from the torch.optim module, and rerun the fit function.\n\nmodel = MLP(784,10).to(dev)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 0.3480, Test Loss: 0.1660, Valid Accuracy: 0.9501\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 0.1392, Test Loss: 0.1289, Valid Accuracy: 0.9597\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 0.0895, Test Loss: 0.0931, Valid Accuracy: 0.9699\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 0.0659, Test Loss: 0.0758, Valid Accuracy: 0.9759\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 0.0490, Test Loss: 0.0700, Valid Accuracy: 0.9797\n\n\nBy utilizing the AdamW optimizer and MLP model, we can see that after 5 epochs, we have a highly accurate model with a 98% accuracy as compared to random prediction of 7-8%."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#training-using-a-simple-cnn-model",
    "href": "nbs/8_NeuralNetworks.html#training-using-a-simple-cnn-model",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.7 Training using a simple CNN model",
    "text": "8.7 Training using a simple CNN model\nAs previously demonstrated, the fit function is highly adaptable as we were able to change our optimizer without making any modifications to the function. Now, we will replace our MLP model with a CNN (Convolutional Neural Network) model. We will begin by defining a basic CNN network.\n\nimport torch.nn.functional as F\nclass Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nThe code above defines a class called Mnist_CNN which is a subclass of nn.Module. It creates an object of the class and initiates three 2D convolutional layers(conv1, conv2, conv3) with different input and output channels, kernel size, stride and padding. The forward method applies the convolution operation on the input tensor with relu activation function, then average pooling is applied to the output tensor and the final output tensor is reshaped to a 1-D tensor.\nNow, we can pass an instance of this model to the fit function for training and validation.\n\nmodel = Mnist_CNN().to(dev)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\nfit(epochs, model, loss_func, optimizer, train_dls, test_dls)\n\n\n\n\n\n\n\nEpoch 1,Train Loss: 1.8228, Test Loss: 1.4976, Valid Accuracy: 0.5442\n\n\n\n\n\n\n\n\nEpoch 2,Train Loss: 1.3562, Test Loss: 1.2602, Valid Accuracy: 0.5958\n\n\n\n\n\n\n\n\nEpoch 3,Train Loss: 1.2113, Test Loss: 1.1522, Valid Accuracy: 0.6144\n\n\n\n\n\n\n\n\nEpoch 4,Train Loss: 1.1286, Test Loss: 1.0886, Valid Accuracy: 0.6187\n\n\n\n\n\n\n\n\nEpoch 5,Train Loss: 1.0741, Test Loss: 1.0454, Valid Accuracy: 0.6308\n\n\nAs can be observed, we are able to seamlessly switch from an MLP to a CNN model by utilizing the adaptable fit function and train the model."
  },
  {
    "objectID": "nbs/8_NeuralNetworks.html#conclusion",
    "href": "nbs/8_NeuralNetworks.html#conclusion",
    "title": "8  Modeling pipeline with Neural networks",
    "section": "8.8 Conclusion",
    "text": "8.8 Conclusion\nIn this chapter, we progressed from a basic linear regression example to building an image classifier using MLP and CNN models. We gained practical experience in creating custom Dataset and Dataloader objects and were introduced to the torchvision library for simplifying this process. Additionally, we developed a versatile fit function, which can be utilized with various models, optimizers, and loss functions for training our models.\nThe idea of flexibility as demonstrated in the fit function is not unique, and there are many frameworks that aim to simplify the model training process by offering high-level APIs, allowing machine learning scientists to focus on building and solving problems, while the frameworks handle the majority of the complexity. Later in the book, we will repeat the same exercise using the fastai library, which is a highly flexible and performant framework built on top of PyTorch, and observe how we can construct neural networks with minimal lines of code."
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#what-is-fastai",
    "href": "nbs/9_IntroductionToFastAI.html#what-is-fastai",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.1 What is fastai?",
    "text": "9.1 What is fastai?\nAs mentioned in the previous chapter, we will now look at a extremely popular deep learning framework called fastai. fastai is an open-source software library for machine learning, which provides high-level APIs for deep learning applications. It is built on top of PyTorch and is designed to be easy to use and flexible, allowing developers to quickly prototype and implement their ideas. fastai provides a wide range of pre-trained models, including state-of-the-art models for computer vision, natural language processing, and recommendation systems. Additionally, fastai offers a number of useful tools and utilities for data processing and model evaluation, making it a popular choice among researchers and practitioners alike.\nThe creators of fastai have also created accompanying educational resources to assist in learning deep learning through the use of their framework. Both the course and book are highly recommended.\n\nHere is the link to the free course: Link\nHere is the link to the book: Link\n\nThe creators also have a peer-review paper published explaining high-level functionality and layered approach to the fastai library- Link."
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#introduction-to-fastai-api",
    "href": "nbs/9_IntroductionToFastAI.html#introduction-to-fastai-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.2 Introduction to fastai API",
    "text": "9.2 Introduction to fastai API\n\n\n\nFig 9.1: The layered API from fastai.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nThe image presented above differs from the original diagram in the fastai paper, as it represents my interpretation of using fastai in various use cases. In the original paper, the “lower level API” mentioned is similar to using raw PyTorch, so in my depiction, I consider the “Application layer” as the high-level API. This is my personal interpretation of the fastai framework and how it fits into the deep learning pipeline.\n\n\n\nfastai offers three levels of abstraction: high-level, mid-level, and low-level APIs, each catering to different use cases and levels of customization.\n\nHigh-level API: The high-level API in fastai provides a simplified and opinionated interface for training deep learning models. It offers easy-to-use functions for tasks such as data preprocessing, model architecture selection, and hyperparameter tuning, making it suitable for users who want a streamlined and automated approach to deep learning. The high-level API in fastai also includes high-level abstractions for common deep learning tasks, such as image classification, text classification, and object detection, allowing users to quickly build complex models with just a few lines of code. Example of creating a data loader could be done using the following syntax: {Task}DataLoaders.from_{source}, where {Task} represents the specific task you are working on, such as vision, segmentation, text, or tabular, and {source} represents the data source you are using for your task.\nMid-level API: The mid-level API in fastai offers a balance between customization and abstraction. It provides more control over the training process compared to the high-level API, while still offering some higher-level abstractions. With the mid-level API, users can define their own custom data loaders, specify custom architectures, and configure custom training loops, allowing for more fine-grained customization of the deep learning pipeline. This makes it suitable for users who need more flexibility and control over their models while still benefiting from some abstraction and convenience. Example of creating a data loader would involve using DataBlock API. The DataBlock API provides a flexible and powerful way to define the entire data loading pipeline, including data preprocessing, splitting, and batching, all in one place.\nLow-level API: The low-level API in fastai gives users complete control over the deep learning pipeline. It provides direct access to PyTorch’s core functionalities, such as tensors, modules, and optimizers, allowing users to build models from scratch with full customization. The low-level API is suitable for advanced users who are already familiar with PyTorch and want to leverage fastai’s utilities for additional convenience, but still need fine-grained control over every aspect of the model training process. An example of creating a data loader with fastai involves defining our own custom Dataset class and using the fastai DataLoader class as a wrapper around it.\n\nNow that we have some understanding of the different levels of fastai API, let’s dive into training our model using an example with the MNIST dataset, implemented with varying levels of API abstraction.\nDownload instruction can be found in Downloading Data from Kaggle section of Modeling pipeline with Neural Networks chapter."
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#mnist-example-using-high-level-api",
    "href": "nbs/9_IntroductionToFastAI.html#mnist-example-using-high-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.3 MNIST example using High level API",
    "text": "9.3 MNIST example using High level API\n\n9.3.1 Creating DataLoaders"
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#mnist-example-using-mid-level-api",
    "href": "nbs/9_IntroductionToFastAI.html#mnist-example-using-mid-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.4 MNIST example using Mid level API",
    "text": "9.4 MNIST example using Mid level API"
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#mnist-example-using-low-level-api",
    "href": "nbs/9_IntroductionToFastAI.html#mnist-example-using-low-level-api",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.6 MNIST example using Low level API",
    "text": "9.6 MNIST example using Low level API"
  },
  {
    "objectID": "nbs/9_IntroductionToFastAI.html#creating-a-dataloader",
    "href": "nbs/9_IntroductionToFastAI.html#creating-a-dataloader",
    "title": "9  Introduction to fastai (draft)",
    "section": "9.5 Creating a dataloader",
    "text": "9.5 Creating a dataloader\n\nfrom fastai.vision.all import *\n\n\ndPath = Path(\"../data/mnist_png/\")\n\n\ndPath.ls()\n\n(#3) [Path('../data/mnist_png/models'),Path('../data/mnist_png/testing'),Path('../data/mnist_png/training')]\n\n\n\nget_image_files(dPath)\n\n(#70000) [Path('../data/mnist_png/testing/0/10.png'),Path('../data/mnist_png/testing/0/1001.png'),Path('../data/mnist_png/testing/0/1009.png'),Path('../data/mnist_png/testing/0/101.png'),Path('../data/mnist_png/testing/0/1034.png'),Path('../data/mnist_png/testing/0/1047.png'),Path('../data/mnist_png/testing/0/1061.png'),Path('../data/mnist_png/testing/0/1084.png'),Path('../data/mnist_png/testing/0/1094.png'),Path('../data/mnist_png/testing/0/1121.png')...]\n\n\n\ndataset = DataBlock(\n                blocks = (ImageBlock(cls = PILImageBW), CategoryBlock),\n                get_items = get_image_files,\n                splitter = GrandparentSplitter(train_name='training', valid_name='testing'),\n                get_y = parent_label,\n                item_tfms = Resize(28),\n                batch_tfms = None\n            )\n\ndls = dataset.dataloaders(dPath, bs=128)\n\n\nprint(dls.vocab) ## Prints class labels\nprint(dls.c) ## Prints number of classes\ndls.show_batch(max_n=24,figsize=(10,6)) ## Show sample data\n\n['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n10\n\n\n\n\n\n\ndls.one_batch()[0].shape, dls.one_batch()[1].shape\n\n(torch.Size([128, 1, 28, 28]), torch.Size([128]))\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(n_in, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, n_out)\n        )\n    def forward(self, x):\n        return self.model(x.view(-1,784))\n\n\n## Defining the learner\nmodel = MLP(784, 10)\nmlp_learner = Learner(\n    dls = dls, \n    model=model, \n    loss_func=F.cross_entropy, \n    model_dir=dPath/\"models\",\n    metrics=accuracy)\n\n\n## Finidng Ideal learning late\nmlp_learner.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0005754399462603033)\n\n\n\n\n\n\nmlp_learner.fit_one_cycle(5,5e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.415048\n      0.640518\n      0.835600\n      00:37\n    \n    \n      1\n      0.299409\n      0.296634\n      0.935100\n      00:37\n    \n    \n      2\n      0.198500\n      0.212431\n      0.950100\n      00:38\n    \n    \n      3\n      0.119415\n      0.128112\n      0.967800\n      00:38\n    \n    \n      4\n      0.065490\n      0.106387\n      0.973300\n      00:38"
  }
]